{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.4953733993831199,
  "eval_steps": 500,
  "global_step": 2650,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0001869333582577811,
      "grad_norm": 28.95795249938965,
      "learning_rate": 0.0002,
      "loss": 7.9533,
      "step": 1
    },
    {
      "epoch": 0.0003738667165155622,
      "grad_norm": 14.621254920959473,
      "learning_rate": 0.00019992523364485983,
      "loss": 6.855,
      "step": 2
    },
    {
      "epoch": 0.0005608000747733433,
      "grad_norm": 11.017350196838379,
      "learning_rate": 0.00019985046728971965,
      "loss": 6.0057,
      "step": 3
    },
    {
      "epoch": 0.0007477334330311244,
      "grad_norm": 12.176111221313477,
      "learning_rate": 0.00019977570093457944,
      "loss": 5.2855,
      "step": 4
    },
    {
      "epoch": 0.0009346667912889055,
      "grad_norm": 18.372241973876953,
      "learning_rate": 0.00019970093457943926,
      "loss": 4.3072,
      "step": 5
    },
    {
      "epoch": 0.0011216001495466866,
      "grad_norm": 39.31769943237305,
      "learning_rate": 0.00019962616822429908,
      "loss": 3.1788,
      "step": 6
    },
    {
      "epoch": 0.0013085335078044677,
      "grad_norm": 15.446724891662598,
      "learning_rate": 0.0001995514018691589,
      "loss": 3.2827,
      "step": 7
    },
    {
      "epoch": 0.0014954668660622488,
      "grad_norm": 8.932825088500977,
      "learning_rate": 0.0001994766355140187,
      "loss": 2.339,
      "step": 8
    },
    {
      "epoch": 0.00168240022432003,
      "grad_norm": 5.575058937072754,
      "learning_rate": 0.0001994018691588785,
      "loss": 1.5286,
      "step": 9
    },
    {
      "epoch": 0.001869333582577811,
      "grad_norm": 8.50461483001709,
      "learning_rate": 0.00019932710280373833,
      "loss": 1.9411,
      "step": 10
    },
    {
      "epoch": 0.002056266940835592,
      "grad_norm": 8.797102928161621,
      "learning_rate": 0.00019925233644859814,
      "loss": 1.4733,
      "step": 11
    },
    {
      "epoch": 0.002243200299093373,
      "grad_norm": 8.300895690917969,
      "learning_rate": 0.00019917757009345794,
      "loss": 2.3162,
      "step": 12
    },
    {
      "epoch": 0.002430133657351154,
      "grad_norm": 9.021040916442871,
      "learning_rate": 0.00019910280373831775,
      "loss": 2.5057,
      "step": 13
    },
    {
      "epoch": 0.0026170670156089354,
      "grad_norm": 5.6675920486450195,
      "learning_rate": 0.0001990280373831776,
      "loss": 1.3962,
      "step": 14
    },
    {
      "epoch": 0.0028040003738667163,
      "grad_norm": 7.029674530029297,
      "learning_rate": 0.0001989532710280374,
      "loss": 1.757,
      "step": 15
    },
    {
      "epoch": 0.0029909337321244976,
      "grad_norm": 4.929495811462402,
      "learning_rate": 0.0001988785046728972,
      "loss": 1.5512,
      "step": 16
    },
    {
      "epoch": 0.0031778670903822785,
      "grad_norm": 4.885506629943848,
      "learning_rate": 0.00019880373831775703,
      "loss": 1.8262,
      "step": 17
    },
    {
      "epoch": 0.00336480044864006,
      "grad_norm": 4.042532444000244,
      "learning_rate": 0.00019872897196261685,
      "loss": 1.2007,
      "step": 18
    },
    {
      "epoch": 0.0035517338068978407,
      "grad_norm": 5.341353416442871,
      "learning_rate": 0.00019865420560747664,
      "loss": 1.6365,
      "step": 19
    },
    {
      "epoch": 0.003738667165155622,
      "grad_norm": 7.263873100280762,
      "learning_rate": 0.00019857943925233646,
      "loss": 2.0631,
      "step": 20
    },
    {
      "epoch": 0.003925600523413403,
      "grad_norm": 4.885860919952393,
      "learning_rate": 0.00019850467289719628,
      "loss": 1.5239,
      "step": 21
    },
    {
      "epoch": 0.004112533881671184,
      "grad_norm": 4.16333532333374,
      "learning_rate": 0.0001984299065420561,
      "loss": 1.7254,
      "step": 22
    },
    {
      "epoch": 0.004299467239928965,
      "grad_norm": 4.312268257141113,
      "learning_rate": 0.0001983551401869159,
      "loss": 1.7493,
      "step": 23
    },
    {
      "epoch": 0.004486400598186746,
      "grad_norm": 3.600210666656494,
      "learning_rate": 0.0001982803738317757,
      "loss": 1.1261,
      "step": 24
    },
    {
      "epoch": 0.004673333956444528,
      "grad_norm": 5.72406530380249,
      "learning_rate": 0.00019820560747663553,
      "loss": 2.103,
      "step": 25
    },
    {
      "epoch": 0.004860267314702308,
      "grad_norm": 5.399073600769043,
      "learning_rate": 0.00019813084112149535,
      "loss": 1.3663,
      "step": 26
    },
    {
      "epoch": 0.0050472006729600895,
      "grad_norm": 5.4925031661987305,
      "learning_rate": 0.00019805607476635514,
      "loss": 1.3447,
      "step": 27
    },
    {
      "epoch": 0.005234134031217871,
      "grad_norm": 5.530284881591797,
      "learning_rate": 0.00019798130841121496,
      "loss": 1.8998,
      "step": 28
    },
    {
      "epoch": 0.005421067389475652,
      "grad_norm": 5.42996883392334,
      "learning_rate": 0.00019790654205607478,
      "loss": 1.1013,
      "step": 29
    },
    {
      "epoch": 0.005608000747733433,
      "grad_norm": 4.61420202255249,
      "learning_rate": 0.0001978317757009346,
      "loss": 1.2716,
      "step": 30
    },
    {
      "epoch": 0.005794934105991214,
      "grad_norm": 5.9548468589782715,
      "learning_rate": 0.00019775700934579439,
      "loss": 0.9205,
      "step": 31
    },
    {
      "epoch": 0.005981867464248995,
      "grad_norm": 5.470587253570557,
      "learning_rate": 0.0001976822429906542,
      "loss": 0.7057,
      "step": 32
    },
    {
      "epoch": 0.0061688008225067766,
      "grad_norm": 4.851755619049072,
      "learning_rate": 0.00019760747663551402,
      "loss": 1.0077,
      "step": 33
    },
    {
      "epoch": 0.006355734180764557,
      "grad_norm": 5.894190788269043,
      "learning_rate": 0.00019753271028037384,
      "loss": 1.6284,
      "step": 34
    },
    {
      "epoch": 0.006542667539022338,
      "grad_norm": 4.836962699890137,
      "learning_rate": 0.00019745794392523363,
      "loss": 1.1739,
      "step": 35
    },
    {
      "epoch": 0.00672960089728012,
      "grad_norm": 6.807077884674072,
      "learning_rate": 0.00019738317757009345,
      "loss": 1.2476,
      "step": 36
    },
    {
      "epoch": 0.006916534255537901,
      "grad_norm": 4.217123031616211,
      "learning_rate": 0.00019730841121495327,
      "loss": 1.2259,
      "step": 37
    },
    {
      "epoch": 0.007103467613795681,
      "grad_norm": 6.091845989227295,
      "learning_rate": 0.0001972336448598131,
      "loss": 1.5455,
      "step": 38
    },
    {
      "epoch": 0.007290400972053463,
      "grad_norm": 3.963721752166748,
      "learning_rate": 0.0001971588785046729,
      "loss": 1.1952,
      "step": 39
    },
    {
      "epoch": 0.007477334330311244,
      "grad_norm": 5.572592735290527,
      "learning_rate": 0.00019708411214953273,
      "loss": 1.6528,
      "step": 40
    },
    {
      "epoch": 0.007664267688569025,
      "grad_norm": 4.47229528427124,
      "learning_rate": 0.00019700934579439255,
      "loss": 1.3045,
      "step": 41
    },
    {
      "epoch": 0.007851201046826807,
      "grad_norm": 4.0958099365234375,
      "learning_rate": 0.00019693457943925237,
      "loss": 1.1546,
      "step": 42
    },
    {
      "epoch": 0.008038134405084588,
      "grad_norm": 4.225883483886719,
      "learning_rate": 0.00019685981308411216,
      "loss": 1.1089,
      "step": 43
    },
    {
      "epoch": 0.008225067763342368,
      "grad_norm": 4.121451377868652,
      "learning_rate": 0.00019678504672897198,
      "loss": 1.6802,
      "step": 44
    },
    {
      "epoch": 0.008412001121600149,
      "grad_norm": 2.799602746963501,
      "learning_rate": 0.0001967102803738318,
      "loss": 1.0189,
      "step": 45
    },
    {
      "epoch": 0.00859893447985793,
      "grad_norm": 3.845877170562744,
      "learning_rate": 0.00019663551401869161,
      "loss": 0.8519,
      "step": 46
    },
    {
      "epoch": 0.008785867838115712,
      "grad_norm": 4.137421607971191,
      "learning_rate": 0.0001965607476635514,
      "loss": 0.7584,
      "step": 47
    },
    {
      "epoch": 0.008972801196373493,
      "grad_norm": 3.96213698387146,
      "learning_rate": 0.00019648598130841123,
      "loss": 0.8185,
      "step": 48
    },
    {
      "epoch": 0.009159734554631274,
      "grad_norm": 4.542984962463379,
      "learning_rate": 0.00019641121495327104,
      "loss": 0.9039,
      "step": 49
    },
    {
      "epoch": 0.009346667912889055,
      "grad_norm": 5.348071098327637,
      "learning_rate": 0.00019633644859813086,
      "loss": 0.8809,
      "step": 50
    },
    {
      "epoch": 0.009533601271146837,
      "grad_norm": 4.972592353820801,
      "learning_rate": 0.00019626168224299065,
      "loss": 0.8867,
      "step": 51
    },
    {
      "epoch": 0.009720534629404616,
      "grad_norm": 3.7315752506256104,
      "learning_rate": 0.00019618691588785047,
      "loss": 0.7397,
      "step": 52
    },
    {
      "epoch": 0.009907467987662398,
      "grad_norm": 5.030660152435303,
      "learning_rate": 0.0001961121495327103,
      "loss": 1.209,
      "step": 53
    },
    {
      "epoch": 0.010094401345920179,
      "grad_norm": 5.870164394378662,
      "learning_rate": 0.0001960373831775701,
      "loss": 1.2501,
      "step": 54
    },
    {
      "epoch": 0.01028133470417796,
      "grad_norm": 5.794811725616455,
      "learning_rate": 0.0001959626168224299,
      "loss": 0.8813,
      "step": 55
    },
    {
      "epoch": 0.010468268062435742,
      "grad_norm": 4.874475479125977,
      "learning_rate": 0.00019588785046728972,
      "loss": 0.7591,
      "step": 56
    },
    {
      "epoch": 0.010655201420693523,
      "grad_norm": 4.900634765625,
      "learning_rate": 0.00019581308411214954,
      "loss": 0.9032,
      "step": 57
    },
    {
      "epoch": 0.010842134778951304,
      "grad_norm": 5.597862720489502,
      "learning_rate": 0.00019573831775700936,
      "loss": 1.1195,
      "step": 58
    },
    {
      "epoch": 0.011029068137209086,
      "grad_norm": 4.1828436851501465,
      "learning_rate": 0.00019566355140186915,
      "loss": 0.8078,
      "step": 59
    },
    {
      "epoch": 0.011216001495466865,
      "grad_norm": 4.172274589538574,
      "learning_rate": 0.00019558878504672897,
      "loss": 1.063,
      "step": 60
    },
    {
      "epoch": 0.011402934853724647,
      "grad_norm": 3.3637712001800537,
      "learning_rate": 0.0001955140186915888,
      "loss": 0.6298,
      "step": 61
    },
    {
      "epoch": 0.011589868211982428,
      "grad_norm": 3.307512044906616,
      "learning_rate": 0.0001954392523364486,
      "loss": 0.7423,
      "step": 62
    },
    {
      "epoch": 0.01177680157024021,
      "grad_norm": 3.569164752960205,
      "learning_rate": 0.00019536448598130843,
      "loss": 0.7364,
      "step": 63
    },
    {
      "epoch": 0.01196373492849799,
      "grad_norm": 4.934080123901367,
      "learning_rate": 0.00019528971962616825,
      "loss": 1.1805,
      "step": 64
    },
    {
      "epoch": 0.012150668286755772,
      "grad_norm": 3.9241578578948975,
      "learning_rate": 0.00019521495327102806,
      "loss": 1.102,
      "step": 65
    },
    {
      "epoch": 0.012337601645013553,
      "grad_norm": 3.9105234146118164,
      "learning_rate": 0.00019514018691588786,
      "loss": 1.1039,
      "step": 66
    },
    {
      "epoch": 0.012524535003271334,
      "grad_norm": 5.604002475738525,
      "learning_rate": 0.00019506542056074768,
      "loss": 1.0855,
      "step": 67
    },
    {
      "epoch": 0.012711468361529114,
      "grad_norm": 4.796741008758545,
      "learning_rate": 0.0001949906542056075,
      "loss": 1.0733,
      "step": 68
    },
    {
      "epoch": 0.012898401719786895,
      "grad_norm": 4.638655185699463,
      "learning_rate": 0.0001949158878504673,
      "loss": 0.7138,
      "step": 69
    },
    {
      "epoch": 0.013085335078044677,
      "grad_norm": 3.8813974857330322,
      "learning_rate": 0.0001948411214953271,
      "loss": 0.6925,
      "step": 70
    },
    {
      "epoch": 0.013272268436302458,
      "grad_norm": 4.501897811889648,
      "learning_rate": 0.00019476635514018692,
      "loss": 0.8316,
      "step": 71
    },
    {
      "epoch": 0.01345920179456024,
      "grad_norm": 4.089796543121338,
      "learning_rate": 0.00019469158878504674,
      "loss": 0.8507,
      "step": 72
    },
    {
      "epoch": 0.01364613515281802,
      "grad_norm": 4.251369476318359,
      "learning_rate": 0.00019461682242990656,
      "loss": 0.5755,
      "step": 73
    },
    {
      "epoch": 0.013833068511075802,
      "grad_norm": 4.995138168334961,
      "learning_rate": 0.00019454205607476635,
      "loss": 1.1484,
      "step": 74
    },
    {
      "epoch": 0.014020001869333583,
      "grad_norm": 5.093594551086426,
      "learning_rate": 0.00019446728971962617,
      "loss": 0.8165,
      "step": 75
    },
    {
      "epoch": 0.014206935227591363,
      "grad_norm": 4.243933200836182,
      "learning_rate": 0.000194392523364486,
      "loss": 0.8897,
      "step": 76
    },
    {
      "epoch": 0.014393868585849144,
      "grad_norm": 4.0094523429870605,
      "learning_rate": 0.0001943177570093458,
      "loss": 0.5407,
      "step": 77
    },
    {
      "epoch": 0.014580801944106925,
      "grad_norm": 6.3651251792907715,
      "learning_rate": 0.0001942429906542056,
      "loss": 0.9759,
      "step": 78
    },
    {
      "epoch": 0.014767735302364707,
      "grad_norm": 4.08912467956543,
      "learning_rate": 0.00019416822429906542,
      "loss": 0.6195,
      "step": 79
    },
    {
      "epoch": 0.014954668660622488,
      "grad_norm": 10.479945182800293,
      "learning_rate": 0.00019409345794392524,
      "loss": 0.7546,
      "step": 80
    },
    {
      "epoch": 0.01514160201888027,
      "grad_norm": 10.276570320129395,
      "learning_rate": 0.00019401869158878506,
      "loss": 0.961,
      "step": 81
    },
    {
      "epoch": 0.01532853537713805,
      "grad_norm": 5.202259063720703,
      "learning_rate": 0.00019394392523364485,
      "loss": 0.7285,
      "step": 82
    },
    {
      "epoch": 0.015515468735395832,
      "grad_norm": 4.417999267578125,
      "learning_rate": 0.00019386915887850467,
      "loss": 0.676,
      "step": 83
    },
    {
      "epoch": 0.015702402093653613,
      "grad_norm": 5.402785301208496,
      "learning_rate": 0.0001937943925233645,
      "loss": 0.8331,
      "step": 84
    },
    {
      "epoch": 0.015889335451911395,
      "grad_norm": 3.9065263271331787,
      "learning_rate": 0.0001937196261682243,
      "loss": 0.5051,
      "step": 85
    },
    {
      "epoch": 0.016076268810169176,
      "grad_norm": 3.9249303340911865,
      "learning_rate": 0.00019364485981308413,
      "loss": 0.6257,
      "step": 86
    },
    {
      "epoch": 0.016263202168426957,
      "grad_norm": 4.137866973876953,
      "learning_rate": 0.00019357009345794392,
      "loss": 0.6824,
      "step": 87
    },
    {
      "epoch": 0.016450135526684735,
      "grad_norm": 5.187356948852539,
      "learning_rate": 0.00019349532710280376,
      "loss": 0.605,
      "step": 88
    },
    {
      "epoch": 0.016637068884942516,
      "grad_norm": 4.535901069641113,
      "learning_rate": 0.00019342056074766358,
      "loss": 0.5719,
      "step": 89
    },
    {
      "epoch": 0.016824002243200298,
      "grad_norm": 4.330751895904541,
      "learning_rate": 0.00019334579439252337,
      "loss": 0.6125,
      "step": 90
    },
    {
      "epoch": 0.01701093560145808,
      "grad_norm": 5.445030212402344,
      "learning_rate": 0.0001932710280373832,
      "loss": 0.8343,
      "step": 91
    },
    {
      "epoch": 0.01719786895971586,
      "grad_norm": 4.899934768676758,
      "learning_rate": 0.000193196261682243,
      "loss": 0.679,
      "step": 92
    },
    {
      "epoch": 0.017384802317973642,
      "grad_norm": 3.257974624633789,
      "learning_rate": 0.00019312149532710283,
      "loss": 0.511,
      "step": 93
    },
    {
      "epoch": 0.017571735676231423,
      "grad_norm": 2.957566976547241,
      "learning_rate": 0.00019304672897196262,
      "loss": 0.5693,
      "step": 94
    },
    {
      "epoch": 0.017758669034489204,
      "grad_norm": 2.694600820541382,
      "learning_rate": 0.00019297196261682244,
      "loss": 0.4441,
      "step": 95
    },
    {
      "epoch": 0.017945602392746986,
      "grad_norm": 3.6796326637268066,
      "learning_rate": 0.00019289719626168226,
      "loss": 0.6111,
      "step": 96
    },
    {
      "epoch": 0.018132535751004767,
      "grad_norm": 2.799875020980835,
      "learning_rate": 0.00019282242990654208,
      "loss": 0.5597,
      "step": 97
    },
    {
      "epoch": 0.01831946910926255,
      "grad_norm": 5.310861110687256,
      "learning_rate": 0.00019274766355140187,
      "loss": 0.5446,
      "step": 98
    },
    {
      "epoch": 0.01850640246752033,
      "grad_norm": 3.5314440727233887,
      "learning_rate": 0.0001926728971962617,
      "loss": 0.5019,
      "step": 99
    },
    {
      "epoch": 0.01869333582577811,
      "grad_norm": 5.3484721183776855,
      "learning_rate": 0.0001925981308411215,
      "loss": 0.7016,
      "step": 100
    },
    {
      "epoch": 0.018880269184035892,
      "grad_norm": 4.559046745300293,
      "learning_rate": 0.00019252336448598133,
      "loss": 0.6155,
      "step": 101
    },
    {
      "epoch": 0.019067202542293674,
      "grad_norm": 5.233382225036621,
      "learning_rate": 0.00019244859813084112,
      "loss": 0.4848,
      "step": 102
    },
    {
      "epoch": 0.019254135900551455,
      "grad_norm": 4.8345255851745605,
      "learning_rate": 0.00019237383177570094,
      "loss": 0.6223,
      "step": 103
    },
    {
      "epoch": 0.019441069258809233,
      "grad_norm": 3.126300096511841,
      "learning_rate": 0.00019229906542056076,
      "loss": 0.5221,
      "step": 104
    },
    {
      "epoch": 0.019628002617067014,
      "grad_norm": 3.919543743133545,
      "learning_rate": 0.00019222429906542058,
      "loss": 0.5667,
      "step": 105
    },
    {
      "epoch": 0.019814935975324795,
      "grad_norm": 6.805061340332031,
      "learning_rate": 0.00019214953271028037,
      "loss": 0.7141,
      "step": 106
    },
    {
      "epoch": 0.020001869333582577,
      "grad_norm": 3.6845428943634033,
      "learning_rate": 0.00019207476635514019,
      "loss": 0.5288,
      "step": 107
    },
    {
      "epoch": 0.020188802691840358,
      "grad_norm": 4.368716239929199,
      "learning_rate": 0.000192,
      "loss": 0.6682,
      "step": 108
    },
    {
      "epoch": 0.02037573605009814,
      "grad_norm": 4.043107986450195,
      "learning_rate": 0.00019192523364485982,
      "loss": 0.5764,
      "step": 109
    },
    {
      "epoch": 0.02056266940835592,
      "grad_norm": 4.070779323577881,
      "learning_rate": 0.00019185046728971962,
      "loss": 0.5464,
      "step": 110
    },
    {
      "epoch": 0.020749602766613702,
      "grad_norm": 7.387327194213867,
      "learning_rate": 0.00019177570093457943,
      "loss": 0.5108,
      "step": 111
    },
    {
      "epoch": 0.020936536124871483,
      "grad_norm": 4.731065273284912,
      "learning_rate": 0.00019170093457943925,
      "loss": 0.6376,
      "step": 112
    },
    {
      "epoch": 0.021123469483129265,
      "grad_norm": 6.8221821784973145,
      "learning_rate": 0.00019162616822429907,
      "loss": 0.7044,
      "step": 113
    },
    {
      "epoch": 0.021310402841387046,
      "grad_norm": 5.0284600257873535,
      "learning_rate": 0.0001915514018691589,
      "loss": 0.6005,
      "step": 114
    },
    {
      "epoch": 0.021497336199644827,
      "grad_norm": 3.6473896503448486,
      "learning_rate": 0.0001914766355140187,
      "loss": 0.5071,
      "step": 115
    },
    {
      "epoch": 0.02168426955790261,
      "grad_norm": 3.6200826168060303,
      "learning_rate": 0.00019140186915887853,
      "loss": 0.5501,
      "step": 116
    },
    {
      "epoch": 0.02187120291616039,
      "grad_norm": 3.6343822479248047,
      "learning_rate": 0.00019132710280373832,
      "loss": 0.4228,
      "step": 117
    },
    {
      "epoch": 0.02205813627441817,
      "grad_norm": 2.922851800918579,
      "learning_rate": 0.00019125233644859814,
      "loss": 0.5203,
      "step": 118
    },
    {
      "epoch": 0.022245069632675953,
      "grad_norm": 5.092449188232422,
      "learning_rate": 0.00019117757009345796,
      "loss": 0.6173,
      "step": 119
    },
    {
      "epoch": 0.02243200299093373,
      "grad_norm": 4.002330303192139,
      "learning_rate": 0.00019110280373831778,
      "loss": 0.4543,
      "step": 120
    },
    {
      "epoch": 0.02261893634919151,
      "grad_norm": 3.9392824172973633,
      "learning_rate": 0.0001910280373831776,
      "loss": 0.4413,
      "step": 121
    },
    {
      "epoch": 0.022805869707449293,
      "grad_norm": 4.706637382507324,
      "learning_rate": 0.0001909532710280374,
      "loss": 0.6378,
      "step": 122
    },
    {
      "epoch": 0.022992803065707074,
      "grad_norm": 4.835588455200195,
      "learning_rate": 0.0001908785046728972,
      "loss": 0.5407,
      "step": 123
    },
    {
      "epoch": 0.023179736423964856,
      "grad_norm": 4.546491622924805,
      "learning_rate": 0.00019080373831775703,
      "loss": 0.6797,
      "step": 124
    },
    {
      "epoch": 0.023366669782222637,
      "grad_norm": 3.955454111099243,
      "learning_rate": 0.00019072897196261684,
      "loss": 0.5475,
      "step": 125
    },
    {
      "epoch": 0.02355360314048042,
      "grad_norm": 3.0767745971679688,
      "learning_rate": 0.00019065420560747664,
      "loss": 0.4111,
      "step": 126
    },
    {
      "epoch": 0.0237405364987382,
      "grad_norm": 3.729928493499756,
      "learning_rate": 0.00019057943925233645,
      "loss": 0.4694,
      "step": 127
    },
    {
      "epoch": 0.02392746985699598,
      "grad_norm": 4.376532554626465,
      "learning_rate": 0.00019050467289719627,
      "loss": 0.5933,
      "step": 128
    },
    {
      "epoch": 0.024114403215253762,
      "grad_norm": 5.374238967895508,
      "learning_rate": 0.0001904299065420561,
      "loss": 0.6301,
      "step": 129
    },
    {
      "epoch": 0.024301336573511544,
      "grad_norm": 4.866232872009277,
      "learning_rate": 0.00019035514018691588,
      "loss": 0.4632,
      "step": 130
    },
    {
      "epoch": 0.024488269931769325,
      "grad_norm": 6.602817058563232,
      "learning_rate": 0.0001902803738317757,
      "loss": 0.6827,
      "step": 131
    },
    {
      "epoch": 0.024675203290027106,
      "grad_norm": 4.9802069664001465,
      "learning_rate": 0.00019020560747663552,
      "loss": 0.6177,
      "step": 132
    },
    {
      "epoch": 0.024862136648284888,
      "grad_norm": 4.806365966796875,
      "learning_rate": 0.00019013084112149534,
      "loss": 0.5892,
      "step": 133
    },
    {
      "epoch": 0.02504907000654267,
      "grad_norm": 3.6464555263519287,
      "learning_rate": 0.00019005607476635513,
      "loss": 0.4843,
      "step": 134
    },
    {
      "epoch": 0.02523600336480045,
      "grad_norm": 2.6525356769561768,
      "learning_rate": 0.00018998130841121495,
      "loss": 0.4469,
      "step": 135
    },
    {
      "epoch": 0.025422936723058228,
      "grad_norm": 5.841170787811279,
      "learning_rate": 0.00018990654205607477,
      "loss": 0.5972,
      "step": 136
    },
    {
      "epoch": 0.02560987008131601,
      "grad_norm": 2.353330373764038,
      "learning_rate": 0.0001898317757009346,
      "loss": 0.4697,
      "step": 137
    },
    {
      "epoch": 0.02579680343957379,
      "grad_norm": 3.8270626068115234,
      "learning_rate": 0.0001897570093457944,
      "loss": 0.5299,
      "step": 138
    },
    {
      "epoch": 0.025983736797831572,
      "grad_norm": 4.6418890953063965,
      "learning_rate": 0.00018968224299065423,
      "loss": 0.6091,
      "step": 139
    },
    {
      "epoch": 0.026170670156089353,
      "grad_norm": 3.3542139530181885,
      "learning_rate": 0.00018960747663551405,
      "loss": 0.4838,
      "step": 140
    },
    {
      "epoch": 0.026357603514347135,
      "grad_norm": 4.103880405426025,
      "learning_rate": 0.00018953271028037384,
      "loss": 0.5096,
      "step": 141
    },
    {
      "epoch": 0.026544536872604916,
      "grad_norm": 5.365690231323242,
      "learning_rate": 0.00018945794392523366,
      "loss": 0.5032,
      "step": 142
    },
    {
      "epoch": 0.026731470230862697,
      "grad_norm": 3.8814780712127686,
      "learning_rate": 0.00018938317757009348,
      "loss": 0.5974,
      "step": 143
    },
    {
      "epoch": 0.02691840358912048,
      "grad_norm": 3.1503920555114746,
      "learning_rate": 0.0001893084112149533,
      "loss": 0.6372,
      "step": 144
    },
    {
      "epoch": 0.02710533694737826,
      "grad_norm": 4.192237377166748,
      "learning_rate": 0.00018923364485981309,
      "loss": 0.6094,
      "step": 145
    },
    {
      "epoch": 0.02729227030563604,
      "grad_norm": 4.916670799255371,
      "learning_rate": 0.0001891588785046729,
      "loss": 0.4386,
      "step": 146
    },
    {
      "epoch": 0.027479203663893823,
      "grad_norm": 4.141907215118408,
      "learning_rate": 0.00018908411214953272,
      "loss": 0.5827,
      "step": 147
    },
    {
      "epoch": 0.027666137022151604,
      "grad_norm": 3.7497100830078125,
      "learning_rate": 0.00018900934579439254,
      "loss": 0.4954,
      "step": 148
    },
    {
      "epoch": 0.027853070380409385,
      "grad_norm": 7.971512317657471,
      "learning_rate": 0.00018893457943925233,
      "loss": 0.5342,
      "step": 149
    },
    {
      "epoch": 0.028040003738667166,
      "grad_norm": 4.982323169708252,
      "learning_rate": 0.00018885981308411215,
      "loss": 0.5168,
      "step": 150
    },
    {
      "epoch": 0.028226937096924948,
      "grad_norm": 4.363846778869629,
      "learning_rate": 0.00018878504672897197,
      "loss": 0.5567,
      "step": 151
    },
    {
      "epoch": 0.028413870455182726,
      "grad_norm": 4.827208995819092,
      "learning_rate": 0.0001887102803738318,
      "loss": 0.6575,
      "step": 152
    },
    {
      "epoch": 0.028600803813440507,
      "grad_norm": 5.249680995941162,
      "learning_rate": 0.00018863551401869158,
      "loss": 0.6215,
      "step": 153
    },
    {
      "epoch": 0.02878773717169829,
      "grad_norm": 4.113430500030518,
      "learning_rate": 0.0001885607476635514,
      "loss": 0.4392,
      "step": 154
    },
    {
      "epoch": 0.02897467052995607,
      "grad_norm": 3.2222490310668945,
      "learning_rate": 0.00018848598130841122,
      "loss": 0.4461,
      "step": 155
    },
    {
      "epoch": 0.02916160388821385,
      "grad_norm": 7.929064750671387,
      "learning_rate": 0.00018841121495327104,
      "loss": 0.5274,
      "step": 156
    },
    {
      "epoch": 0.029348537246471632,
      "grad_norm": 4.014432907104492,
      "learning_rate": 0.00018833644859813083,
      "loss": 0.4462,
      "step": 157
    },
    {
      "epoch": 0.029535470604729414,
      "grad_norm": 3.3105194568634033,
      "learning_rate": 0.00018826168224299065,
      "loss": 0.5757,
      "step": 158
    },
    {
      "epoch": 0.029722403962987195,
      "grad_norm": 3.872420310974121,
      "learning_rate": 0.00018818691588785047,
      "loss": 0.5856,
      "step": 159
    },
    {
      "epoch": 0.029909337321244976,
      "grad_norm": 4.675248622894287,
      "learning_rate": 0.0001881121495327103,
      "loss": 0.5303,
      "step": 160
    },
    {
      "epoch": 0.030096270679502758,
      "grad_norm": 4.090839862823486,
      "learning_rate": 0.00018803738317757008,
      "loss": 0.5371,
      "step": 161
    },
    {
      "epoch": 0.03028320403776054,
      "grad_norm": 6.386361122131348,
      "learning_rate": 0.00018796261682242993,
      "loss": 0.5651,
      "step": 162
    },
    {
      "epoch": 0.03047013739601832,
      "grad_norm": 4.60520076751709,
      "learning_rate": 0.00018788785046728974,
      "loss": 0.5531,
      "step": 163
    },
    {
      "epoch": 0.0306570707542761,
      "grad_norm": 3.373316526412964,
      "learning_rate": 0.00018781308411214956,
      "loss": 0.4617,
      "step": 164
    },
    {
      "epoch": 0.030844004112533883,
      "grad_norm": 5.03005838394165,
      "learning_rate": 0.00018773831775700935,
      "loss": 0.6303,
      "step": 165
    },
    {
      "epoch": 0.031030937470791664,
      "grad_norm": 4.059988498687744,
      "learning_rate": 0.00018766355140186917,
      "loss": 0.4245,
      "step": 166
    },
    {
      "epoch": 0.031217870829049445,
      "grad_norm": 3.749427556991577,
      "learning_rate": 0.000187588785046729,
      "loss": 0.4622,
      "step": 167
    },
    {
      "epoch": 0.03140480418730723,
      "grad_norm": 4.46465539932251,
      "learning_rate": 0.0001875140186915888,
      "loss": 0.5867,
      "step": 168
    },
    {
      "epoch": 0.031591737545565005,
      "grad_norm": 5.660756587982178,
      "learning_rate": 0.0001874392523364486,
      "loss": 0.5299,
      "step": 169
    },
    {
      "epoch": 0.03177867090382279,
      "grad_norm": 2.850865125656128,
      "learning_rate": 0.00018736448598130842,
      "loss": 0.4941,
      "step": 170
    },
    {
      "epoch": 0.03196560426208057,
      "grad_norm": 4.939967632293701,
      "learning_rate": 0.00018728971962616824,
      "loss": 0.5284,
      "step": 171
    },
    {
      "epoch": 0.03215253762033835,
      "grad_norm": 4.779682159423828,
      "learning_rate": 0.00018721495327102806,
      "loss": 0.4682,
      "step": 172
    },
    {
      "epoch": 0.03233947097859613,
      "grad_norm": 2.557558298110962,
      "learning_rate": 0.00018714018691588785,
      "loss": 0.6095,
      "step": 173
    },
    {
      "epoch": 0.032526404336853915,
      "grad_norm": 5.15502405166626,
      "learning_rate": 0.00018706542056074767,
      "loss": 0.4115,
      "step": 174
    },
    {
      "epoch": 0.03271333769511169,
      "grad_norm": 3.943389415740967,
      "learning_rate": 0.0001869906542056075,
      "loss": 0.4783,
      "step": 175
    },
    {
      "epoch": 0.03290027105336947,
      "grad_norm": 3.294552803039551,
      "learning_rate": 0.0001869158878504673,
      "loss": 0.4318,
      "step": 176
    },
    {
      "epoch": 0.033087204411627255,
      "grad_norm": 4.979494571685791,
      "learning_rate": 0.0001868411214953271,
      "loss": 0.5178,
      "step": 177
    },
    {
      "epoch": 0.03327413776988503,
      "grad_norm": 3.102705478668213,
      "learning_rate": 0.00018676635514018692,
      "loss": 0.3733,
      "step": 178
    },
    {
      "epoch": 0.03346107112814282,
      "grad_norm": 3.927077293395996,
      "learning_rate": 0.00018669158878504674,
      "loss": 0.3428,
      "step": 179
    },
    {
      "epoch": 0.033648004486400596,
      "grad_norm": 5.847337245941162,
      "learning_rate": 0.00018661682242990656,
      "loss": 0.4739,
      "step": 180
    },
    {
      "epoch": 0.03383493784465838,
      "grad_norm": 5.5182108879089355,
      "learning_rate": 0.00018654205607476635,
      "loss": 0.441,
      "step": 181
    },
    {
      "epoch": 0.03402187120291616,
      "grad_norm": 3.7003777027130127,
      "learning_rate": 0.00018646728971962617,
      "loss": 0.4793,
      "step": 182
    },
    {
      "epoch": 0.03420880456117394,
      "grad_norm": 4.969820976257324,
      "learning_rate": 0.00018639252336448599,
      "loss": 0.4175,
      "step": 183
    },
    {
      "epoch": 0.03439573791943172,
      "grad_norm": 4.381360054016113,
      "learning_rate": 0.0001863177570093458,
      "loss": 0.4075,
      "step": 184
    },
    {
      "epoch": 0.034582671277689506,
      "grad_norm": 5.948050498962402,
      "learning_rate": 0.0001862429906542056,
      "loss": 0.5327,
      "step": 185
    },
    {
      "epoch": 0.034769604635947284,
      "grad_norm": 4.046971797943115,
      "learning_rate": 0.00018616822429906542,
      "loss": 0.4907,
      "step": 186
    },
    {
      "epoch": 0.03495653799420507,
      "grad_norm": 4.813292503356934,
      "learning_rate": 0.00018609345794392526,
      "loss": 0.4293,
      "step": 187
    },
    {
      "epoch": 0.035143471352462846,
      "grad_norm": 5.8612895011901855,
      "learning_rate": 0.00018601869158878505,
      "loss": 0.3684,
      "step": 188
    },
    {
      "epoch": 0.03533040471072063,
      "grad_norm": 5.0187177658081055,
      "learning_rate": 0.00018594392523364487,
      "loss": 0.5085,
      "step": 189
    },
    {
      "epoch": 0.03551733806897841,
      "grad_norm": 3.6480393409729004,
      "learning_rate": 0.0001858691588785047,
      "loss": 0.3422,
      "step": 190
    },
    {
      "epoch": 0.035704271427236194,
      "grad_norm": 5.694571018218994,
      "learning_rate": 0.0001857943925233645,
      "loss": 0.545,
      "step": 191
    },
    {
      "epoch": 0.03589120478549397,
      "grad_norm": 4.133347988128662,
      "learning_rate": 0.0001857196261682243,
      "loss": 0.4335,
      "step": 192
    },
    {
      "epoch": 0.03607813814375175,
      "grad_norm": 5.610676288604736,
      "learning_rate": 0.00018564485981308412,
      "loss": 0.4262,
      "step": 193
    },
    {
      "epoch": 0.036265071502009534,
      "grad_norm": 4.992023944854736,
      "learning_rate": 0.00018557009345794394,
      "loss": 0.4932,
      "step": 194
    },
    {
      "epoch": 0.03645200486026731,
      "grad_norm": 4.621827602386475,
      "learning_rate": 0.00018549532710280376,
      "loss": 0.3379,
      "step": 195
    },
    {
      "epoch": 0.0366389382185251,
      "grad_norm": 5.589760780334473,
      "learning_rate": 0.00018542056074766355,
      "loss": 0.6044,
      "step": 196
    },
    {
      "epoch": 0.036825871576782875,
      "grad_norm": 3.0526556968688965,
      "learning_rate": 0.00018534579439252337,
      "loss": 0.3505,
      "step": 197
    },
    {
      "epoch": 0.03701280493504066,
      "grad_norm": 3.519707202911377,
      "learning_rate": 0.0001852710280373832,
      "loss": 0.3327,
      "step": 198
    },
    {
      "epoch": 0.03719973829329844,
      "grad_norm": 4.181057453155518,
      "learning_rate": 0.000185196261682243,
      "loss": 0.3829,
      "step": 199
    },
    {
      "epoch": 0.03738667165155622,
      "grad_norm": 3.402177572250366,
      "learning_rate": 0.0001851214953271028,
      "loss": 0.463,
      "step": 200
    },
    {
      "epoch": 0.037573605009814,
      "grad_norm": 4.273218154907227,
      "learning_rate": 0.00018504672897196262,
      "loss": 0.4838,
      "step": 201
    },
    {
      "epoch": 0.037760538368071785,
      "grad_norm": 3.1149027347564697,
      "learning_rate": 0.00018497196261682244,
      "loss": 0.4313,
      "step": 202
    },
    {
      "epoch": 0.03794747172632956,
      "grad_norm": 5.076042652130127,
      "learning_rate": 0.00018489719626168226,
      "loss": 0.3828,
      "step": 203
    },
    {
      "epoch": 0.03813440508458735,
      "grad_norm": 4.125304222106934,
      "learning_rate": 0.00018482242990654207,
      "loss": 0.5016,
      "step": 204
    },
    {
      "epoch": 0.038321338442845125,
      "grad_norm": 6.3181047439575195,
      "learning_rate": 0.00018474766355140187,
      "loss": 0.6517,
      "step": 205
    },
    {
      "epoch": 0.03850827180110291,
      "grad_norm": 4.2493510246276855,
      "learning_rate": 0.00018467289719626168,
      "loss": 0.3892,
      "step": 206
    },
    {
      "epoch": 0.03869520515936069,
      "grad_norm": 3.732041358947754,
      "learning_rate": 0.0001845981308411215,
      "loss": 0.4894,
      "step": 207
    },
    {
      "epoch": 0.038882138517618466,
      "grad_norm": 4.542972087860107,
      "learning_rate": 0.00018452336448598132,
      "loss": 0.4197,
      "step": 208
    },
    {
      "epoch": 0.03906907187587625,
      "grad_norm": 4.759786128997803,
      "learning_rate": 0.00018444859813084111,
      "loss": 0.4519,
      "step": 209
    },
    {
      "epoch": 0.03925600523413403,
      "grad_norm": 4.615501403808594,
      "learning_rate": 0.00018437383177570093,
      "loss": 0.4114,
      "step": 210
    },
    {
      "epoch": 0.03944293859239181,
      "grad_norm": 5.313327312469482,
      "learning_rate": 0.00018429906542056075,
      "loss": 0.4076,
      "step": 211
    },
    {
      "epoch": 0.03962987195064959,
      "grad_norm": 7.716803550720215,
      "learning_rate": 0.00018422429906542057,
      "loss": 0.4553,
      "step": 212
    },
    {
      "epoch": 0.039816805308907376,
      "grad_norm": 5.09454345703125,
      "learning_rate": 0.0001841495327102804,
      "loss": 0.3866,
      "step": 213
    },
    {
      "epoch": 0.040003738667165153,
      "grad_norm": 4.411536693572998,
      "learning_rate": 0.0001840747663551402,
      "loss": 0.3431,
      "step": 214
    },
    {
      "epoch": 0.04019067202542294,
      "grad_norm": 5.28104305267334,
      "learning_rate": 0.00018400000000000003,
      "loss": 0.4711,
      "step": 215
    },
    {
      "epoch": 0.040377605383680716,
      "grad_norm": 4.754611492156982,
      "learning_rate": 0.00018392523364485982,
      "loss": 0.5038,
      "step": 216
    },
    {
      "epoch": 0.0405645387419385,
      "grad_norm": 4.534412384033203,
      "learning_rate": 0.00018385046728971964,
      "loss": 0.6134,
      "step": 217
    },
    {
      "epoch": 0.04075147210019628,
      "grad_norm": 5.649053573608398,
      "learning_rate": 0.00018377570093457946,
      "loss": 0.3731,
      "step": 218
    },
    {
      "epoch": 0.040938405458454064,
      "grad_norm": 4.950375556945801,
      "learning_rate": 0.00018370093457943928,
      "loss": 0.4108,
      "step": 219
    },
    {
      "epoch": 0.04112533881671184,
      "grad_norm": 3.938021659851074,
      "learning_rate": 0.00018362616822429907,
      "loss": 0.5444,
      "step": 220
    },
    {
      "epoch": 0.041312272174969626,
      "grad_norm": 4.4604811668396,
      "learning_rate": 0.00018355140186915889,
      "loss": 0.4111,
      "step": 221
    },
    {
      "epoch": 0.041499205533227404,
      "grad_norm": 4.5451459884643555,
      "learning_rate": 0.0001834766355140187,
      "loss": 0.4773,
      "step": 222
    },
    {
      "epoch": 0.04168613889148519,
      "grad_norm": 2.587562322616577,
      "learning_rate": 0.00018340186915887852,
      "loss": 0.3154,
      "step": 223
    },
    {
      "epoch": 0.04187307224974297,
      "grad_norm": 2.9429731369018555,
      "learning_rate": 0.00018332710280373832,
      "loss": 0.423,
      "step": 224
    },
    {
      "epoch": 0.042060005608000745,
      "grad_norm": 3.4301257133483887,
      "learning_rate": 0.00018325233644859813,
      "loss": 0.3377,
      "step": 225
    },
    {
      "epoch": 0.04224693896625853,
      "grad_norm": 2.5411202907562256,
      "learning_rate": 0.00018317757009345795,
      "loss": 0.3832,
      "step": 226
    },
    {
      "epoch": 0.04243387232451631,
      "grad_norm": 3.587700128555298,
      "learning_rate": 0.00018310280373831777,
      "loss": 0.3809,
      "step": 227
    },
    {
      "epoch": 0.04262080568277409,
      "grad_norm": 3.4783921241760254,
      "learning_rate": 0.00018302803738317756,
      "loss": 0.4036,
      "step": 228
    },
    {
      "epoch": 0.04280773904103187,
      "grad_norm": 5.663629531860352,
      "learning_rate": 0.00018295327102803738,
      "loss": 0.568,
      "step": 229
    },
    {
      "epoch": 0.042994672399289655,
      "grad_norm": 3.9513256549835205,
      "learning_rate": 0.0001828785046728972,
      "loss": 0.4975,
      "step": 230
    },
    {
      "epoch": 0.04318160575754743,
      "grad_norm": 6.135684967041016,
      "learning_rate": 0.00018280373831775702,
      "loss": 0.5997,
      "step": 231
    },
    {
      "epoch": 0.04336853911580522,
      "grad_norm": 3.364591121673584,
      "learning_rate": 0.0001827289719626168,
      "loss": 0.3077,
      "step": 232
    },
    {
      "epoch": 0.043555472474062995,
      "grad_norm": 4.272729873657227,
      "learning_rate": 0.00018265420560747663,
      "loss": 0.3921,
      "step": 233
    },
    {
      "epoch": 0.04374240583232078,
      "grad_norm": 3.4372458457946777,
      "learning_rate": 0.00018257943925233645,
      "loss": 0.4117,
      "step": 234
    },
    {
      "epoch": 0.04392933919057856,
      "grad_norm": 6.0082011222839355,
      "learning_rate": 0.00018250467289719627,
      "loss": 0.5592,
      "step": 235
    },
    {
      "epoch": 0.04411627254883634,
      "grad_norm": 4.537705421447754,
      "learning_rate": 0.00018242990654205606,
      "loss": 0.4316,
      "step": 236
    },
    {
      "epoch": 0.04430320590709412,
      "grad_norm": 4.445235729217529,
      "learning_rate": 0.0001823551401869159,
      "loss": 0.3243,
      "step": 237
    },
    {
      "epoch": 0.044490139265351905,
      "grad_norm": 7.875504016876221,
      "learning_rate": 0.00018228037383177573,
      "loss": 0.4518,
      "step": 238
    },
    {
      "epoch": 0.04467707262360968,
      "grad_norm": 4.035912990570068,
      "learning_rate": 0.00018220560747663554,
      "loss": 0.3941,
      "step": 239
    },
    {
      "epoch": 0.04486400598186746,
      "grad_norm": 4.440282821655273,
      "learning_rate": 0.00018213084112149534,
      "loss": 0.2968,
      "step": 240
    },
    {
      "epoch": 0.045050939340125246,
      "grad_norm": 8.53770637512207,
      "learning_rate": 0.00018205607476635516,
      "loss": 0.6337,
      "step": 241
    },
    {
      "epoch": 0.04523787269838302,
      "grad_norm": 4.23185920715332,
      "learning_rate": 0.00018198130841121497,
      "loss": 0.3925,
      "step": 242
    },
    {
      "epoch": 0.04542480605664081,
      "grad_norm": 5.203549861907959,
      "learning_rate": 0.0001819065420560748,
      "loss": 0.4665,
      "step": 243
    },
    {
      "epoch": 0.045611739414898586,
      "grad_norm": 5.167134761810303,
      "learning_rate": 0.00018183177570093458,
      "loss": 0.5228,
      "step": 244
    },
    {
      "epoch": 0.04579867277315637,
      "grad_norm": 3.551952600479126,
      "learning_rate": 0.0001817570093457944,
      "loss": 0.3438,
      "step": 245
    },
    {
      "epoch": 0.04598560613141415,
      "grad_norm": 4.097814083099365,
      "learning_rate": 0.00018168224299065422,
      "loss": 0.3791,
      "step": 246
    },
    {
      "epoch": 0.046172539489671933,
      "grad_norm": 3.4993948936462402,
      "learning_rate": 0.00018160747663551404,
      "loss": 0.2797,
      "step": 247
    },
    {
      "epoch": 0.04635947284792971,
      "grad_norm": 2.6259148120880127,
      "learning_rate": 0.00018153271028037383,
      "loss": 0.3649,
      "step": 248
    },
    {
      "epoch": 0.046546406206187496,
      "grad_norm": 6.457727909088135,
      "learning_rate": 0.00018145794392523365,
      "loss": 0.4146,
      "step": 249
    },
    {
      "epoch": 0.046733339564445274,
      "grad_norm": 3.965266227722168,
      "learning_rate": 0.00018138317757009347,
      "loss": 0.3797,
      "step": 250
    },
    {
      "epoch": 0.04692027292270306,
      "grad_norm": 4.05739688873291,
      "learning_rate": 0.0001813084112149533,
      "loss": 0.3352,
      "step": 251
    },
    {
      "epoch": 0.04710720628096084,
      "grad_norm": 4.082630157470703,
      "learning_rate": 0.00018123364485981308,
      "loss": 0.3463,
      "step": 252
    },
    {
      "epoch": 0.04729413963921862,
      "grad_norm": 7.7636399269104,
      "learning_rate": 0.0001811588785046729,
      "loss": 0.583,
      "step": 253
    },
    {
      "epoch": 0.0474810729974764,
      "grad_norm": 4.207523345947266,
      "learning_rate": 0.00018108411214953272,
      "loss": 0.391,
      "step": 254
    },
    {
      "epoch": 0.047668006355734184,
      "grad_norm": 3.0764119625091553,
      "learning_rate": 0.00018100934579439254,
      "loss": 0.2602,
      "step": 255
    },
    {
      "epoch": 0.04785493971399196,
      "grad_norm": 2.9833850860595703,
      "learning_rate": 0.00018093457943925233,
      "loss": 0.2827,
      "step": 256
    },
    {
      "epoch": 0.04804187307224974,
      "grad_norm": 6.416106700897217,
      "learning_rate": 0.00018085981308411215,
      "loss": 0.4329,
      "step": 257
    },
    {
      "epoch": 0.048228806430507525,
      "grad_norm": 5.169780731201172,
      "learning_rate": 0.00018078504672897197,
      "loss": 0.288,
      "step": 258
    },
    {
      "epoch": 0.0484157397887653,
      "grad_norm": 7.370294094085693,
      "learning_rate": 0.00018071028037383179,
      "loss": 0.4389,
      "step": 259
    },
    {
      "epoch": 0.04860267314702309,
      "grad_norm": 4.510062217712402,
      "learning_rate": 0.00018063551401869158,
      "loss": 0.4416,
      "step": 260
    },
    {
      "epoch": 0.048789606505280865,
      "grad_norm": 7.656269073486328,
      "learning_rate": 0.0001805607476635514,
      "loss": 0.3342,
      "step": 261
    },
    {
      "epoch": 0.04897653986353865,
      "grad_norm": 8.384500503540039,
      "learning_rate": 0.00018048598130841124,
      "loss": 0.5398,
      "step": 262
    },
    {
      "epoch": 0.04916347322179643,
      "grad_norm": 4.690392971038818,
      "learning_rate": 0.00018041121495327103,
      "loss": 0.2311,
      "step": 263
    },
    {
      "epoch": 0.04935040658005421,
      "grad_norm": 5.262564182281494,
      "learning_rate": 0.00018033644859813085,
      "loss": 0.4856,
      "step": 264
    },
    {
      "epoch": 0.04953733993831199,
      "grad_norm": 7.76027774810791,
      "learning_rate": 0.00018026168224299067,
      "loss": 0.5298,
      "step": 265
    },
    {
      "epoch": 0.049724273296569775,
      "grad_norm": 5.235107421875,
      "learning_rate": 0.0001801869158878505,
      "loss": 0.5249,
      "step": 266
    },
    {
      "epoch": 0.04991120665482755,
      "grad_norm": 4.082423686981201,
      "learning_rate": 0.00018011214953271028,
      "loss": 0.4685,
      "step": 267
    },
    {
      "epoch": 0.05009814001308534,
      "grad_norm": 4.779214382171631,
      "learning_rate": 0.0001800373831775701,
      "loss": 0.3916,
      "step": 268
    },
    {
      "epoch": 0.050285073371343116,
      "grad_norm": 2.8122055530548096,
      "learning_rate": 0.00017996261682242992,
      "loss": 0.4249,
      "step": 269
    },
    {
      "epoch": 0.0504720067296009,
      "grad_norm": 3.5652360916137695,
      "learning_rate": 0.00017988785046728974,
      "loss": 0.2627,
      "step": 270
    },
    {
      "epoch": 0.05065894008785868,
      "grad_norm": 9.134478569030762,
      "learning_rate": 0.00017981308411214953,
      "loss": 0.3503,
      "step": 271
    },
    {
      "epoch": 0.050845873446116456,
      "grad_norm": 2.381563663482666,
      "learning_rate": 0.00017973831775700935,
      "loss": 0.2924,
      "step": 272
    },
    {
      "epoch": 0.05103280680437424,
      "grad_norm": 5.9050984382629395,
      "learning_rate": 0.00017966355140186917,
      "loss": 0.4995,
      "step": 273
    },
    {
      "epoch": 0.05121974016263202,
      "grad_norm": 3.191232204437256,
      "learning_rate": 0.000179588785046729,
      "loss": 0.3963,
      "step": 274
    },
    {
      "epoch": 0.0514066735208898,
      "grad_norm": 4.345893859863281,
      "learning_rate": 0.00017951401869158878,
      "loss": 0.3704,
      "step": 275
    },
    {
      "epoch": 0.05159360687914758,
      "grad_norm": 4.141597747802734,
      "learning_rate": 0.0001794392523364486,
      "loss": 0.417,
      "step": 276
    },
    {
      "epoch": 0.051780540237405366,
      "grad_norm": 1.8435384035110474,
      "learning_rate": 0.00017936448598130842,
      "loss": 0.2583,
      "step": 277
    },
    {
      "epoch": 0.051967473595663144,
      "grad_norm": 3.9417688846588135,
      "learning_rate": 0.00017928971962616824,
      "loss": 0.4393,
      "step": 278
    },
    {
      "epoch": 0.05215440695392093,
      "grad_norm": 3.4254989624023438,
      "learning_rate": 0.00017921495327102803,
      "loss": 0.4907,
      "step": 279
    },
    {
      "epoch": 0.05234134031217871,
      "grad_norm": 2.343045234680176,
      "learning_rate": 0.00017914018691588785,
      "loss": 0.2363,
      "step": 280
    },
    {
      "epoch": 0.05252827367043649,
      "grad_norm": 2.1469357013702393,
      "learning_rate": 0.00017906542056074767,
      "loss": 0.2678,
      "step": 281
    },
    {
      "epoch": 0.05271520702869427,
      "grad_norm": 5.471090793609619,
      "learning_rate": 0.00017899065420560748,
      "loss": 0.3851,
      "step": 282
    },
    {
      "epoch": 0.052902140386952054,
      "grad_norm": 5.056649208068848,
      "learning_rate": 0.00017891588785046728,
      "loss": 0.2785,
      "step": 283
    },
    {
      "epoch": 0.05308907374520983,
      "grad_norm": 3.9778523445129395,
      "learning_rate": 0.0001788411214953271,
      "loss": 0.3143,
      "step": 284
    },
    {
      "epoch": 0.05327600710346762,
      "grad_norm": 4.590080261230469,
      "learning_rate": 0.00017876635514018691,
      "loss": 0.4602,
      "step": 285
    },
    {
      "epoch": 0.053462940461725394,
      "grad_norm": 6.33094596862793,
      "learning_rate": 0.00017869158878504676,
      "loss": 0.461,
      "step": 286
    },
    {
      "epoch": 0.05364987381998318,
      "grad_norm": 1.9101918935775757,
      "learning_rate": 0.00017861682242990655,
      "loss": 0.3068,
      "step": 287
    },
    {
      "epoch": 0.05383680717824096,
      "grad_norm": 3.541408061981201,
      "learning_rate": 0.00017854205607476637,
      "loss": 0.3006,
      "step": 288
    },
    {
      "epoch": 0.054023740536498735,
      "grad_norm": 3.2877509593963623,
      "learning_rate": 0.0001784672897196262,
      "loss": 0.2591,
      "step": 289
    },
    {
      "epoch": 0.05421067389475652,
      "grad_norm": 3.951772928237915,
      "learning_rate": 0.000178392523364486,
      "loss": 0.4209,
      "step": 290
    },
    {
      "epoch": 0.0543976072530143,
      "grad_norm": 4.431342124938965,
      "learning_rate": 0.0001783177570093458,
      "loss": 0.4529,
      "step": 291
    },
    {
      "epoch": 0.05458454061127208,
      "grad_norm": 3.5391128063201904,
      "learning_rate": 0.00017824299065420562,
      "loss": 0.3341,
      "step": 292
    },
    {
      "epoch": 0.05477147396952986,
      "grad_norm": 4.537031173706055,
      "learning_rate": 0.00017816822429906544,
      "loss": 0.4887,
      "step": 293
    },
    {
      "epoch": 0.054958407327787645,
      "grad_norm": 10.09101676940918,
      "learning_rate": 0.00017809345794392526,
      "loss": 0.349,
      "step": 294
    },
    {
      "epoch": 0.05514534068604542,
      "grad_norm": 6.178766250610352,
      "learning_rate": 0.00017801869158878505,
      "loss": 0.4149,
      "step": 295
    },
    {
      "epoch": 0.05533227404430321,
      "grad_norm": 10.205055236816406,
      "learning_rate": 0.00017794392523364487,
      "loss": 0.4192,
      "step": 296
    },
    {
      "epoch": 0.055519207402560986,
      "grad_norm": 9.622108459472656,
      "learning_rate": 0.00017786915887850469,
      "loss": 0.41,
      "step": 297
    },
    {
      "epoch": 0.05570614076081877,
      "grad_norm": 5.746344566345215,
      "learning_rate": 0.0001777943925233645,
      "loss": 0.3333,
      "step": 298
    },
    {
      "epoch": 0.05589307411907655,
      "grad_norm": 8.719905853271484,
      "learning_rate": 0.0001777196261682243,
      "loss": 0.5513,
      "step": 299
    },
    {
      "epoch": 0.05608000747733433,
      "grad_norm": 6.67329740524292,
      "learning_rate": 0.00017764485981308412,
      "loss": 0.4579,
      "step": 300
    },
    {
      "epoch": 0.05626694083559211,
      "grad_norm": 5.8861308097839355,
      "learning_rate": 0.00017757009345794393,
      "loss": 0.4165,
      "step": 301
    },
    {
      "epoch": 0.056453874193849896,
      "grad_norm": 6.4854416847229,
      "learning_rate": 0.00017749532710280375,
      "loss": 0.3755,
      "step": 302
    },
    {
      "epoch": 0.05664080755210767,
      "grad_norm": 5.834249019622803,
      "learning_rate": 0.00017742056074766355,
      "loss": 0.4253,
      "step": 303
    },
    {
      "epoch": 0.05682774091036545,
      "grad_norm": 7.771470069885254,
      "learning_rate": 0.00017734579439252336,
      "loss": 0.4547,
      "step": 304
    },
    {
      "epoch": 0.057014674268623236,
      "grad_norm": 3.6955955028533936,
      "learning_rate": 0.00017727102803738318,
      "loss": 0.3931,
      "step": 305
    },
    {
      "epoch": 0.057201607626881014,
      "grad_norm": 4.152834892272949,
      "learning_rate": 0.000177196261682243,
      "loss": 0.374,
      "step": 306
    },
    {
      "epoch": 0.0573885409851388,
      "grad_norm": 5.273102283477783,
      "learning_rate": 0.0001771214953271028,
      "loss": 0.4205,
      "step": 307
    },
    {
      "epoch": 0.05757547434339658,
      "grad_norm": 5.1931586265563965,
      "learning_rate": 0.0001770467289719626,
      "loss": 0.4332,
      "step": 308
    },
    {
      "epoch": 0.05776240770165436,
      "grad_norm": 2.8320705890655518,
      "learning_rate": 0.00017697196261682243,
      "loss": 0.2478,
      "step": 309
    },
    {
      "epoch": 0.05794934105991214,
      "grad_norm": 4.169552803039551,
      "learning_rate": 0.00017689719626168225,
      "loss": 0.3389,
      "step": 310
    },
    {
      "epoch": 0.058136274418169924,
      "grad_norm": 6.919758319854736,
      "learning_rate": 0.00017682242990654207,
      "loss": 0.3822,
      "step": 311
    },
    {
      "epoch": 0.0583232077764277,
      "grad_norm": 4.029165744781494,
      "learning_rate": 0.0001767476635514019,
      "loss": 0.3552,
      "step": 312
    },
    {
      "epoch": 0.05851014113468549,
      "grad_norm": 4.588530540466309,
      "learning_rate": 0.0001766728971962617,
      "loss": 0.4245,
      "step": 313
    },
    {
      "epoch": 0.058697074492943264,
      "grad_norm": 5.359803199768066,
      "learning_rate": 0.0001765981308411215,
      "loss": 0.5013,
      "step": 314
    },
    {
      "epoch": 0.05888400785120105,
      "grad_norm": 7.747561454772949,
      "learning_rate": 0.00017652336448598132,
      "loss": 0.3954,
      "step": 315
    },
    {
      "epoch": 0.05907094120945883,
      "grad_norm": 4.451800346374512,
      "learning_rate": 0.00017644859813084114,
      "loss": 0.2249,
      "step": 316
    },
    {
      "epoch": 0.05925787456771661,
      "grad_norm": 4.487832069396973,
      "learning_rate": 0.00017637383177570096,
      "loss": 0.3164,
      "step": 317
    },
    {
      "epoch": 0.05944480792597439,
      "grad_norm": 3.6529457569122314,
      "learning_rate": 0.00017629906542056075,
      "loss": 0.4146,
      "step": 318
    },
    {
      "epoch": 0.059631741284232175,
      "grad_norm": 4.434107303619385,
      "learning_rate": 0.00017622429906542057,
      "loss": 0.3523,
      "step": 319
    },
    {
      "epoch": 0.05981867464248995,
      "grad_norm": 7.0028581619262695,
      "learning_rate": 0.00017614953271028038,
      "loss": 0.3522,
      "step": 320
    },
    {
      "epoch": 0.06000560800074773,
      "grad_norm": 3.195767879486084,
      "learning_rate": 0.0001760747663551402,
      "loss": 0.2809,
      "step": 321
    },
    {
      "epoch": 0.060192541359005515,
      "grad_norm": 3.9821343421936035,
      "learning_rate": 0.00017600000000000002,
      "loss": 0.3493,
      "step": 322
    },
    {
      "epoch": 0.06037947471726329,
      "grad_norm": 6.778625011444092,
      "learning_rate": 0.00017592523364485981,
      "loss": 0.4182,
      "step": 323
    },
    {
      "epoch": 0.06056640807552108,
      "grad_norm": 4.867863655090332,
      "learning_rate": 0.00017585046728971963,
      "loss": 0.4346,
      "step": 324
    },
    {
      "epoch": 0.060753341433778855,
      "grad_norm": 5.062012195587158,
      "learning_rate": 0.00017577570093457945,
      "loss": 0.3638,
      "step": 325
    },
    {
      "epoch": 0.06094027479203664,
      "grad_norm": 2.4132118225097656,
      "learning_rate": 0.00017570093457943927,
      "loss": 0.2076,
      "step": 326
    },
    {
      "epoch": 0.06112720815029442,
      "grad_norm": 4.385496139526367,
      "learning_rate": 0.00017562616822429906,
      "loss": 0.3557,
      "step": 327
    },
    {
      "epoch": 0.0613141415085522,
      "grad_norm": 4.510688781738281,
      "learning_rate": 0.00017555140186915888,
      "loss": 0.3731,
      "step": 328
    },
    {
      "epoch": 0.06150107486680998,
      "grad_norm": 7.2152533531188965,
      "learning_rate": 0.0001754766355140187,
      "loss": 0.4051,
      "step": 329
    },
    {
      "epoch": 0.061688008225067766,
      "grad_norm": 6.283703804016113,
      "learning_rate": 0.00017540186915887852,
      "loss": 0.3641,
      "step": 330
    },
    {
      "epoch": 0.06187494158332554,
      "grad_norm": 4.631438255310059,
      "learning_rate": 0.0001753271028037383,
      "loss": 0.3595,
      "step": 331
    },
    {
      "epoch": 0.06206187494158333,
      "grad_norm": 5.908663272857666,
      "learning_rate": 0.00017525233644859813,
      "loss": 0.3794,
      "step": 332
    },
    {
      "epoch": 0.062248808299841106,
      "grad_norm": 4.685825824737549,
      "learning_rate": 0.00017517757009345795,
      "loss": 0.3654,
      "step": 333
    },
    {
      "epoch": 0.06243574165809889,
      "grad_norm": 4.565145492553711,
      "learning_rate": 0.00017510280373831777,
      "loss": 0.3048,
      "step": 334
    },
    {
      "epoch": 0.06262267501635667,
      "grad_norm": 2.98158860206604,
      "learning_rate": 0.00017502803738317756,
      "loss": 0.2538,
      "step": 335
    },
    {
      "epoch": 0.06280960837461445,
      "grad_norm": 9.395712852478027,
      "learning_rate": 0.0001749532710280374,
      "loss": 0.7456,
      "step": 336
    },
    {
      "epoch": 0.06299654173287222,
      "grad_norm": 5.565773010253906,
      "learning_rate": 0.00017487850467289722,
      "loss": 0.3706,
      "step": 337
    },
    {
      "epoch": 0.06318347509113001,
      "grad_norm": 6.046017646789551,
      "learning_rate": 0.00017480373831775702,
      "loss": 0.3553,
      "step": 338
    },
    {
      "epoch": 0.0633704084493878,
      "grad_norm": 3.3933660984039307,
      "learning_rate": 0.00017472897196261683,
      "loss": 0.3178,
      "step": 339
    },
    {
      "epoch": 0.06355734180764558,
      "grad_norm": 6.764980316162109,
      "learning_rate": 0.00017465420560747665,
      "loss": 0.2823,
      "step": 340
    },
    {
      "epoch": 0.06374427516590335,
      "grad_norm": 3.707158327102661,
      "learning_rate": 0.00017457943925233647,
      "loss": 0.3257,
      "step": 341
    },
    {
      "epoch": 0.06393120852416113,
      "grad_norm": 4.96585750579834,
      "learning_rate": 0.00017450467289719626,
      "loss": 0.3598,
      "step": 342
    },
    {
      "epoch": 0.06411814188241892,
      "grad_norm": 6.515915870666504,
      "learning_rate": 0.00017442990654205608,
      "loss": 0.3892,
      "step": 343
    },
    {
      "epoch": 0.0643050752406767,
      "grad_norm": 4.707850933074951,
      "learning_rate": 0.0001743551401869159,
      "loss": 0.2154,
      "step": 344
    },
    {
      "epoch": 0.06449200859893447,
      "grad_norm": 4.311240196228027,
      "learning_rate": 0.00017428037383177572,
      "loss": 0.3397,
      "step": 345
    },
    {
      "epoch": 0.06467894195719226,
      "grad_norm": 6.644630432128906,
      "learning_rate": 0.0001742056074766355,
      "loss": 0.4134,
      "step": 346
    },
    {
      "epoch": 0.06486587531545004,
      "grad_norm": 2.5529963970184326,
      "learning_rate": 0.00017413084112149533,
      "loss": 0.2092,
      "step": 347
    },
    {
      "epoch": 0.06505280867370783,
      "grad_norm": 3.378136157989502,
      "learning_rate": 0.00017405607476635515,
      "loss": 0.3923,
      "step": 348
    },
    {
      "epoch": 0.0652397420319656,
      "grad_norm": 3.8810245990753174,
      "learning_rate": 0.00017398130841121497,
      "loss": 0.4014,
      "step": 349
    },
    {
      "epoch": 0.06542667539022338,
      "grad_norm": 4.196625232696533,
      "learning_rate": 0.00017390654205607476,
      "loss": 0.3098,
      "step": 350
    },
    {
      "epoch": 0.06561360874848117,
      "grad_norm": 2.5681512355804443,
      "learning_rate": 0.00017383177570093458,
      "loss": 0.2941,
      "step": 351
    },
    {
      "epoch": 0.06580054210673894,
      "grad_norm": 3.6979305744171143,
      "learning_rate": 0.0001737570093457944,
      "loss": 0.5196,
      "step": 352
    },
    {
      "epoch": 0.06598747546499673,
      "grad_norm": 4.411864757537842,
      "learning_rate": 0.00017368224299065422,
      "loss": 0.3457,
      "step": 353
    },
    {
      "epoch": 0.06617440882325451,
      "grad_norm": 2.906892776489258,
      "learning_rate": 0.000173607476635514,
      "loss": 0.3496,
      "step": 354
    },
    {
      "epoch": 0.0663613421815123,
      "grad_norm": 3.2980616092681885,
      "learning_rate": 0.00017353271028037383,
      "loss": 0.4086,
      "step": 355
    },
    {
      "epoch": 0.06654827553977007,
      "grad_norm": 4.5339789390563965,
      "learning_rate": 0.00017345794392523365,
      "loss": 0.4152,
      "step": 356
    },
    {
      "epoch": 0.06673520889802785,
      "grad_norm": 2.7501025199890137,
      "learning_rate": 0.00017338317757009347,
      "loss": 0.2645,
      "step": 357
    },
    {
      "epoch": 0.06692214225628564,
      "grad_norm": 3.596066474914551,
      "learning_rate": 0.00017330841121495326,
      "loss": 0.3689,
      "step": 358
    },
    {
      "epoch": 0.06710907561454342,
      "grad_norm": 5.615121841430664,
      "learning_rate": 0.00017323364485981308,
      "loss": 0.4098,
      "step": 359
    },
    {
      "epoch": 0.06729600897280119,
      "grad_norm": 5.2368669509887695,
      "learning_rate": 0.0001731588785046729,
      "loss": 0.4322,
      "step": 360
    },
    {
      "epoch": 0.06748294233105898,
      "grad_norm": 4.185701370239258,
      "learning_rate": 0.00017308411214953274,
      "loss": 0.3441,
      "step": 361
    },
    {
      "epoch": 0.06766987568931676,
      "grad_norm": 4.123817443847656,
      "learning_rate": 0.00017300934579439253,
      "loss": 0.2426,
      "step": 362
    },
    {
      "epoch": 0.06785680904757455,
      "grad_norm": 3.60052490234375,
      "learning_rate": 0.00017293457943925235,
      "loss": 0.3275,
      "step": 363
    },
    {
      "epoch": 0.06804374240583232,
      "grad_norm": 8.551530838012695,
      "learning_rate": 0.00017285981308411217,
      "loss": 0.3602,
      "step": 364
    },
    {
      "epoch": 0.0682306757640901,
      "grad_norm": 4.365546226501465,
      "learning_rate": 0.000172785046728972,
      "loss": 0.2695,
      "step": 365
    },
    {
      "epoch": 0.06841760912234789,
      "grad_norm": 3.2274186611175537,
      "learning_rate": 0.00017271028037383178,
      "loss": 0.2521,
      "step": 366
    },
    {
      "epoch": 0.06860454248060567,
      "grad_norm": 3.7458956241607666,
      "learning_rate": 0.0001726355140186916,
      "loss": 0.3075,
      "step": 367
    },
    {
      "epoch": 0.06879147583886344,
      "grad_norm": 4.963877201080322,
      "learning_rate": 0.00017256074766355142,
      "loss": 0.3882,
      "step": 368
    },
    {
      "epoch": 0.06897840919712123,
      "grad_norm": 5.084437847137451,
      "learning_rate": 0.00017248598130841124,
      "loss": 0.3061,
      "step": 369
    },
    {
      "epoch": 0.06916534255537901,
      "grad_norm": 4.725696086883545,
      "learning_rate": 0.00017241121495327103,
      "loss": 0.48,
      "step": 370
    },
    {
      "epoch": 0.06935227591363678,
      "grad_norm": 4.805395126342773,
      "learning_rate": 0.00017233644859813085,
      "loss": 0.412,
      "step": 371
    },
    {
      "epoch": 0.06953920927189457,
      "grad_norm": 3.4867031574249268,
      "learning_rate": 0.00017226168224299067,
      "loss": 0.3354,
      "step": 372
    },
    {
      "epoch": 0.06972614263015235,
      "grad_norm": 5.291724681854248,
      "learning_rate": 0.0001721869158878505,
      "loss": 0.2937,
      "step": 373
    },
    {
      "epoch": 0.06991307598841014,
      "grad_norm": 3.4759576320648193,
      "learning_rate": 0.00017211214953271028,
      "loss": 0.2835,
      "step": 374
    },
    {
      "epoch": 0.07010000934666791,
      "grad_norm": 5.817439079284668,
      "learning_rate": 0.0001720373831775701,
      "loss": 0.386,
      "step": 375
    },
    {
      "epoch": 0.07028694270492569,
      "grad_norm": 6.9422287940979,
      "learning_rate": 0.00017196261682242992,
      "loss": 0.4151,
      "step": 376
    },
    {
      "epoch": 0.07047387606318348,
      "grad_norm": 3.8278796672821045,
      "learning_rate": 0.00017188785046728973,
      "loss": 0.2553,
      "step": 377
    },
    {
      "epoch": 0.07066080942144126,
      "grad_norm": 4.877851963043213,
      "learning_rate": 0.00017181308411214953,
      "loss": 0.3637,
      "step": 378
    },
    {
      "epoch": 0.07084774277969903,
      "grad_norm": 7.054345607757568,
      "learning_rate": 0.00017173831775700935,
      "loss": 0.4775,
      "step": 379
    },
    {
      "epoch": 0.07103467613795682,
      "grad_norm": 5.822596073150635,
      "learning_rate": 0.00017166355140186916,
      "loss": 0.3255,
      "step": 380
    },
    {
      "epoch": 0.0712216094962146,
      "grad_norm": 3.752624273300171,
      "learning_rate": 0.00017158878504672898,
      "loss": 0.2453,
      "step": 381
    },
    {
      "epoch": 0.07140854285447239,
      "grad_norm": 6.928903102874756,
      "learning_rate": 0.00017151401869158878,
      "loss": 0.4762,
      "step": 382
    },
    {
      "epoch": 0.07159547621273016,
      "grad_norm": 3.8825838565826416,
      "learning_rate": 0.0001714392523364486,
      "loss": 0.3747,
      "step": 383
    },
    {
      "epoch": 0.07178240957098794,
      "grad_norm": 3.1344969272613525,
      "learning_rate": 0.0001713644859813084,
      "loss": 0.322,
      "step": 384
    },
    {
      "epoch": 0.07196934292924573,
      "grad_norm": 6.406907558441162,
      "learning_rate": 0.00017128971962616823,
      "loss": 0.3284,
      "step": 385
    },
    {
      "epoch": 0.0721562762875035,
      "grad_norm": 5.948357105255127,
      "learning_rate": 0.00017121495327102805,
      "loss": 0.25,
      "step": 386
    },
    {
      "epoch": 0.07234320964576128,
      "grad_norm": 5.0916876792907715,
      "learning_rate": 0.00017114018691588787,
      "loss": 0.4216,
      "step": 387
    },
    {
      "epoch": 0.07253014300401907,
      "grad_norm": 5.716095447540283,
      "learning_rate": 0.0001710654205607477,
      "loss": 0.3935,
      "step": 388
    },
    {
      "epoch": 0.07271707636227685,
      "grad_norm": 3.8443078994750977,
      "learning_rate": 0.00017099065420560748,
      "loss": 0.2967,
      "step": 389
    },
    {
      "epoch": 0.07290400972053462,
      "grad_norm": 5.2402849197387695,
      "learning_rate": 0.0001709158878504673,
      "loss": 0.3297,
      "step": 390
    },
    {
      "epoch": 0.07309094307879241,
      "grad_norm": 4.076809406280518,
      "learning_rate": 0.00017084112149532712,
      "loss": 0.2697,
      "step": 391
    },
    {
      "epoch": 0.0732778764370502,
      "grad_norm": 3.2365152835845947,
      "learning_rate": 0.00017076635514018694,
      "loss": 0.1815,
      "step": 392
    },
    {
      "epoch": 0.07346480979530798,
      "grad_norm": 4.585525989532471,
      "learning_rate": 0.00017069158878504673,
      "loss": 0.3017,
      "step": 393
    },
    {
      "epoch": 0.07365174315356575,
      "grad_norm": 9.498068809509277,
      "learning_rate": 0.00017061682242990655,
      "loss": 0.5285,
      "step": 394
    },
    {
      "epoch": 0.07383867651182353,
      "grad_norm": 4.740981578826904,
      "learning_rate": 0.00017054205607476637,
      "loss": 0.4472,
      "step": 395
    },
    {
      "epoch": 0.07402560987008132,
      "grad_norm": 4.180441856384277,
      "learning_rate": 0.00017046728971962618,
      "loss": 0.2443,
      "step": 396
    },
    {
      "epoch": 0.0742125432283391,
      "grad_norm": 6.636492729187012,
      "learning_rate": 0.00017039252336448598,
      "loss": 0.4725,
      "step": 397
    },
    {
      "epoch": 0.07439947658659687,
      "grad_norm": 3.460273265838623,
      "learning_rate": 0.0001703177570093458,
      "loss": 0.3513,
      "step": 398
    },
    {
      "epoch": 0.07458640994485466,
      "grad_norm": 3.8025612831115723,
      "learning_rate": 0.00017024299065420561,
      "loss": 0.2877,
      "step": 399
    },
    {
      "epoch": 0.07477334330311244,
      "grad_norm": 3.7445571422576904,
      "learning_rate": 0.00017016822429906543,
      "loss": 0.3078,
      "step": 400
    },
    {
      "epoch": 0.07496027666137021,
      "grad_norm": 4.393806457519531,
      "learning_rate": 0.00017009345794392523,
      "loss": 0.3795,
      "step": 401
    },
    {
      "epoch": 0.075147210019628,
      "grad_norm": 2.7443454265594482,
      "learning_rate": 0.00017001869158878504,
      "loss": 0.3018,
      "step": 402
    },
    {
      "epoch": 0.07533414337788578,
      "grad_norm": 3.5824015140533447,
      "learning_rate": 0.00016994392523364486,
      "loss": 0.3543,
      "step": 403
    },
    {
      "epoch": 0.07552107673614357,
      "grad_norm": 2.643390655517578,
      "learning_rate": 0.00016986915887850468,
      "loss": 0.317,
      "step": 404
    },
    {
      "epoch": 0.07570801009440134,
      "grad_norm": 3.7169547080993652,
      "learning_rate": 0.0001697943925233645,
      "loss": 0.3376,
      "step": 405
    },
    {
      "epoch": 0.07589494345265912,
      "grad_norm": 3.586303234100342,
      "learning_rate": 0.0001697196261682243,
      "loss": 0.3699,
      "step": 406
    },
    {
      "epoch": 0.07608187681091691,
      "grad_norm": 2.89302396774292,
      "learning_rate": 0.0001696448598130841,
      "loss": 0.3634,
      "step": 407
    },
    {
      "epoch": 0.0762688101691747,
      "grad_norm": 4.02740478515625,
      "learning_rate": 0.00016957009345794393,
      "loss": 0.4032,
      "step": 408
    },
    {
      "epoch": 0.07645574352743247,
      "grad_norm": 4.785987854003906,
      "learning_rate": 0.00016949532710280375,
      "loss": 0.3935,
      "step": 409
    },
    {
      "epoch": 0.07664267688569025,
      "grad_norm": 3.3331494331359863,
      "learning_rate": 0.00016942056074766357,
      "loss": 0.3457,
      "step": 410
    },
    {
      "epoch": 0.07682961024394803,
      "grad_norm": 5.78428840637207,
      "learning_rate": 0.0001693457943925234,
      "loss": 0.3737,
      "step": 411
    },
    {
      "epoch": 0.07701654360220582,
      "grad_norm": 9.165288925170898,
      "learning_rate": 0.0001692710280373832,
      "loss": 0.5108,
      "step": 412
    },
    {
      "epoch": 0.07720347696046359,
      "grad_norm": 3.4763710498809814,
      "learning_rate": 0.000169196261682243,
      "loss": 0.3083,
      "step": 413
    },
    {
      "epoch": 0.07739041031872138,
      "grad_norm": 3.2431745529174805,
      "learning_rate": 0.00016912149532710282,
      "loss": 0.2594,
      "step": 414
    },
    {
      "epoch": 0.07757734367697916,
      "grad_norm": 5.140531063079834,
      "learning_rate": 0.00016904672897196263,
      "loss": 0.3029,
      "step": 415
    },
    {
      "epoch": 0.07776427703523693,
      "grad_norm": 2.2974281311035156,
      "learning_rate": 0.00016897196261682245,
      "loss": 0.3175,
      "step": 416
    },
    {
      "epoch": 0.07795121039349472,
      "grad_norm": 4.809028148651123,
      "learning_rate": 0.00016889719626168225,
      "loss": 0.3132,
      "step": 417
    },
    {
      "epoch": 0.0781381437517525,
      "grad_norm": 4.288883686065674,
      "learning_rate": 0.00016882242990654206,
      "loss": 0.284,
      "step": 418
    },
    {
      "epoch": 0.07832507711001029,
      "grad_norm": 3.0383331775665283,
      "learning_rate": 0.00016874766355140188,
      "loss": 0.352,
      "step": 419
    },
    {
      "epoch": 0.07851201046826806,
      "grad_norm": 4.7215962409973145,
      "learning_rate": 0.0001686728971962617,
      "loss": 0.4633,
      "step": 420
    },
    {
      "epoch": 0.07869894382652584,
      "grad_norm": 3.9112250804901123,
      "learning_rate": 0.0001685981308411215,
      "loss": 0.3482,
      "step": 421
    },
    {
      "epoch": 0.07888587718478363,
      "grad_norm": 2.698265790939331,
      "learning_rate": 0.0001685233644859813,
      "loss": 0.2541,
      "step": 422
    },
    {
      "epoch": 0.07907281054304141,
      "grad_norm": 3.729604721069336,
      "learning_rate": 0.00016844859813084113,
      "loss": 0.4032,
      "step": 423
    },
    {
      "epoch": 0.07925974390129918,
      "grad_norm": 2.9173688888549805,
      "learning_rate": 0.00016837383177570095,
      "loss": 0.368,
      "step": 424
    },
    {
      "epoch": 0.07944667725955697,
      "grad_norm": 4.629589557647705,
      "learning_rate": 0.00016829906542056074,
      "loss": 0.4089,
      "step": 425
    },
    {
      "epoch": 0.07963361061781475,
      "grad_norm": 4.680357933044434,
      "learning_rate": 0.00016822429906542056,
      "loss": 0.2423,
      "step": 426
    },
    {
      "epoch": 0.07982054397607254,
      "grad_norm": 4.124783039093018,
      "learning_rate": 0.00016814953271028038,
      "loss": 0.2113,
      "step": 427
    },
    {
      "epoch": 0.08000747733433031,
      "grad_norm": 5.649404048919678,
      "learning_rate": 0.0001680747663551402,
      "loss": 0.3299,
      "step": 428
    },
    {
      "epoch": 0.08019441069258809,
      "grad_norm": 7.526793479919434,
      "learning_rate": 0.000168,
      "loss": 0.3146,
      "step": 429
    },
    {
      "epoch": 0.08038134405084588,
      "grad_norm": 7.6197075843811035,
      "learning_rate": 0.0001679252336448598,
      "loss": 0.4405,
      "step": 430
    },
    {
      "epoch": 0.08056827740910366,
      "grad_norm": 3.8835742473602295,
      "learning_rate": 0.00016785046728971963,
      "loss": 0.2105,
      "step": 431
    },
    {
      "epoch": 0.08075521076736143,
      "grad_norm": 3.7018885612487793,
      "learning_rate": 0.00016777570093457945,
      "loss": 0.2359,
      "step": 432
    },
    {
      "epoch": 0.08094214412561922,
      "grad_norm": 7.004591464996338,
      "learning_rate": 0.00016770093457943924,
      "loss": 0.433,
      "step": 433
    },
    {
      "epoch": 0.081129077483877,
      "grad_norm": 4.060725688934326,
      "learning_rate": 0.00016762616822429906,
      "loss": 0.2495,
      "step": 434
    },
    {
      "epoch": 0.08131601084213477,
      "grad_norm": 5.36539888381958,
      "learning_rate": 0.0001675514018691589,
      "loss": 0.318,
      "step": 435
    },
    {
      "epoch": 0.08150294420039256,
      "grad_norm": 7.066064357757568,
      "learning_rate": 0.00016747663551401872,
      "loss": 0.5224,
      "step": 436
    },
    {
      "epoch": 0.08168987755865034,
      "grad_norm": 8.715592384338379,
      "learning_rate": 0.00016740186915887851,
      "loss": 0.4894,
      "step": 437
    },
    {
      "epoch": 0.08187681091690813,
      "grad_norm": 4.046661853790283,
      "learning_rate": 0.00016732710280373833,
      "loss": 0.2189,
      "step": 438
    },
    {
      "epoch": 0.0820637442751659,
      "grad_norm": 7.151280879974365,
      "learning_rate": 0.00016725233644859815,
      "loss": 0.3402,
      "step": 439
    },
    {
      "epoch": 0.08225067763342368,
      "grad_norm": 6.660279750823975,
      "learning_rate": 0.00016717757009345797,
      "loss": 0.383,
      "step": 440
    },
    {
      "epoch": 0.08243761099168147,
      "grad_norm": 2.5825469493865967,
      "learning_rate": 0.00016710280373831776,
      "loss": 0.3672,
      "step": 441
    },
    {
      "epoch": 0.08262454434993925,
      "grad_norm": 2.931867837905884,
      "learning_rate": 0.00016702803738317758,
      "loss": 0.1896,
      "step": 442
    },
    {
      "epoch": 0.08281147770819702,
      "grad_norm": 4.676253318786621,
      "learning_rate": 0.0001669532710280374,
      "loss": 0.2632,
      "step": 443
    },
    {
      "epoch": 0.08299841106645481,
      "grad_norm": 3.3697285652160645,
      "learning_rate": 0.00016687850467289722,
      "loss": 0.3962,
      "step": 444
    },
    {
      "epoch": 0.08318534442471259,
      "grad_norm": 2.667938232421875,
      "learning_rate": 0.000166803738317757,
      "loss": 0.2724,
      "step": 445
    },
    {
      "epoch": 0.08337227778297038,
      "grad_norm": 6.216400146484375,
      "learning_rate": 0.00016672897196261683,
      "loss": 0.4006,
      "step": 446
    },
    {
      "epoch": 0.08355921114122815,
      "grad_norm": 5.70839786529541,
      "learning_rate": 0.00016665420560747665,
      "loss": 0.358,
      "step": 447
    },
    {
      "epoch": 0.08374614449948593,
      "grad_norm": 6.530995845794678,
      "learning_rate": 0.00016657943925233647,
      "loss": 0.3066,
      "step": 448
    },
    {
      "epoch": 0.08393307785774372,
      "grad_norm": 4.717289924621582,
      "learning_rate": 0.00016650467289719626,
      "loss": 0.3002,
      "step": 449
    },
    {
      "epoch": 0.08412001121600149,
      "grad_norm": 4.875504493713379,
      "learning_rate": 0.00016642990654205608,
      "loss": 0.3976,
      "step": 450
    },
    {
      "epoch": 0.08430694457425927,
      "grad_norm": 3.638789176940918,
      "learning_rate": 0.0001663551401869159,
      "loss": 0.2813,
      "step": 451
    },
    {
      "epoch": 0.08449387793251706,
      "grad_norm": 5.240088939666748,
      "learning_rate": 0.00016628037383177572,
      "loss": 0.2543,
      "step": 452
    },
    {
      "epoch": 0.08468081129077484,
      "grad_norm": 7.601846694946289,
      "learning_rate": 0.0001662056074766355,
      "loss": 0.2892,
      "step": 453
    },
    {
      "epoch": 0.08486774464903261,
      "grad_norm": 10.071269035339355,
      "learning_rate": 0.00016613084112149533,
      "loss": 0.4047,
      "step": 454
    },
    {
      "epoch": 0.0850546780072904,
      "grad_norm": 3.962507486343384,
      "learning_rate": 0.00016605607476635515,
      "loss": 0.3401,
      "step": 455
    },
    {
      "epoch": 0.08524161136554818,
      "grad_norm": 3.6743392944335938,
      "learning_rate": 0.00016598130841121496,
      "loss": 0.2996,
      "step": 456
    },
    {
      "epoch": 0.08542854472380597,
      "grad_norm": 4.966899394989014,
      "learning_rate": 0.00016590654205607476,
      "loss": 0.2841,
      "step": 457
    },
    {
      "epoch": 0.08561547808206374,
      "grad_norm": 3.2095797061920166,
      "learning_rate": 0.00016583177570093458,
      "loss": 0.2471,
      "step": 458
    },
    {
      "epoch": 0.08580241144032152,
      "grad_norm": 2.6238067150115967,
      "learning_rate": 0.0001657570093457944,
      "loss": 0.2518,
      "step": 459
    },
    {
      "epoch": 0.08598934479857931,
      "grad_norm": 2.7951149940490723,
      "learning_rate": 0.0001656822429906542,
      "loss": 0.2213,
      "step": 460
    },
    {
      "epoch": 0.0861762781568371,
      "grad_norm": 5.378810405731201,
      "learning_rate": 0.00016560747663551403,
      "loss": 0.3486,
      "step": 461
    },
    {
      "epoch": 0.08636321151509486,
      "grad_norm": 5.531798839569092,
      "learning_rate": 0.00016553271028037385,
      "loss": 0.3992,
      "step": 462
    },
    {
      "epoch": 0.08655014487335265,
      "grad_norm": 6.523358345031738,
      "learning_rate": 0.00016545794392523367,
      "loss": 0.3369,
      "step": 463
    },
    {
      "epoch": 0.08673707823161043,
      "grad_norm": 5.659882068634033,
      "learning_rate": 0.00016538317757009346,
      "loss": 0.2441,
      "step": 464
    },
    {
      "epoch": 0.0869240115898682,
      "grad_norm": 4.516894340515137,
      "learning_rate": 0.00016530841121495328,
      "loss": 0.278,
      "step": 465
    },
    {
      "epoch": 0.08711094494812599,
      "grad_norm": 4.973386764526367,
      "learning_rate": 0.0001652336448598131,
      "loss": 0.2685,
      "step": 466
    },
    {
      "epoch": 0.08729787830638377,
      "grad_norm": 4.196166515350342,
      "learning_rate": 0.00016515887850467292,
      "loss": 0.3679,
      "step": 467
    },
    {
      "epoch": 0.08748481166464156,
      "grad_norm": 5.4756035804748535,
      "learning_rate": 0.0001650841121495327,
      "loss": 0.3746,
      "step": 468
    },
    {
      "epoch": 0.08767174502289933,
      "grad_norm": 5.627291679382324,
      "learning_rate": 0.00016500934579439253,
      "loss": 0.3203,
      "step": 469
    },
    {
      "epoch": 0.08785867838115712,
      "grad_norm": 6.577986240386963,
      "learning_rate": 0.00016493457943925235,
      "loss": 0.5344,
      "step": 470
    },
    {
      "epoch": 0.0880456117394149,
      "grad_norm": 4.522249698638916,
      "learning_rate": 0.00016485981308411217,
      "loss": 0.3192,
      "step": 471
    },
    {
      "epoch": 0.08823254509767268,
      "grad_norm": 4.225454807281494,
      "learning_rate": 0.00016478504672897196,
      "loss": 0.2381,
      "step": 472
    },
    {
      "epoch": 0.08841947845593046,
      "grad_norm": 3.143634557723999,
      "learning_rate": 0.00016471028037383178,
      "loss": 0.35,
      "step": 473
    },
    {
      "epoch": 0.08860641181418824,
      "grad_norm": 4.314452648162842,
      "learning_rate": 0.0001646355140186916,
      "loss": 0.3252,
      "step": 474
    },
    {
      "epoch": 0.08879334517244603,
      "grad_norm": 2.7376232147216797,
      "learning_rate": 0.00016456074766355141,
      "loss": 0.2206,
      "step": 475
    },
    {
      "epoch": 0.08898027853070381,
      "grad_norm": 3.266397714614868,
      "learning_rate": 0.0001644859813084112,
      "loss": 0.2412,
      "step": 476
    },
    {
      "epoch": 0.08916721188896158,
      "grad_norm": 3.974367141723633,
      "learning_rate": 0.00016441121495327103,
      "loss": 0.2664,
      "step": 477
    },
    {
      "epoch": 0.08935414524721937,
      "grad_norm": 4.302728652954102,
      "learning_rate": 0.00016433644859813084,
      "loss": 0.2851,
      "step": 478
    },
    {
      "epoch": 0.08954107860547715,
      "grad_norm": 3.8659348487854004,
      "learning_rate": 0.00016426168224299066,
      "loss": 0.3186,
      "step": 479
    },
    {
      "epoch": 0.08972801196373492,
      "grad_norm": 2.76822829246521,
      "learning_rate": 0.00016418691588785045,
      "loss": 0.1867,
      "step": 480
    },
    {
      "epoch": 0.0899149453219927,
      "grad_norm": 6.848794460296631,
      "learning_rate": 0.00016411214953271027,
      "loss": 0.332,
      "step": 481
    },
    {
      "epoch": 0.09010187868025049,
      "grad_norm": 4.492616176605225,
      "learning_rate": 0.0001640373831775701,
      "loss": 0.2015,
      "step": 482
    },
    {
      "epoch": 0.09028881203850828,
      "grad_norm": 5.943608283996582,
      "learning_rate": 0.0001639626168224299,
      "loss": 0.2593,
      "step": 483
    },
    {
      "epoch": 0.09047574539676605,
      "grad_norm": 3.9286348819732666,
      "learning_rate": 0.00016388785046728973,
      "loss": 0.2014,
      "step": 484
    },
    {
      "epoch": 0.09066267875502383,
      "grad_norm": 6.067765712738037,
      "learning_rate": 0.00016381308411214955,
      "loss": 0.3907,
      "step": 485
    },
    {
      "epoch": 0.09084961211328162,
      "grad_norm": 5.713895320892334,
      "learning_rate": 0.00016373831775700937,
      "loss": 0.1778,
      "step": 486
    },
    {
      "epoch": 0.0910365454715394,
      "grad_norm": 5.870315074920654,
      "learning_rate": 0.0001636635514018692,
      "loss": 0.2281,
      "step": 487
    },
    {
      "epoch": 0.09122347882979717,
      "grad_norm": 7.795034885406494,
      "learning_rate": 0.00016358878504672898,
      "loss": 0.2032,
      "step": 488
    },
    {
      "epoch": 0.09141041218805496,
      "grad_norm": 4.002894878387451,
      "learning_rate": 0.0001635140186915888,
      "loss": 0.1983,
      "step": 489
    },
    {
      "epoch": 0.09159734554631274,
      "grad_norm": 5.7626848220825195,
      "learning_rate": 0.00016343925233644862,
      "loss": 0.2957,
      "step": 490
    },
    {
      "epoch": 0.09178427890457053,
      "grad_norm": 6.201452255249023,
      "learning_rate": 0.00016336448598130844,
      "loss": 0.2472,
      "step": 491
    },
    {
      "epoch": 0.0919712122628283,
      "grad_norm": 11.506464958190918,
      "learning_rate": 0.00016328971962616823,
      "loss": 0.4127,
      "step": 492
    },
    {
      "epoch": 0.09215814562108608,
      "grad_norm": 2.6512396335601807,
      "learning_rate": 0.00016321495327102805,
      "loss": 0.1583,
      "step": 493
    },
    {
      "epoch": 0.09234507897934387,
      "grad_norm": 5.607067584991455,
      "learning_rate": 0.00016314018691588786,
      "loss": 0.2748,
      "step": 494
    },
    {
      "epoch": 0.09253201233760165,
      "grad_norm": 5.293304920196533,
      "learning_rate": 0.00016306542056074768,
      "loss": 0.3659,
      "step": 495
    },
    {
      "epoch": 0.09271894569585942,
      "grad_norm": 2.731952428817749,
      "learning_rate": 0.00016299065420560748,
      "loss": 0.138,
      "step": 496
    },
    {
      "epoch": 0.09290587905411721,
      "grad_norm": 3.491961717605591,
      "learning_rate": 0.0001629158878504673,
      "loss": 0.2547,
      "step": 497
    },
    {
      "epoch": 0.09309281241237499,
      "grad_norm": 4.041195392608643,
      "learning_rate": 0.0001628411214953271,
      "loss": 0.3628,
      "step": 498
    },
    {
      "epoch": 0.09327974577063276,
      "grad_norm": 3.086581230163574,
      "learning_rate": 0.00016276635514018693,
      "loss": 0.2656,
      "step": 499
    },
    {
      "epoch": 0.09346667912889055,
      "grad_norm": 3.268292188644409,
      "learning_rate": 0.00016269158878504672,
      "loss": 0.2383,
      "step": 500
    },
    {
      "epoch": 0.09365361248714833,
      "grad_norm": 2.6226143836975098,
      "learning_rate": 0.00016261682242990654,
      "loss": 0.2859,
      "step": 501
    },
    {
      "epoch": 0.09384054584540612,
      "grad_norm": 4.732573509216309,
      "learning_rate": 0.00016254205607476636,
      "loss": 0.2664,
      "step": 502
    },
    {
      "epoch": 0.09402747920366389,
      "grad_norm": 3.858949661254883,
      "learning_rate": 0.00016246728971962618,
      "loss": 0.3599,
      "step": 503
    },
    {
      "epoch": 0.09421441256192167,
      "grad_norm": 3.8198676109313965,
      "learning_rate": 0.00016239252336448597,
      "loss": 0.2189,
      "step": 504
    },
    {
      "epoch": 0.09440134592017946,
      "grad_norm": 2.6017470359802246,
      "learning_rate": 0.0001623177570093458,
      "loss": 0.1487,
      "step": 505
    },
    {
      "epoch": 0.09458827927843724,
      "grad_norm": 7.716111183166504,
      "learning_rate": 0.0001622429906542056,
      "loss": 0.2047,
      "step": 506
    },
    {
      "epoch": 0.09477521263669501,
      "grad_norm": 4.118503093719482,
      "learning_rate": 0.00016216822429906543,
      "loss": 0.2931,
      "step": 507
    },
    {
      "epoch": 0.0949621459949528,
      "grad_norm": 5.211121082305908,
      "learning_rate": 0.00016209345794392522,
      "loss": 0.3357,
      "step": 508
    },
    {
      "epoch": 0.09514907935321058,
      "grad_norm": 6.744927883148193,
      "learning_rate": 0.00016201869158878507,
      "loss": 0.4127,
      "step": 509
    },
    {
      "epoch": 0.09533601271146837,
      "grad_norm": 4.892672061920166,
      "learning_rate": 0.00016194392523364489,
      "loss": 0.3361,
      "step": 510
    },
    {
      "epoch": 0.09552294606972614,
      "grad_norm": 5.969621181488037,
      "learning_rate": 0.00016186915887850468,
      "loss": 0.2927,
      "step": 511
    },
    {
      "epoch": 0.09570987942798392,
      "grad_norm": 8.31759262084961,
      "learning_rate": 0.0001617943925233645,
      "loss": 0.3806,
      "step": 512
    },
    {
      "epoch": 0.09589681278624171,
      "grad_norm": 17.889131546020508,
      "learning_rate": 0.00016171962616822431,
      "loss": 0.5291,
      "step": 513
    },
    {
      "epoch": 0.09608374614449948,
      "grad_norm": 7.890864849090576,
      "learning_rate": 0.00016164485981308413,
      "loss": 0.6427,
      "step": 514
    },
    {
      "epoch": 0.09627067950275726,
      "grad_norm": 6.007721900939941,
      "learning_rate": 0.00016157009345794393,
      "loss": 0.3193,
      "step": 515
    },
    {
      "epoch": 0.09645761286101505,
      "grad_norm": 3.4380643367767334,
      "learning_rate": 0.00016149532710280374,
      "loss": 0.2681,
      "step": 516
    },
    {
      "epoch": 0.09664454621927283,
      "grad_norm": 5.097695827484131,
      "learning_rate": 0.00016142056074766356,
      "loss": 0.2292,
      "step": 517
    },
    {
      "epoch": 0.0968314795775306,
      "grad_norm": 5.033875465393066,
      "learning_rate": 0.00016134579439252338,
      "loss": 0.3615,
      "step": 518
    },
    {
      "epoch": 0.09701841293578839,
      "grad_norm": 3.5544896125793457,
      "learning_rate": 0.00016127102803738317,
      "loss": 0.1875,
      "step": 519
    },
    {
      "epoch": 0.09720534629404617,
      "grad_norm": 4.560766220092773,
      "learning_rate": 0.000161196261682243,
      "loss": 0.5604,
      "step": 520
    },
    {
      "epoch": 0.09739227965230396,
      "grad_norm": 4.60211706161499,
      "learning_rate": 0.0001611214953271028,
      "loss": 0.2702,
      "step": 521
    },
    {
      "epoch": 0.09757921301056173,
      "grad_norm": 3.492741346359253,
      "learning_rate": 0.00016104672897196263,
      "loss": 0.2945,
      "step": 522
    },
    {
      "epoch": 0.09776614636881951,
      "grad_norm": 8.132017135620117,
      "learning_rate": 0.00016097196261682245,
      "loss": 0.4082,
      "step": 523
    },
    {
      "epoch": 0.0979530797270773,
      "grad_norm": 3.426723003387451,
      "learning_rate": 0.00016089719626168224,
      "loss": 0.275,
      "step": 524
    },
    {
      "epoch": 0.09814001308533508,
      "grad_norm": 3.9181509017944336,
      "learning_rate": 0.00016082242990654206,
      "loss": 0.307,
      "step": 525
    },
    {
      "epoch": 0.09832694644359286,
      "grad_norm": 3.2813258171081543,
      "learning_rate": 0.00016074766355140188,
      "loss": 0.2542,
      "step": 526
    },
    {
      "epoch": 0.09851387980185064,
      "grad_norm": 7.391297340393066,
      "learning_rate": 0.0001606728971962617,
      "loss": 0.3757,
      "step": 527
    },
    {
      "epoch": 0.09870081316010842,
      "grad_norm": 8.34495735168457,
      "learning_rate": 0.0001605981308411215,
      "loss": 0.4472,
      "step": 528
    },
    {
      "epoch": 0.0988877465183662,
      "grad_norm": 3.6782736778259277,
      "learning_rate": 0.0001605233644859813,
      "loss": 0.2676,
      "step": 529
    },
    {
      "epoch": 0.09907467987662398,
      "grad_norm": 3.8802144527435303,
      "learning_rate": 0.00016044859813084113,
      "loss": 0.3316,
      "step": 530
    },
    {
      "epoch": 0.09926161323488177,
      "grad_norm": 3.616137742996216,
      "learning_rate": 0.00016037383177570095,
      "loss": 0.2434,
      "step": 531
    },
    {
      "epoch": 0.09944854659313955,
      "grad_norm": 5.330123424530029,
      "learning_rate": 0.00016029906542056074,
      "loss": 0.3028,
      "step": 532
    },
    {
      "epoch": 0.09963547995139732,
      "grad_norm": 4.578590393066406,
      "learning_rate": 0.00016022429906542056,
      "loss": 0.328,
      "step": 533
    },
    {
      "epoch": 0.0998224133096551,
      "grad_norm": 4.043977737426758,
      "learning_rate": 0.0001601495327102804,
      "loss": 0.4395,
      "step": 534
    },
    {
      "epoch": 0.10000934666791289,
      "grad_norm": 2.82239031791687,
      "learning_rate": 0.0001600747663551402,
      "loss": 0.2648,
      "step": 535
    },
    {
      "epoch": 0.10019628002617068,
      "grad_norm": 5.831345558166504,
      "learning_rate": 0.00016,
      "loss": 0.4314,
      "step": 536
    },
    {
      "epoch": 0.10038321338442845,
      "grad_norm": 3.0709011554718018,
      "learning_rate": 0.00015992523364485983,
      "loss": 0.3644,
      "step": 537
    },
    {
      "epoch": 0.10057014674268623,
      "grad_norm": 2.8498330116271973,
      "learning_rate": 0.00015985046728971965,
      "loss": 0.2401,
      "step": 538
    },
    {
      "epoch": 0.10075708010094402,
      "grad_norm": 7.1912126541137695,
      "learning_rate": 0.00015977570093457944,
      "loss": 0.4036,
      "step": 539
    },
    {
      "epoch": 0.1009440134592018,
      "grad_norm": 2.7521095275878906,
      "learning_rate": 0.00015970093457943926,
      "loss": 0.2266,
      "step": 540
    },
    {
      "epoch": 0.10113094681745957,
      "grad_norm": 3.7936174869537354,
      "learning_rate": 0.00015962616822429908,
      "loss": 0.3462,
      "step": 541
    },
    {
      "epoch": 0.10131788017571736,
      "grad_norm": 2.652277946472168,
      "learning_rate": 0.0001595514018691589,
      "loss": 0.2621,
      "step": 542
    },
    {
      "epoch": 0.10150481353397514,
      "grad_norm": 3.5278844833374023,
      "learning_rate": 0.0001594766355140187,
      "loss": 0.1539,
      "step": 543
    },
    {
      "epoch": 0.10169174689223291,
      "grad_norm": 5.6807732582092285,
      "learning_rate": 0.0001594018691588785,
      "loss": 0.2654,
      "step": 544
    },
    {
      "epoch": 0.1018786802504907,
      "grad_norm": 2.976480484008789,
      "learning_rate": 0.00015932710280373833,
      "loss": 0.1632,
      "step": 545
    },
    {
      "epoch": 0.10206561360874848,
      "grad_norm": 5.12968111038208,
      "learning_rate": 0.00015925233644859815,
      "loss": 0.4218,
      "step": 546
    },
    {
      "epoch": 0.10225254696700627,
      "grad_norm": 7.8314008712768555,
      "learning_rate": 0.00015917757009345794,
      "loss": 0.377,
      "step": 547
    },
    {
      "epoch": 0.10243948032526404,
      "grad_norm": 6.627962589263916,
      "learning_rate": 0.00015910280373831776,
      "loss": 0.2404,
      "step": 548
    },
    {
      "epoch": 0.10262641368352182,
      "grad_norm": 7.896306991577148,
      "learning_rate": 0.00015902803738317758,
      "loss": 0.4463,
      "step": 549
    },
    {
      "epoch": 0.1028133470417796,
      "grad_norm": 10.5477294921875,
      "learning_rate": 0.0001589532710280374,
      "loss": 0.4177,
      "step": 550
    },
    {
      "epoch": 0.10300028040003739,
      "grad_norm": 3.9631848335266113,
      "learning_rate": 0.0001588785046728972,
      "loss": 0.2362,
      "step": 551
    },
    {
      "epoch": 0.10318721375829516,
      "grad_norm": 3.573092222213745,
      "learning_rate": 0.000158803738317757,
      "loss": 0.2066,
      "step": 552
    },
    {
      "epoch": 0.10337414711655295,
      "grad_norm": 8.691794395446777,
      "learning_rate": 0.00015872897196261683,
      "loss": 0.3056,
      "step": 553
    },
    {
      "epoch": 0.10356108047481073,
      "grad_norm": 4.2701640129089355,
      "learning_rate": 0.00015865420560747664,
      "loss": 0.2681,
      "step": 554
    },
    {
      "epoch": 0.10374801383306852,
      "grad_norm": 4.17018461227417,
      "learning_rate": 0.00015857943925233644,
      "loss": 0.1441,
      "step": 555
    },
    {
      "epoch": 0.10393494719132629,
      "grad_norm": 3.9772355556488037,
      "learning_rate": 0.00015850467289719625,
      "loss": 0.2701,
      "step": 556
    },
    {
      "epoch": 0.10412188054958407,
      "grad_norm": 4.9383087158203125,
      "learning_rate": 0.00015842990654205607,
      "loss": 0.2664,
      "step": 557
    },
    {
      "epoch": 0.10430881390784186,
      "grad_norm": 6.769442558288574,
      "learning_rate": 0.0001583551401869159,
      "loss": 0.3944,
      "step": 558
    },
    {
      "epoch": 0.10449574726609964,
      "grad_norm": 6.758599281311035,
      "learning_rate": 0.0001582803738317757,
      "loss": 0.2903,
      "step": 559
    },
    {
      "epoch": 0.10468268062435741,
      "grad_norm": 2.9292962551116943,
      "learning_rate": 0.00015820560747663553,
      "loss": 0.1996,
      "step": 560
    },
    {
      "epoch": 0.1048696139826152,
      "grad_norm": 6.240577220916748,
      "learning_rate": 0.00015813084112149535,
      "loss": 0.2289,
      "step": 561
    },
    {
      "epoch": 0.10505654734087298,
      "grad_norm": 4.314499378204346,
      "learning_rate": 0.00015805607476635517,
      "loss": 0.2231,
      "step": 562
    },
    {
      "epoch": 0.10524348069913075,
      "grad_norm": 8.97903060913086,
      "learning_rate": 0.00015798130841121496,
      "loss": 0.3332,
      "step": 563
    },
    {
      "epoch": 0.10543041405738854,
      "grad_norm": 3.1879186630249023,
      "learning_rate": 0.00015790654205607478,
      "loss": 0.1457,
      "step": 564
    },
    {
      "epoch": 0.10561734741564632,
      "grad_norm": 3.3846383094787598,
      "learning_rate": 0.0001578317757009346,
      "loss": 0.2038,
      "step": 565
    },
    {
      "epoch": 0.10580428077390411,
      "grad_norm": 6.990555286407471,
      "learning_rate": 0.00015775700934579442,
      "loss": 0.4065,
      "step": 566
    },
    {
      "epoch": 0.10599121413216188,
      "grad_norm": 5.042025566101074,
      "learning_rate": 0.0001576822429906542,
      "loss": 0.3201,
      "step": 567
    },
    {
      "epoch": 0.10617814749041966,
      "grad_norm": 3.722186326980591,
      "learning_rate": 0.00015760747663551403,
      "loss": 0.2179,
      "step": 568
    },
    {
      "epoch": 0.10636508084867745,
      "grad_norm": 4.903820991516113,
      "learning_rate": 0.00015753271028037385,
      "loss": 0.2571,
      "step": 569
    },
    {
      "epoch": 0.10655201420693523,
      "grad_norm": 12.380327224731445,
      "learning_rate": 0.00015745794392523366,
      "loss": 0.3646,
      "step": 570
    },
    {
      "epoch": 0.106738947565193,
      "grad_norm": 7.5583319664001465,
      "learning_rate": 0.00015738317757009346,
      "loss": 0.2586,
      "step": 571
    },
    {
      "epoch": 0.10692588092345079,
      "grad_norm": 4.373205184936523,
      "learning_rate": 0.00015730841121495328,
      "loss": 0.352,
      "step": 572
    },
    {
      "epoch": 0.10711281428170857,
      "grad_norm": 6.232450008392334,
      "learning_rate": 0.0001572336448598131,
      "loss": 0.252,
      "step": 573
    },
    {
      "epoch": 0.10729974763996636,
      "grad_norm": 3.701850652694702,
      "learning_rate": 0.0001571588785046729,
      "loss": 0.1852,
      "step": 574
    },
    {
      "epoch": 0.10748668099822413,
      "grad_norm": 3.4285387992858887,
      "learning_rate": 0.0001570841121495327,
      "loss": 0.3133,
      "step": 575
    },
    {
      "epoch": 0.10767361435648191,
      "grad_norm": 8.362515449523926,
      "learning_rate": 0.00015700934579439252,
      "loss": 0.4342,
      "step": 576
    },
    {
      "epoch": 0.1078605477147397,
      "grad_norm": 3.289475917816162,
      "learning_rate": 0.00015693457943925234,
      "loss": 0.2571,
      "step": 577
    },
    {
      "epoch": 0.10804748107299747,
      "grad_norm": 3.8304951190948486,
      "learning_rate": 0.00015685981308411216,
      "loss": 0.3948,
      "step": 578
    },
    {
      "epoch": 0.10823441443125525,
      "grad_norm": 2.7548298835754395,
      "learning_rate": 0.00015678504672897195,
      "loss": 0.221,
      "step": 579
    },
    {
      "epoch": 0.10842134778951304,
      "grad_norm": 5.445391654968262,
      "learning_rate": 0.00015671028037383177,
      "loss": 0.2907,
      "step": 580
    },
    {
      "epoch": 0.10860828114777082,
      "grad_norm": 4.647691249847412,
      "learning_rate": 0.0001566355140186916,
      "loss": 0.3854,
      "step": 581
    },
    {
      "epoch": 0.1087952145060286,
      "grad_norm": 3.060637950897217,
      "learning_rate": 0.0001565607476635514,
      "loss": 0.1664,
      "step": 582
    },
    {
      "epoch": 0.10898214786428638,
      "grad_norm": 3.736422538757324,
      "learning_rate": 0.0001564859813084112,
      "loss": 0.2222,
      "step": 583
    },
    {
      "epoch": 0.10916908122254416,
      "grad_norm": 4.980441570281982,
      "learning_rate": 0.00015641121495327105,
      "loss": 0.4073,
      "step": 584
    },
    {
      "epoch": 0.10935601458080195,
      "grad_norm": 3.933237075805664,
      "learning_rate": 0.00015633644859813087,
      "loss": 0.2615,
      "step": 585
    },
    {
      "epoch": 0.10954294793905972,
      "grad_norm": 2.8764488697052,
      "learning_rate": 0.00015626168224299066,
      "loss": 0.2296,
      "step": 586
    },
    {
      "epoch": 0.1097298812973175,
      "grad_norm": 10.73617935180664,
      "learning_rate": 0.00015618691588785048,
      "loss": 0.4508,
      "step": 587
    },
    {
      "epoch": 0.10991681465557529,
      "grad_norm": 4.353697299957275,
      "learning_rate": 0.0001561121495327103,
      "loss": 0.2333,
      "step": 588
    },
    {
      "epoch": 0.11010374801383307,
      "grad_norm": 7.117440700531006,
      "learning_rate": 0.00015603738317757011,
      "loss": 0.3669,
      "step": 589
    },
    {
      "epoch": 0.11029068137209085,
      "grad_norm": 6.760433197021484,
      "learning_rate": 0.0001559626168224299,
      "loss": 0.3054,
      "step": 590
    },
    {
      "epoch": 0.11047761473034863,
      "grad_norm": 6.286616325378418,
      "learning_rate": 0.00015588785046728973,
      "loss": 0.2585,
      "step": 591
    },
    {
      "epoch": 0.11066454808860642,
      "grad_norm": 10.680754661560059,
      "learning_rate": 0.00015581308411214954,
      "loss": 0.5239,
      "step": 592
    },
    {
      "epoch": 0.11085148144686419,
      "grad_norm": 2.5476746559143066,
      "learning_rate": 0.00015573831775700936,
      "loss": 0.1673,
      "step": 593
    },
    {
      "epoch": 0.11103841480512197,
      "grad_norm": 4.632869243621826,
      "learning_rate": 0.00015566355140186915,
      "loss": 0.2753,
      "step": 594
    },
    {
      "epoch": 0.11122534816337976,
      "grad_norm": 4.754827976226807,
      "learning_rate": 0.00015558878504672897,
      "loss": 0.2836,
      "step": 595
    },
    {
      "epoch": 0.11141228152163754,
      "grad_norm": 10.191203117370605,
      "learning_rate": 0.0001555140186915888,
      "loss": 0.2208,
      "step": 596
    },
    {
      "epoch": 0.11159921487989531,
      "grad_norm": 17.04555320739746,
      "learning_rate": 0.0001554392523364486,
      "loss": 0.2942,
      "step": 597
    },
    {
      "epoch": 0.1117861482381531,
      "grad_norm": 4.504672050476074,
      "learning_rate": 0.0001553644859813084,
      "loss": 0.4164,
      "step": 598
    },
    {
      "epoch": 0.11197308159641088,
      "grad_norm": 5.120833873748779,
      "learning_rate": 0.00015528971962616822,
      "loss": 0.1876,
      "step": 599
    },
    {
      "epoch": 0.11216001495466867,
      "grad_norm": 4.245889663696289,
      "learning_rate": 0.00015521495327102804,
      "loss": 0.2368,
      "step": 600
    },
    {
      "epoch": 0.11234694831292644,
      "grad_norm": 2.6436426639556885,
      "learning_rate": 0.00015514018691588786,
      "loss": 0.1987,
      "step": 601
    },
    {
      "epoch": 0.11253388167118422,
      "grad_norm": 5.423171520233154,
      "learning_rate": 0.00015506542056074765,
      "loss": 0.2979,
      "step": 602
    },
    {
      "epoch": 0.112720815029442,
      "grad_norm": 3.8755686283111572,
      "learning_rate": 0.00015499065420560747,
      "loss": 0.1478,
      "step": 603
    },
    {
      "epoch": 0.11290774838769979,
      "grad_norm": 9.523313522338867,
      "learning_rate": 0.0001549158878504673,
      "loss": 0.4159,
      "step": 604
    },
    {
      "epoch": 0.11309468174595756,
      "grad_norm": 2.791602849960327,
      "learning_rate": 0.0001548411214953271,
      "loss": 0.252,
      "step": 605
    },
    {
      "epoch": 0.11328161510421535,
      "grad_norm": 5.674592971801758,
      "learning_rate": 0.00015476635514018693,
      "loss": 0.2999,
      "step": 606
    },
    {
      "epoch": 0.11346854846247313,
      "grad_norm": 6.386527061462402,
      "learning_rate": 0.00015469158878504672,
      "loss": 0.3279,
      "step": 607
    },
    {
      "epoch": 0.1136554818207309,
      "grad_norm": 4.242475986480713,
      "learning_rate": 0.00015461682242990656,
      "loss": 0.2884,
      "step": 608
    },
    {
      "epoch": 0.11384241517898869,
      "grad_norm": 3.177438974380493,
      "learning_rate": 0.00015454205607476638,
      "loss": 0.2159,
      "step": 609
    },
    {
      "epoch": 0.11402934853724647,
      "grad_norm": 5.585165977478027,
      "learning_rate": 0.00015446728971962618,
      "loss": 0.3578,
      "step": 610
    },
    {
      "epoch": 0.11421628189550426,
      "grad_norm": 4.981574058532715,
      "learning_rate": 0.000154392523364486,
      "loss": 0.3042,
      "step": 611
    },
    {
      "epoch": 0.11440321525376203,
      "grad_norm": 3.9133012294769287,
      "learning_rate": 0.0001543177570093458,
      "loss": 0.3644,
      "step": 612
    },
    {
      "epoch": 0.11459014861201981,
      "grad_norm": 3.6573445796966553,
      "learning_rate": 0.00015424299065420563,
      "loss": 0.3044,
      "step": 613
    },
    {
      "epoch": 0.1147770819702776,
      "grad_norm": 2.6431901454925537,
      "learning_rate": 0.00015416822429906542,
      "loss": 0.1915,
      "step": 614
    },
    {
      "epoch": 0.11496401532853538,
      "grad_norm": 4.790956020355225,
      "learning_rate": 0.00015409345794392524,
      "loss": 0.3224,
      "step": 615
    },
    {
      "epoch": 0.11515094868679315,
      "grad_norm": 3.8131024837493896,
      "learning_rate": 0.00015401869158878506,
      "loss": 0.3482,
      "step": 616
    },
    {
      "epoch": 0.11533788204505094,
      "grad_norm": 2.15445876121521,
      "learning_rate": 0.00015394392523364488,
      "loss": 0.2432,
      "step": 617
    },
    {
      "epoch": 0.11552481540330872,
      "grad_norm": 4.386061191558838,
      "learning_rate": 0.00015386915887850467,
      "loss": 0.3045,
      "step": 618
    },
    {
      "epoch": 0.11571174876156651,
      "grad_norm": 6.8166680335998535,
      "learning_rate": 0.0001537943925233645,
      "loss": 0.3088,
      "step": 619
    },
    {
      "epoch": 0.11589868211982428,
      "grad_norm": 7.911377906799316,
      "learning_rate": 0.0001537196261682243,
      "loss": 0.3349,
      "step": 620
    },
    {
      "epoch": 0.11608561547808206,
      "grad_norm": 3.223095178604126,
      "learning_rate": 0.00015364485981308413,
      "loss": 0.1973,
      "step": 621
    },
    {
      "epoch": 0.11627254883633985,
      "grad_norm": 4.480245590209961,
      "learning_rate": 0.00015357009345794392,
      "loss": 0.1824,
      "step": 622
    },
    {
      "epoch": 0.11645948219459763,
      "grad_norm": 2.3660244941711426,
      "learning_rate": 0.00015349532710280374,
      "loss": 0.2452,
      "step": 623
    },
    {
      "epoch": 0.1166464155528554,
      "grad_norm": 6.585710048675537,
      "learning_rate": 0.00015342056074766356,
      "loss": 0.2814,
      "step": 624
    },
    {
      "epoch": 0.11683334891111319,
      "grad_norm": 5.924216270446777,
      "learning_rate": 0.00015334579439252338,
      "loss": 0.2776,
      "step": 625
    },
    {
      "epoch": 0.11702028226937097,
      "grad_norm": 3.666078805923462,
      "learning_rate": 0.00015327102803738317,
      "loss": 0.3508,
      "step": 626
    },
    {
      "epoch": 0.11720721562762874,
      "grad_norm": 3.189021587371826,
      "learning_rate": 0.000153196261682243,
      "loss": 0.2577,
      "step": 627
    },
    {
      "epoch": 0.11739414898588653,
      "grad_norm": 5.5432658195495605,
      "learning_rate": 0.0001531214953271028,
      "loss": 0.3586,
      "step": 628
    },
    {
      "epoch": 0.11758108234414431,
      "grad_norm": 9.66438102722168,
      "learning_rate": 0.00015304672897196263,
      "loss": 0.1374,
      "step": 629
    },
    {
      "epoch": 0.1177680157024021,
      "grad_norm": 4.0390706062316895,
      "learning_rate": 0.00015297196261682242,
      "loss": 0.2669,
      "step": 630
    },
    {
      "epoch": 0.11795494906065987,
      "grad_norm": 4.2661943435668945,
      "learning_rate": 0.00015289719626168224,
      "loss": 0.2357,
      "step": 631
    },
    {
      "epoch": 0.11814188241891765,
      "grad_norm": 5.597467422485352,
      "learning_rate": 0.00015282242990654206,
      "loss": 0.3663,
      "step": 632
    },
    {
      "epoch": 0.11832881577717544,
      "grad_norm": 7.372241497039795,
      "learning_rate": 0.00015274766355140187,
      "loss": 0.3306,
      "step": 633
    },
    {
      "epoch": 0.11851574913543322,
      "grad_norm": 4.083491802215576,
      "learning_rate": 0.0001526728971962617,
      "loss": 0.301,
      "step": 634
    },
    {
      "epoch": 0.118702682493691,
      "grad_norm": 4.303440093994141,
      "learning_rate": 0.0001525981308411215,
      "loss": 0.3764,
      "step": 635
    },
    {
      "epoch": 0.11888961585194878,
      "grad_norm": 5.451353073120117,
      "learning_rate": 0.00015252336448598133,
      "loss": 0.3666,
      "step": 636
    },
    {
      "epoch": 0.11907654921020656,
      "grad_norm": 3.0813825130462646,
      "learning_rate": 0.00015244859813084115,
      "loss": 0.2468,
      "step": 637
    },
    {
      "epoch": 0.11926348256846435,
      "grad_norm": 2.7777934074401855,
      "learning_rate": 0.00015237383177570094,
      "loss": 0.2304,
      "step": 638
    },
    {
      "epoch": 0.11945041592672212,
      "grad_norm": 2.1186771392822266,
      "learning_rate": 0.00015229906542056076,
      "loss": 0.2139,
      "step": 639
    },
    {
      "epoch": 0.1196373492849799,
      "grad_norm": 3.4052071571350098,
      "learning_rate": 0.00015222429906542058,
      "loss": 0.2157,
      "step": 640
    },
    {
      "epoch": 0.11982428264323769,
      "grad_norm": 4.417631149291992,
      "learning_rate": 0.0001521495327102804,
      "loss": 0.3274,
      "step": 641
    },
    {
      "epoch": 0.12001121600149546,
      "grad_norm": 2.3495945930480957,
      "learning_rate": 0.0001520747663551402,
      "loss": 0.2264,
      "step": 642
    },
    {
      "epoch": 0.12019814935975325,
      "grad_norm": 2.564134359359741,
      "learning_rate": 0.000152,
      "loss": 0.2873,
      "step": 643
    },
    {
      "epoch": 0.12038508271801103,
      "grad_norm": 3.811795473098755,
      "learning_rate": 0.00015192523364485983,
      "loss": 0.2172,
      "step": 644
    },
    {
      "epoch": 0.12057201607626881,
      "grad_norm": 3.727064609527588,
      "learning_rate": 0.00015185046728971965,
      "loss": 0.1702,
      "step": 645
    },
    {
      "epoch": 0.12075894943452659,
      "grad_norm": 15.565170288085938,
      "learning_rate": 0.00015177570093457944,
      "loss": 0.424,
      "step": 646
    },
    {
      "epoch": 0.12094588279278437,
      "grad_norm": 1.9626095294952393,
      "learning_rate": 0.00015170093457943926,
      "loss": 0.238,
      "step": 647
    },
    {
      "epoch": 0.12113281615104216,
      "grad_norm": 5.213286876678467,
      "learning_rate": 0.00015162616822429908,
      "loss": 0.2683,
      "step": 648
    },
    {
      "epoch": 0.12131974950929994,
      "grad_norm": 4.557398319244385,
      "learning_rate": 0.0001515514018691589,
      "loss": 0.2562,
      "step": 649
    },
    {
      "epoch": 0.12150668286755771,
      "grad_norm": 5.329291820526123,
      "learning_rate": 0.00015147663551401869,
      "loss": 0.2545,
      "step": 650
    },
    {
      "epoch": 0.1216936162258155,
      "grad_norm": 5.558221340179443,
      "learning_rate": 0.0001514018691588785,
      "loss": 0.2187,
      "step": 651
    },
    {
      "epoch": 0.12188054958407328,
      "grad_norm": 8.271523475646973,
      "learning_rate": 0.00015132710280373832,
      "loss": 0.4125,
      "step": 652
    },
    {
      "epoch": 0.12206748294233107,
      "grad_norm": 4.289553642272949,
      "learning_rate": 0.00015125233644859814,
      "loss": 0.3298,
      "step": 653
    },
    {
      "epoch": 0.12225441630058884,
      "grad_norm": 4.0856475830078125,
      "learning_rate": 0.00015117757009345793,
      "loss": 0.2965,
      "step": 654
    },
    {
      "epoch": 0.12244134965884662,
      "grad_norm": 3.6447947025299072,
      "learning_rate": 0.00015110280373831775,
      "loss": 0.293,
      "step": 655
    },
    {
      "epoch": 0.1226282830171044,
      "grad_norm": 5.498687267303467,
      "learning_rate": 0.00015102803738317757,
      "loss": 0.333,
      "step": 656
    },
    {
      "epoch": 0.12281521637536218,
      "grad_norm": 3.716212034225464,
      "learning_rate": 0.0001509532710280374,
      "loss": 0.1992,
      "step": 657
    },
    {
      "epoch": 0.12300214973361996,
      "grad_norm": 6.161970138549805,
      "learning_rate": 0.0001508785046728972,
      "loss": 0.2219,
      "step": 658
    },
    {
      "epoch": 0.12318908309187775,
      "grad_norm": 2.9372198581695557,
      "learning_rate": 0.00015080373831775703,
      "loss": 0.1808,
      "step": 659
    },
    {
      "epoch": 0.12337601645013553,
      "grad_norm": 4.3279900550842285,
      "learning_rate": 0.00015072897196261685,
      "loss": 0.2922,
      "step": 660
    },
    {
      "epoch": 0.1235629498083933,
      "grad_norm": 3.1535139083862305,
      "learning_rate": 0.00015065420560747664,
      "loss": 0.2684,
      "step": 661
    },
    {
      "epoch": 0.12374988316665109,
      "grad_norm": 5.35379695892334,
      "learning_rate": 0.00015057943925233646,
      "loss": 0.2735,
      "step": 662
    },
    {
      "epoch": 0.12393681652490887,
      "grad_norm": 4.5732645988464355,
      "learning_rate": 0.00015050467289719628,
      "loss": 0.4364,
      "step": 663
    },
    {
      "epoch": 0.12412374988316666,
      "grad_norm": 3.603727102279663,
      "learning_rate": 0.0001504299065420561,
      "loss": 0.1986,
      "step": 664
    },
    {
      "epoch": 0.12431068324142443,
      "grad_norm": 2.5849766731262207,
      "learning_rate": 0.0001503551401869159,
      "loss": 0.2098,
      "step": 665
    },
    {
      "epoch": 0.12449761659968221,
      "grad_norm": 3.5649967193603516,
      "learning_rate": 0.0001502803738317757,
      "loss": 0.1812,
      "step": 666
    },
    {
      "epoch": 0.12468454995794,
      "grad_norm": 5.981021881103516,
      "learning_rate": 0.00015020560747663553,
      "loss": 0.254,
      "step": 667
    },
    {
      "epoch": 0.12487148331619778,
      "grad_norm": 4.713133335113525,
      "learning_rate": 0.00015013084112149534,
      "loss": 0.2429,
      "step": 668
    },
    {
      "epoch": 0.12505841667445555,
      "grad_norm": 8.945528984069824,
      "learning_rate": 0.00015005607476635514,
      "loss": 0.2732,
      "step": 669
    },
    {
      "epoch": 0.12524535003271334,
      "grad_norm": 10.842784881591797,
      "learning_rate": 0.00014998130841121496,
      "loss": 0.3276,
      "step": 670
    },
    {
      "epoch": 0.12543228339097112,
      "grad_norm": 5.091331481933594,
      "learning_rate": 0.00014990654205607477,
      "loss": 0.316,
      "step": 671
    },
    {
      "epoch": 0.1256192167492289,
      "grad_norm": 8.102872848510742,
      "learning_rate": 0.0001498317757009346,
      "loss": 0.4049,
      "step": 672
    },
    {
      "epoch": 0.1258061501074867,
      "grad_norm": 7.571207046508789,
      "learning_rate": 0.00014975700934579438,
      "loss": 0.304,
      "step": 673
    },
    {
      "epoch": 0.12599308346574445,
      "grad_norm": 3.747913360595703,
      "learning_rate": 0.0001496822429906542,
      "loss": 0.1282,
      "step": 674
    },
    {
      "epoch": 0.12618001682400223,
      "grad_norm": 3.4165642261505127,
      "learning_rate": 0.00014960747663551402,
      "loss": 0.2408,
      "step": 675
    },
    {
      "epoch": 0.12636695018226002,
      "grad_norm": 8.733354568481445,
      "learning_rate": 0.00014953271028037384,
      "loss": 0.2891,
      "step": 676
    },
    {
      "epoch": 0.1265538835405178,
      "grad_norm": 6.965294361114502,
      "learning_rate": 0.00014945794392523363,
      "loss": 0.4093,
      "step": 677
    },
    {
      "epoch": 0.1267408168987756,
      "grad_norm": 6.373272895812988,
      "learning_rate": 0.00014938317757009345,
      "loss": 0.3457,
      "step": 678
    },
    {
      "epoch": 0.12692775025703337,
      "grad_norm": 6.929954528808594,
      "learning_rate": 0.00014930841121495327,
      "loss": 0.4022,
      "step": 679
    },
    {
      "epoch": 0.12711468361529116,
      "grad_norm": 7.659861087799072,
      "learning_rate": 0.0001492336448598131,
      "loss": 0.4111,
      "step": 680
    },
    {
      "epoch": 0.12730161697354894,
      "grad_norm": 2.778294563293457,
      "learning_rate": 0.00014915887850467288,
      "loss": 0.194,
      "step": 681
    },
    {
      "epoch": 0.1274885503318067,
      "grad_norm": 4.455080986022949,
      "learning_rate": 0.0001490841121495327,
      "loss": 0.2755,
      "step": 682
    },
    {
      "epoch": 0.12767548369006448,
      "grad_norm": 3.816378593444824,
      "learning_rate": 0.00014900934579439255,
      "loss": 0.2339,
      "step": 683
    },
    {
      "epoch": 0.12786241704832227,
      "grad_norm": 3.4979565143585205,
      "learning_rate": 0.00014893457943925237,
      "loss": 0.3096,
      "step": 684
    },
    {
      "epoch": 0.12804935040658005,
      "grad_norm": 3.113682270050049,
      "learning_rate": 0.00014885981308411216,
      "loss": 0.1408,
      "step": 685
    },
    {
      "epoch": 0.12823628376483784,
      "grad_norm": 2.8050503730773926,
      "learning_rate": 0.00014878504672897198,
      "loss": 0.2984,
      "step": 686
    },
    {
      "epoch": 0.12842321712309562,
      "grad_norm": 2.326402187347412,
      "learning_rate": 0.0001487102803738318,
      "loss": 0.273,
      "step": 687
    },
    {
      "epoch": 0.1286101504813534,
      "grad_norm": 2.3371152877807617,
      "learning_rate": 0.0001486355140186916,
      "loss": 0.2839,
      "step": 688
    },
    {
      "epoch": 0.12879708383961117,
      "grad_norm": 3.724904775619507,
      "learning_rate": 0.0001485607476635514,
      "loss": 0.1652,
      "step": 689
    },
    {
      "epoch": 0.12898401719786895,
      "grad_norm": 5.774971008300781,
      "learning_rate": 0.00014848598130841122,
      "loss": 0.2216,
      "step": 690
    },
    {
      "epoch": 0.12917095055612673,
      "grad_norm": 4.06535530090332,
      "learning_rate": 0.00014841121495327104,
      "loss": 0.2706,
      "step": 691
    },
    {
      "epoch": 0.12935788391438452,
      "grad_norm": 4.617464065551758,
      "learning_rate": 0.00014833644859813086,
      "loss": 0.2084,
      "step": 692
    },
    {
      "epoch": 0.1295448172726423,
      "grad_norm": 3.8523619174957275,
      "learning_rate": 0.00014826168224299065,
      "loss": 0.1877,
      "step": 693
    },
    {
      "epoch": 0.1297317506309001,
      "grad_norm": 4.940908908843994,
      "learning_rate": 0.00014818691588785047,
      "loss": 0.4392,
      "step": 694
    },
    {
      "epoch": 0.12991868398915787,
      "grad_norm": 9.051532745361328,
      "learning_rate": 0.0001481121495327103,
      "loss": 0.2993,
      "step": 695
    },
    {
      "epoch": 0.13010561734741566,
      "grad_norm": 6.253376483917236,
      "learning_rate": 0.0001480373831775701,
      "loss": 0.35,
      "step": 696
    },
    {
      "epoch": 0.13029255070567342,
      "grad_norm": 1.7706751823425293,
      "learning_rate": 0.0001479626168224299,
      "loss": 0.1241,
      "step": 697
    },
    {
      "epoch": 0.1304794840639312,
      "grad_norm": 3.8197555541992188,
      "learning_rate": 0.00014788785046728972,
      "loss": 0.1485,
      "step": 698
    },
    {
      "epoch": 0.13066641742218899,
      "grad_norm": 4.147230625152588,
      "learning_rate": 0.00014781308411214954,
      "loss": 0.3192,
      "step": 699
    },
    {
      "epoch": 0.13085335078044677,
      "grad_norm": 4.012048721313477,
      "learning_rate": 0.00014773831775700936,
      "loss": 0.15,
      "step": 700
    },
    {
      "epoch": 0.13104028413870455,
      "grad_norm": 14.689864158630371,
      "learning_rate": 0.00014766355140186915,
      "loss": 0.2948,
      "step": 701
    },
    {
      "epoch": 0.13122721749696234,
      "grad_norm": 3.132366895675659,
      "learning_rate": 0.00014758878504672897,
      "loss": 0.24,
      "step": 702
    },
    {
      "epoch": 0.13141415085522012,
      "grad_norm": 3.0965664386749268,
      "learning_rate": 0.0001475140186915888,
      "loss": 0.2071,
      "step": 703
    },
    {
      "epoch": 0.13160108421347788,
      "grad_norm": 3.471226453781128,
      "learning_rate": 0.0001474392523364486,
      "loss": 0.2725,
      "step": 704
    },
    {
      "epoch": 0.13178801757173567,
      "grad_norm": 4.665051460266113,
      "learning_rate": 0.0001473644859813084,
      "loss": 0.2024,
      "step": 705
    },
    {
      "epoch": 0.13197495092999345,
      "grad_norm": 4.69957160949707,
      "learning_rate": 0.00014728971962616822,
      "loss": 0.2276,
      "step": 706
    },
    {
      "epoch": 0.13216188428825124,
      "grad_norm": 6.7846999168396,
      "learning_rate": 0.00014721495327102806,
      "loss": 0.384,
      "step": 707
    },
    {
      "epoch": 0.13234881764650902,
      "grad_norm": 5.550090312957764,
      "learning_rate": 0.00014714018691588786,
      "loss": 0.2662,
      "step": 708
    },
    {
      "epoch": 0.1325357510047668,
      "grad_norm": 3.5063066482543945,
      "learning_rate": 0.00014706542056074767,
      "loss": 0.2397,
      "step": 709
    },
    {
      "epoch": 0.1327226843630246,
      "grad_norm": 7.34426212310791,
      "learning_rate": 0.0001469906542056075,
      "loss": 0.4106,
      "step": 710
    },
    {
      "epoch": 0.13290961772128237,
      "grad_norm": 2.866541862487793,
      "learning_rate": 0.0001469158878504673,
      "loss": 0.3437,
      "step": 711
    },
    {
      "epoch": 0.13309655107954013,
      "grad_norm": 7.841230392456055,
      "learning_rate": 0.0001468411214953271,
      "loss": 0.3791,
      "step": 712
    },
    {
      "epoch": 0.13328348443779792,
      "grad_norm": 4.0296502113342285,
      "learning_rate": 0.00014676635514018692,
      "loss": 0.3184,
      "step": 713
    },
    {
      "epoch": 0.1334704177960557,
      "grad_norm": 3.083066940307617,
      "learning_rate": 0.00014669158878504674,
      "loss": 0.219,
      "step": 714
    },
    {
      "epoch": 0.1336573511543135,
      "grad_norm": 4.363747596740723,
      "learning_rate": 0.00014661682242990656,
      "loss": 0.3541,
      "step": 715
    },
    {
      "epoch": 0.13384428451257127,
      "grad_norm": 3.9956462383270264,
      "learning_rate": 0.00014654205607476635,
      "loss": 0.2685,
      "step": 716
    },
    {
      "epoch": 0.13403121787082906,
      "grad_norm": 2.8996264934539795,
      "learning_rate": 0.00014646728971962617,
      "loss": 0.3001,
      "step": 717
    },
    {
      "epoch": 0.13421815122908684,
      "grad_norm": 5.265876770019531,
      "learning_rate": 0.000146392523364486,
      "loss": 0.2637,
      "step": 718
    },
    {
      "epoch": 0.13440508458734463,
      "grad_norm": 4.693871021270752,
      "learning_rate": 0.0001463177570093458,
      "loss": 0.3139,
      "step": 719
    },
    {
      "epoch": 0.13459201794560238,
      "grad_norm": 2.3889753818511963,
      "learning_rate": 0.00014624299065420563,
      "loss": 0.198,
      "step": 720
    },
    {
      "epoch": 0.13477895130386017,
      "grad_norm": 6.0068535804748535,
      "learning_rate": 0.00014616822429906542,
      "loss": 0.2211,
      "step": 721
    },
    {
      "epoch": 0.13496588466211795,
      "grad_norm": 2.3430466651916504,
      "learning_rate": 0.00014609345794392524,
      "loss": 0.2261,
      "step": 722
    },
    {
      "epoch": 0.13515281802037574,
      "grad_norm": 3.385784864425659,
      "learning_rate": 0.00014601869158878506,
      "loss": 0.2045,
      "step": 723
    },
    {
      "epoch": 0.13533975137863352,
      "grad_norm": 3.9007728099823,
      "learning_rate": 0.00014594392523364488,
      "loss": 0.1507,
      "step": 724
    },
    {
      "epoch": 0.1355266847368913,
      "grad_norm": 4.22988224029541,
      "learning_rate": 0.00014586915887850467,
      "loss": 0.2301,
      "step": 725
    },
    {
      "epoch": 0.1357136180951491,
      "grad_norm": 4.6953935623168945,
      "learning_rate": 0.00014579439252336449,
      "loss": 0.2914,
      "step": 726
    },
    {
      "epoch": 0.13590055145340685,
      "grad_norm": 4.514949321746826,
      "learning_rate": 0.0001457196261682243,
      "loss": 0.2658,
      "step": 727
    },
    {
      "epoch": 0.13608748481166463,
      "grad_norm": 4.399122714996338,
      "learning_rate": 0.00014564485981308412,
      "loss": 0.1939,
      "step": 728
    },
    {
      "epoch": 0.13627441816992242,
      "grad_norm": 2.9353866577148438,
      "learning_rate": 0.00014557009345794392,
      "loss": 0.1816,
      "step": 729
    },
    {
      "epoch": 0.1364613515281802,
      "grad_norm": 7.799497127532959,
      "learning_rate": 0.00014549532710280373,
      "loss": 0.3141,
      "step": 730
    },
    {
      "epoch": 0.136648284886438,
      "grad_norm": 3.302077293395996,
      "learning_rate": 0.00014542056074766355,
      "loss": 0.2737,
      "step": 731
    },
    {
      "epoch": 0.13683521824469577,
      "grad_norm": 5.688361644744873,
      "learning_rate": 0.00014534579439252337,
      "loss": 0.2583,
      "step": 732
    },
    {
      "epoch": 0.13702215160295356,
      "grad_norm": 3.5790464878082275,
      "learning_rate": 0.0001452710280373832,
      "loss": 0.222,
      "step": 733
    },
    {
      "epoch": 0.13720908496121134,
      "grad_norm": 2.0225462913513184,
      "learning_rate": 0.000145196261682243,
      "loss": 0.2048,
      "step": 734
    },
    {
      "epoch": 0.1373960183194691,
      "grad_norm": 3.3803999423980713,
      "learning_rate": 0.00014512149532710283,
      "loss": 0.2274,
      "step": 735
    },
    {
      "epoch": 0.13758295167772688,
      "grad_norm": 4.759778022766113,
      "learning_rate": 0.00014504672897196262,
      "loss": 0.1976,
      "step": 736
    },
    {
      "epoch": 0.13776988503598467,
      "grad_norm": 4.107612133026123,
      "learning_rate": 0.00014497196261682244,
      "loss": 0.3753,
      "step": 737
    },
    {
      "epoch": 0.13795681839424245,
      "grad_norm": 2.15549635887146,
      "learning_rate": 0.00014489719626168226,
      "loss": 0.1575,
      "step": 738
    },
    {
      "epoch": 0.13814375175250024,
      "grad_norm": 5.353437900543213,
      "learning_rate": 0.00014482242990654208,
      "loss": 0.2754,
      "step": 739
    },
    {
      "epoch": 0.13833068511075802,
      "grad_norm": 4.397258281707764,
      "learning_rate": 0.00014474766355140187,
      "loss": 0.2768,
      "step": 740
    },
    {
      "epoch": 0.1385176184690158,
      "grad_norm": 3.898345708847046,
      "learning_rate": 0.0001446728971962617,
      "loss": 0.2392,
      "step": 741
    },
    {
      "epoch": 0.13870455182727356,
      "grad_norm": 5.397886753082275,
      "learning_rate": 0.0001445981308411215,
      "loss": 0.3268,
      "step": 742
    },
    {
      "epoch": 0.13889148518553135,
      "grad_norm": 5.570205211639404,
      "learning_rate": 0.00014452336448598133,
      "loss": 0.242,
      "step": 743
    },
    {
      "epoch": 0.13907841854378913,
      "grad_norm": 3.607835292816162,
      "learning_rate": 0.00014444859813084112,
      "loss": 0.1736,
      "step": 744
    },
    {
      "epoch": 0.13926535190204692,
      "grad_norm": 3.1895759105682373,
      "learning_rate": 0.00014437383177570094,
      "loss": 0.2465,
      "step": 745
    },
    {
      "epoch": 0.1394522852603047,
      "grad_norm": 6.671090126037598,
      "learning_rate": 0.00014429906542056076,
      "loss": 0.2676,
      "step": 746
    },
    {
      "epoch": 0.1396392186185625,
      "grad_norm": 3.08736515045166,
      "learning_rate": 0.00014422429906542057,
      "loss": 0.2131,
      "step": 747
    },
    {
      "epoch": 0.13982615197682027,
      "grad_norm": 3.8462483882904053,
      "learning_rate": 0.00014414953271028037,
      "loss": 0.2661,
      "step": 748
    },
    {
      "epoch": 0.14001308533507806,
      "grad_norm": 8.328754425048828,
      "learning_rate": 0.00014407476635514018,
      "loss": 0.2419,
      "step": 749
    },
    {
      "epoch": 0.14020001869333582,
      "grad_norm": 4.160635948181152,
      "learning_rate": 0.000144,
      "loss": 0.2272,
      "step": 750
    },
    {
      "epoch": 0.1403869520515936,
      "grad_norm": 3.628530740737915,
      "learning_rate": 0.00014392523364485982,
      "loss": 0.2374,
      "step": 751
    },
    {
      "epoch": 0.14057388540985138,
      "grad_norm": 4.34812068939209,
      "learning_rate": 0.00014385046728971961,
      "loss": 0.2864,
      "step": 752
    },
    {
      "epoch": 0.14076081876810917,
      "grad_norm": 2.7368321418762207,
      "learning_rate": 0.00014377570093457943,
      "loss": 0.2996,
      "step": 753
    },
    {
      "epoch": 0.14094775212636695,
      "grad_norm": 4.2335381507873535,
      "learning_rate": 0.00014370093457943925,
      "loss": 0.2823,
      "step": 754
    },
    {
      "epoch": 0.14113468548462474,
      "grad_norm": 3.520167112350464,
      "learning_rate": 0.00014362616822429907,
      "loss": 0.2553,
      "step": 755
    },
    {
      "epoch": 0.14132161884288252,
      "grad_norm": 4.51554536819458,
      "learning_rate": 0.00014355140186915886,
      "loss": 0.2515,
      "step": 756
    },
    {
      "epoch": 0.14150855220114028,
      "grad_norm": 3.2103679180145264,
      "learning_rate": 0.0001434766355140187,
      "loss": 0.2619,
      "step": 757
    },
    {
      "epoch": 0.14169548555939807,
      "grad_norm": 3.7275798320770264,
      "learning_rate": 0.00014340186915887853,
      "loss": 0.1853,
      "step": 758
    },
    {
      "epoch": 0.14188241891765585,
      "grad_norm": 5.575503826141357,
      "learning_rate": 0.00014332710280373835,
      "loss": 0.22,
      "step": 759
    },
    {
      "epoch": 0.14206935227591364,
      "grad_norm": 4.002122402191162,
      "learning_rate": 0.00014325233644859814,
      "loss": 0.1309,
      "step": 760
    },
    {
      "epoch": 0.14225628563417142,
      "grad_norm": 4.666782855987549,
      "learning_rate": 0.00014317757009345796,
      "loss": 0.2375,
      "step": 761
    },
    {
      "epoch": 0.1424432189924292,
      "grad_norm": 3.9011921882629395,
      "learning_rate": 0.00014310280373831778,
      "loss": 0.2478,
      "step": 762
    },
    {
      "epoch": 0.142630152350687,
      "grad_norm": 6.666588306427002,
      "learning_rate": 0.0001430280373831776,
      "loss": 0.48,
      "step": 763
    },
    {
      "epoch": 0.14281708570894477,
      "grad_norm": 1.2707078456878662,
      "learning_rate": 0.0001429532710280374,
      "loss": 0.0769,
      "step": 764
    },
    {
      "epoch": 0.14300401906720253,
      "grad_norm": 9.832050323486328,
      "learning_rate": 0.0001428785046728972,
      "loss": 0.3404,
      "step": 765
    },
    {
      "epoch": 0.14319095242546032,
      "grad_norm": 2.297867774963379,
      "learning_rate": 0.00014280373831775702,
      "loss": 0.2879,
      "step": 766
    },
    {
      "epoch": 0.1433778857837181,
      "grad_norm": 6.331712245941162,
      "learning_rate": 0.00014272897196261684,
      "loss": 0.3108,
      "step": 767
    },
    {
      "epoch": 0.14356481914197589,
      "grad_norm": 7.699277400970459,
      "learning_rate": 0.00014265420560747663,
      "loss": 0.3085,
      "step": 768
    },
    {
      "epoch": 0.14375175250023367,
      "grad_norm": 7.352818965911865,
      "learning_rate": 0.00014257943925233645,
      "loss": 0.2571,
      "step": 769
    },
    {
      "epoch": 0.14393868585849146,
      "grad_norm": 1.9078726768493652,
      "learning_rate": 0.00014250467289719627,
      "loss": 0.1872,
      "step": 770
    },
    {
      "epoch": 0.14412561921674924,
      "grad_norm": 5.106470584869385,
      "learning_rate": 0.0001424299065420561,
      "loss": 0.285,
      "step": 771
    },
    {
      "epoch": 0.144312552575007,
      "grad_norm": 3.5868334770202637,
      "learning_rate": 0.00014235514018691588,
      "loss": 0.2209,
      "step": 772
    },
    {
      "epoch": 0.14449948593326478,
      "grad_norm": 6.157139301300049,
      "learning_rate": 0.0001422803738317757,
      "loss": 0.3049,
      "step": 773
    },
    {
      "epoch": 0.14468641929152257,
      "grad_norm": 3.779491662979126,
      "learning_rate": 0.00014220560747663552,
      "loss": 0.3785,
      "step": 774
    },
    {
      "epoch": 0.14487335264978035,
      "grad_norm": 5.936172962188721,
      "learning_rate": 0.00014213084112149534,
      "loss": 0.1896,
      "step": 775
    },
    {
      "epoch": 0.14506028600803814,
      "grad_norm": 8.032537460327148,
      "learning_rate": 0.00014205607476635513,
      "loss": 0.349,
      "step": 776
    },
    {
      "epoch": 0.14524721936629592,
      "grad_norm": 2.930297613143921,
      "learning_rate": 0.00014198130841121495,
      "loss": 0.2596,
      "step": 777
    },
    {
      "epoch": 0.1454341527245537,
      "grad_norm": 3.0486574172973633,
      "learning_rate": 0.00014190654205607477,
      "loss": 0.2429,
      "step": 778
    },
    {
      "epoch": 0.1456210860828115,
      "grad_norm": 2.9545578956604004,
      "learning_rate": 0.0001418317757009346,
      "loss": 0.2549,
      "step": 779
    },
    {
      "epoch": 0.14580801944106925,
      "grad_norm": 2.485409736633301,
      "learning_rate": 0.00014175700934579438,
      "loss": 0.2054,
      "step": 780
    },
    {
      "epoch": 0.14599495279932703,
      "grad_norm": 5.643674373626709,
      "learning_rate": 0.0001416822429906542,
      "loss": 0.1978,
      "step": 781
    },
    {
      "epoch": 0.14618188615758482,
      "grad_norm": 2.161611557006836,
      "learning_rate": 0.00014160747663551404,
      "loss": 0.1995,
      "step": 782
    },
    {
      "epoch": 0.1463688195158426,
      "grad_norm": 8.011161804199219,
      "learning_rate": 0.00014153271028037384,
      "loss": 0.4184,
      "step": 783
    },
    {
      "epoch": 0.1465557528741004,
      "grad_norm": 3.286740779876709,
      "learning_rate": 0.00014145794392523366,
      "loss": 0.189,
      "step": 784
    },
    {
      "epoch": 0.14674268623235817,
      "grad_norm": 2.9646952152252197,
      "learning_rate": 0.00014138317757009347,
      "loss": 0.1513,
      "step": 785
    },
    {
      "epoch": 0.14692961959061596,
      "grad_norm": 6.6240458488464355,
      "learning_rate": 0.0001413084112149533,
      "loss": 0.1671,
      "step": 786
    },
    {
      "epoch": 0.1471165529488737,
      "grad_norm": 4.307314872741699,
      "learning_rate": 0.00014123364485981308,
      "loss": 0.2662,
      "step": 787
    },
    {
      "epoch": 0.1473034863071315,
      "grad_norm": 5.569575786590576,
      "learning_rate": 0.0001411588785046729,
      "loss": 0.2994,
      "step": 788
    },
    {
      "epoch": 0.14749041966538928,
      "grad_norm": 2.3944530487060547,
      "learning_rate": 0.00014108411214953272,
      "loss": 0.1303,
      "step": 789
    },
    {
      "epoch": 0.14767735302364707,
      "grad_norm": 2.8729641437530518,
      "learning_rate": 0.00014100934579439254,
      "loss": 0.1777,
      "step": 790
    },
    {
      "epoch": 0.14786428638190485,
      "grad_norm": 1.734629511833191,
      "learning_rate": 0.00014093457943925233,
      "loss": 0.1304,
      "step": 791
    },
    {
      "epoch": 0.14805121974016264,
      "grad_norm": 2.89807391166687,
      "learning_rate": 0.00014085981308411215,
      "loss": 0.2,
      "step": 792
    },
    {
      "epoch": 0.14823815309842042,
      "grad_norm": 4.37346076965332,
      "learning_rate": 0.00014078504672897197,
      "loss": 0.2758,
      "step": 793
    },
    {
      "epoch": 0.1484250864566782,
      "grad_norm": 5.4740376472473145,
      "learning_rate": 0.0001407102803738318,
      "loss": 0.3005,
      "step": 794
    },
    {
      "epoch": 0.14861201981493596,
      "grad_norm": 1.9234461784362793,
      "learning_rate": 0.00014063551401869158,
      "loss": 0.1389,
      "step": 795
    },
    {
      "epoch": 0.14879895317319375,
      "grad_norm": 4.007986545562744,
      "learning_rate": 0.0001405607476635514,
      "loss": 0.2063,
      "step": 796
    },
    {
      "epoch": 0.14898588653145153,
      "grad_norm": 5.363847732543945,
      "learning_rate": 0.00014048598130841122,
      "loss": 0.3141,
      "step": 797
    },
    {
      "epoch": 0.14917281988970932,
      "grad_norm": 2.605393171310425,
      "learning_rate": 0.00014041121495327104,
      "loss": 0.2241,
      "step": 798
    },
    {
      "epoch": 0.1493597532479671,
      "grad_norm": 7.25532341003418,
      "learning_rate": 0.00014033644859813083,
      "loss": 0.1956,
      "step": 799
    },
    {
      "epoch": 0.1495466866062249,
      "grad_norm": 4.7009196281433105,
      "learning_rate": 0.00014026168224299065,
      "loss": 0.2565,
      "step": 800
    },
    {
      "epoch": 0.14973361996448267,
      "grad_norm": 4.2773118019104,
      "learning_rate": 0.00014018691588785047,
      "loss": 0.3225,
      "step": 801
    },
    {
      "epoch": 0.14992055332274043,
      "grad_norm": 2.2779862880706787,
      "learning_rate": 0.0001401121495327103,
      "loss": 0.1558,
      "step": 802
    },
    {
      "epoch": 0.15010748668099821,
      "grad_norm": 4.071010112762451,
      "learning_rate": 0.00014003738317757008,
      "loss": 0.2911,
      "step": 803
    },
    {
      "epoch": 0.150294420039256,
      "grad_norm": 5.01444673538208,
      "learning_rate": 0.0001399626168224299,
      "loss": 0.2352,
      "step": 804
    },
    {
      "epoch": 0.15048135339751378,
      "grad_norm": 3.5433919429779053,
      "learning_rate": 0.00013988785046728972,
      "loss": 0.2724,
      "step": 805
    },
    {
      "epoch": 0.15066828675577157,
      "grad_norm": 2.5749757289886475,
      "learning_rate": 0.00013981308411214956,
      "loss": 0.2714,
      "step": 806
    },
    {
      "epoch": 0.15085522011402935,
      "grad_norm": 2.0384247303009033,
      "learning_rate": 0.00013973831775700935,
      "loss": 0.2167,
      "step": 807
    },
    {
      "epoch": 0.15104215347228714,
      "grad_norm": 2.3602304458618164,
      "learning_rate": 0.00013966355140186917,
      "loss": 0.2743,
      "step": 808
    },
    {
      "epoch": 0.15122908683054492,
      "grad_norm": 1.8717005252838135,
      "learning_rate": 0.000139588785046729,
      "loss": 0.1788,
      "step": 809
    },
    {
      "epoch": 0.15141602018880268,
      "grad_norm": 4.586750030517578,
      "learning_rate": 0.0001395140186915888,
      "loss": 0.2617,
      "step": 810
    },
    {
      "epoch": 0.15160295354706047,
      "grad_norm": 3.619107723236084,
      "learning_rate": 0.0001394392523364486,
      "loss": 0.2275,
      "step": 811
    },
    {
      "epoch": 0.15178988690531825,
      "grad_norm": 4.912079811096191,
      "learning_rate": 0.00013936448598130842,
      "loss": 0.3186,
      "step": 812
    },
    {
      "epoch": 0.15197682026357603,
      "grad_norm": 8.775716781616211,
      "learning_rate": 0.00013928971962616824,
      "loss": 0.2902,
      "step": 813
    },
    {
      "epoch": 0.15216375362183382,
      "grad_norm": 2.323575735092163,
      "learning_rate": 0.00013921495327102806,
      "loss": 0.1909,
      "step": 814
    },
    {
      "epoch": 0.1523506869800916,
      "grad_norm": 3.3380661010742188,
      "learning_rate": 0.00013914018691588785,
      "loss": 0.1401,
      "step": 815
    },
    {
      "epoch": 0.1525376203383494,
      "grad_norm": 3.6629765033721924,
      "learning_rate": 0.00013906542056074767,
      "loss": 0.3185,
      "step": 816
    },
    {
      "epoch": 0.15272455369660715,
      "grad_norm": 4.230559825897217,
      "learning_rate": 0.0001389906542056075,
      "loss": 0.1409,
      "step": 817
    },
    {
      "epoch": 0.15291148705486493,
      "grad_norm": 9.437518119812012,
      "learning_rate": 0.0001389158878504673,
      "loss": 0.4614,
      "step": 818
    },
    {
      "epoch": 0.15309842041312272,
      "grad_norm": 8.446029663085938,
      "learning_rate": 0.0001388411214953271,
      "loss": 0.3546,
      "step": 819
    },
    {
      "epoch": 0.1532853537713805,
      "grad_norm": 4.068688869476318,
      "learning_rate": 0.00013876635514018692,
      "loss": 0.2205,
      "step": 820
    },
    {
      "epoch": 0.15347228712963829,
      "grad_norm": 4.354220390319824,
      "learning_rate": 0.00013869158878504674,
      "loss": 0.2192,
      "step": 821
    },
    {
      "epoch": 0.15365922048789607,
      "grad_norm": 8.568276405334473,
      "learning_rate": 0.00013861682242990656,
      "loss": 0.2533,
      "step": 822
    },
    {
      "epoch": 0.15384615384615385,
      "grad_norm": 4.847375392913818,
      "learning_rate": 0.00013854205607476635,
      "loss": 0.3357,
      "step": 823
    },
    {
      "epoch": 0.15403308720441164,
      "grad_norm": 4.101343631744385,
      "learning_rate": 0.00013846728971962617,
      "loss": 0.2778,
      "step": 824
    },
    {
      "epoch": 0.1542200205626694,
      "grad_norm": 2.391831159591675,
      "learning_rate": 0.00013839252336448598,
      "loss": 0.2643,
      "step": 825
    },
    {
      "epoch": 0.15440695392092718,
      "grad_norm": 4.492321014404297,
      "learning_rate": 0.0001383177570093458,
      "loss": 0.2834,
      "step": 826
    },
    {
      "epoch": 0.15459388727918497,
      "grad_norm": 1.8229235410690308,
      "learning_rate": 0.0001382429906542056,
      "loss": 0.151,
      "step": 827
    },
    {
      "epoch": 0.15478082063744275,
      "grad_norm": 4.091841697692871,
      "learning_rate": 0.00013816822429906541,
      "loss": 0.1687,
      "step": 828
    },
    {
      "epoch": 0.15496775399570054,
      "grad_norm": 3.2013959884643555,
      "learning_rate": 0.00013809345794392523,
      "loss": 0.2041,
      "step": 829
    },
    {
      "epoch": 0.15515468735395832,
      "grad_norm": 2.6738393306732178,
      "learning_rate": 0.00013801869158878505,
      "loss": 0.1871,
      "step": 830
    },
    {
      "epoch": 0.1553416207122161,
      "grad_norm": 2.880312204360962,
      "learning_rate": 0.00013794392523364487,
      "loss": 0.16,
      "step": 831
    },
    {
      "epoch": 0.15552855407047386,
      "grad_norm": 7.55530309677124,
      "learning_rate": 0.0001378691588785047,
      "loss": 0.2688,
      "step": 832
    },
    {
      "epoch": 0.15571548742873165,
      "grad_norm": 2.6777474880218506,
      "learning_rate": 0.0001377943925233645,
      "loss": 0.2324,
      "step": 833
    },
    {
      "epoch": 0.15590242078698943,
      "grad_norm": 5.567189693450928,
      "learning_rate": 0.0001377196261682243,
      "loss": 0.301,
      "step": 834
    },
    {
      "epoch": 0.15608935414524722,
      "grad_norm": 3.1408004760742188,
      "learning_rate": 0.00013764485981308412,
      "loss": 0.322,
      "step": 835
    },
    {
      "epoch": 0.156276287503505,
      "grad_norm": 4.140600204467773,
      "learning_rate": 0.00013757009345794394,
      "loss": 0.1532,
      "step": 836
    },
    {
      "epoch": 0.1564632208617628,
      "grad_norm": 4.050083637237549,
      "learning_rate": 0.00013749532710280376,
      "loss": 0.1792,
      "step": 837
    },
    {
      "epoch": 0.15665015422002057,
      "grad_norm": 3.5526058673858643,
      "learning_rate": 0.00013742056074766358,
      "loss": 0.2792,
      "step": 838
    },
    {
      "epoch": 0.15683708757827836,
      "grad_norm": 2.1874427795410156,
      "learning_rate": 0.00013734579439252337,
      "loss": 0.1574,
      "step": 839
    },
    {
      "epoch": 0.1570240209365361,
      "grad_norm": 4.0223894119262695,
      "learning_rate": 0.0001372710280373832,
      "loss": 0.2238,
      "step": 840
    },
    {
      "epoch": 0.1572109542947939,
      "grad_norm": 4.957114219665527,
      "learning_rate": 0.000137196261682243,
      "loss": 0.2717,
      "step": 841
    },
    {
      "epoch": 0.15739788765305168,
      "grad_norm": 4.814484596252441,
      "learning_rate": 0.00013712149532710282,
      "loss": 0.2549,
      "step": 842
    },
    {
      "epoch": 0.15758482101130947,
      "grad_norm": 3.371039867401123,
      "learning_rate": 0.00013704672897196262,
      "loss": 0.1476,
      "step": 843
    },
    {
      "epoch": 0.15777175436956725,
      "grad_norm": 8.555622100830078,
      "learning_rate": 0.00013697196261682243,
      "loss": 0.3285,
      "step": 844
    },
    {
      "epoch": 0.15795868772782504,
      "grad_norm": 12.96856689453125,
      "learning_rate": 0.00013689719626168225,
      "loss": 0.2792,
      "step": 845
    },
    {
      "epoch": 0.15814562108608282,
      "grad_norm": 3.591841220855713,
      "learning_rate": 0.00013682242990654207,
      "loss": 0.2163,
      "step": 846
    },
    {
      "epoch": 0.1583325544443406,
      "grad_norm": 3.2688419818878174,
      "learning_rate": 0.00013674766355140186,
      "loss": 0.2265,
      "step": 847
    },
    {
      "epoch": 0.15851948780259836,
      "grad_norm": 3.113335132598877,
      "learning_rate": 0.00013667289719626168,
      "loss": 0.1606,
      "step": 848
    },
    {
      "epoch": 0.15870642116085615,
      "grad_norm": 3.8613579273223877,
      "learning_rate": 0.0001365981308411215,
      "loss": 0.2103,
      "step": 849
    },
    {
      "epoch": 0.15889335451911393,
      "grad_norm": 5.379683494567871,
      "learning_rate": 0.00013652336448598132,
      "loss": 0.1305,
      "step": 850
    },
    {
      "epoch": 0.15908028787737172,
      "grad_norm": 2.105642318725586,
      "learning_rate": 0.0001364485981308411,
      "loss": 0.2158,
      "step": 851
    },
    {
      "epoch": 0.1592672212356295,
      "grad_norm": 2.2498624324798584,
      "learning_rate": 0.00013637383177570093,
      "loss": 0.2297,
      "step": 852
    },
    {
      "epoch": 0.1594541545938873,
      "grad_norm": 5.998940467834473,
      "learning_rate": 0.00013629906542056075,
      "loss": 0.5041,
      "step": 853
    },
    {
      "epoch": 0.15964108795214507,
      "grad_norm": 7.508262634277344,
      "learning_rate": 0.00013622429906542057,
      "loss": 0.1589,
      "step": 854
    },
    {
      "epoch": 0.15982802131040283,
      "grad_norm": 3.6702117919921875,
      "learning_rate": 0.00013614953271028036,
      "loss": 0.1876,
      "step": 855
    },
    {
      "epoch": 0.16001495466866061,
      "grad_norm": 2.941039800643921,
      "learning_rate": 0.0001360747663551402,
      "loss": 0.2479,
      "step": 856
    },
    {
      "epoch": 0.1602018880269184,
      "grad_norm": 1.459545373916626,
      "learning_rate": 0.00013600000000000003,
      "loss": 0.2501,
      "step": 857
    },
    {
      "epoch": 0.16038882138517618,
      "grad_norm": 2.35255765914917,
      "learning_rate": 0.00013592523364485982,
      "loss": 0.1941,
      "step": 858
    },
    {
      "epoch": 0.16057575474343397,
      "grad_norm": 5.150120258331299,
      "learning_rate": 0.00013585046728971964,
      "loss": 0.2635,
      "step": 859
    },
    {
      "epoch": 0.16076268810169175,
      "grad_norm": 3.4793312549591064,
      "learning_rate": 0.00013577570093457946,
      "loss": 0.3476,
      "step": 860
    },
    {
      "epoch": 0.16094962145994954,
      "grad_norm": 2.5585708618164062,
      "learning_rate": 0.00013570093457943927,
      "loss": 0.1773,
      "step": 861
    },
    {
      "epoch": 0.16113655481820732,
      "grad_norm": 1.7239570617675781,
      "learning_rate": 0.00013562616822429907,
      "loss": 0.1564,
      "step": 862
    },
    {
      "epoch": 0.16132348817646508,
      "grad_norm": 3.3254923820495605,
      "learning_rate": 0.00013555140186915889,
      "loss": 0.2049,
      "step": 863
    },
    {
      "epoch": 0.16151042153472286,
      "grad_norm": 3.1247622966766357,
      "learning_rate": 0.0001354766355140187,
      "loss": 0.2758,
      "step": 864
    },
    {
      "epoch": 0.16169735489298065,
      "grad_norm": 3.8403313159942627,
      "learning_rate": 0.00013540186915887852,
      "loss": 0.3459,
      "step": 865
    },
    {
      "epoch": 0.16188428825123843,
      "grad_norm": 4.73360013961792,
      "learning_rate": 0.00013532710280373831,
      "loss": 0.1532,
      "step": 866
    },
    {
      "epoch": 0.16207122160949622,
      "grad_norm": 3.2642600536346436,
      "learning_rate": 0.00013525233644859813,
      "loss": 0.2139,
      "step": 867
    },
    {
      "epoch": 0.162258154967754,
      "grad_norm": 4.436898231506348,
      "learning_rate": 0.00013517757009345795,
      "loss": 0.2472,
      "step": 868
    },
    {
      "epoch": 0.1624450883260118,
      "grad_norm": 5.482010841369629,
      "learning_rate": 0.00013510280373831777,
      "loss": 0.344,
      "step": 869
    },
    {
      "epoch": 0.16263202168426955,
      "grad_norm": 3.810779094696045,
      "learning_rate": 0.00013502803738317756,
      "loss": 0.2505,
      "step": 870
    },
    {
      "epoch": 0.16281895504252733,
      "grad_norm": 3.304408073425293,
      "learning_rate": 0.00013495327102803738,
      "loss": 0.153,
      "step": 871
    },
    {
      "epoch": 0.16300588840078511,
      "grad_norm": 2.0357189178466797,
      "learning_rate": 0.0001348785046728972,
      "loss": 0.1767,
      "step": 872
    },
    {
      "epoch": 0.1631928217590429,
      "grad_norm": 2.2250559329986572,
      "learning_rate": 0.00013480373831775702,
      "loss": 0.1817,
      "step": 873
    },
    {
      "epoch": 0.16337975511730068,
      "grad_norm": 2.0497539043426514,
      "learning_rate": 0.0001347289719626168,
      "loss": 0.1406,
      "step": 874
    },
    {
      "epoch": 0.16356668847555847,
      "grad_norm": 5.842738628387451,
      "learning_rate": 0.00013465420560747663,
      "loss": 0.2721,
      "step": 875
    },
    {
      "epoch": 0.16375362183381625,
      "grad_norm": 5.946224689483643,
      "learning_rate": 0.00013457943925233645,
      "loss": 0.2932,
      "step": 876
    },
    {
      "epoch": 0.16394055519207404,
      "grad_norm": 3.955274820327759,
      "learning_rate": 0.00013450467289719627,
      "loss": 0.2666,
      "step": 877
    },
    {
      "epoch": 0.1641274885503318,
      "grad_norm": 3.129817247390747,
      "learning_rate": 0.00013442990654205606,
      "loss": 0.3115,
      "step": 878
    },
    {
      "epoch": 0.16431442190858958,
      "grad_norm": 6.2449517250061035,
      "learning_rate": 0.00013435514018691588,
      "loss": 0.3119,
      "step": 879
    },
    {
      "epoch": 0.16450135526684737,
      "grad_norm": 21.50505256652832,
      "learning_rate": 0.0001342803738317757,
      "loss": 0.3559,
      "step": 880
    },
    {
      "epoch": 0.16468828862510515,
      "grad_norm": 5.915285110473633,
      "learning_rate": 0.00013420560747663554,
      "loss": 0.1758,
      "step": 881
    },
    {
      "epoch": 0.16487522198336294,
      "grad_norm": 3.165520429611206,
      "learning_rate": 0.00013413084112149534,
      "loss": 0.2305,
      "step": 882
    },
    {
      "epoch": 0.16506215534162072,
      "grad_norm": 4.123284816741943,
      "learning_rate": 0.00013405607476635515,
      "loss": 0.3454,
      "step": 883
    },
    {
      "epoch": 0.1652490886998785,
      "grad_norm": 2.558659791946411,
      "learning_rate": 0.00013398130841121497,
      "loss": 0.2531,
      "step": 884
    },
    {
      "epoch": 0.16543602205813626,
      "grad_norm": 9.501462936401367,
      "learning_rate": 0.0001339065420560748,
      "loss": 0.3906,
      "step": 885
    },
    {
      "epoch": 0.16562295541639405,
      "grad_norm": 3.5535552501678467,
      "learning_rate": 0.00013383177570093458,
      "loss": 0.2048,
      "step": 886
    },
    {
      "epoch": 0.16580988877465183,
      "grad_norm": 4.216268539428711,
      "learning_rate": 0.0001337570093457944,
      "loss": 0.2412,
      "step": 887
    },
    {
      "epoch": 0.16599682213290962,
      "grad_norm": 1.8421436548233032,
      "learning_rate": 0.00013368224299065422,
      "loss": 0.1342,
      "step": 888
    },
    {
      "epoch": 0.1661837554911674,
      "grad_norm": 3.156907320022583,
      "learning_rate": 0.00013360747663551404,
      "loss": 0.1439,
      "step": 889
    },
    {
      "epoch": 0.16637068884942519,
      "grad_norm": 2.7255637645721436,
      "learning_rate": 0.00013353271028037383,
      "loss": 0.2415,
      "step": 890
    },
    {
      "epoch": 0.16655762220768297,
      "grad_norm": 4.5577239990234375,
      "learning_rate": 0.00013345794392523365,
      "loss": 0.2478,
      "step": 891
    },
    {
      "epoch": 0.16674455556594076,
      "grad_norm": 2.6327767372131348,
      "learning_rate": 0.00013338317757009347,
      "loss": 0.2967,
      "step": 892
    },
    {
      "epoch": 0.1669314889241985,
      "grad_norm": 4.35221004486084,
      "learning_rate": 0.0001333084112149533,
      "loss": 0.2367,
      "step": 893
    },
    {
      "epoch": 0.1671184222824563,
      "grad_norm": 2.9798357486724854,
      "learning_rate": 0.00013323364485981308,
      "loss": 0.2724,
      "step": 894
    },
    {
      "epoch": 0.16730535564071408,
      "grad_norm": 6.706631183624268,
      "learning_rate": 0.0001331588785046729,
      "loss": 0.2784,
      "step": 895
    },
    {
      "epoch": 0.16749228899897187,
      "grad_norm": 4.1652350425720215,
      "learning_rate": 0.00013308411214953272,
      "loss": 0.1861,
      "step": 896
    },
    {
      "epoch": 0.16767922235722965,
      "grad_norm": 5.698426723480225,
      "learning_rate": 0.00013300934579439254,
      "loss": 0.2878,
      "step": 897
    },
    {
      "epoch": 0.16786615571548744,
      "grad_norm": 2.141303539276123,
      "learning_rate": 0.00013293457943925233,
      "loss": 0.1942,
      "step": 898
    },
    {
      "epoch": 0.16805308907374522,
      "grad_norm": 2.6163318157196045,
      "learning_rate": 0.00013285981308411215,
      "loss": 0.1995,
      "step": 899
    },
    {
      "epoch": 0.16824002243200298,
      "grad_norm": 2.6607930660247803,
      "learning_rate": 0.00013278504672897197,
      "loss": 0.1888,
      "step": 900
    },
    {
      "epoch": 0.16842695579026076,
      "grad_norm": 8.353466987609863,
      "learning_rate": 0.00013271028037383179,
      "loss": 0.2445,
      "step": 901
    },
    {
      "epoch": 0.16861388914851855,
      "grad_norm": 1.3663363456726074,
      "learning_rate": 0.00013263551401869158,
      "loss": 0.1683,
      "step": 902
    },
    {
      "epoch": 0.16880082250677633,
      "grad_norm": 3.1489524841308594,
      "learning_rate": 0.0001325607476635514,
      "loss": 0.2567,
      "step": 903
    },
    {
      "epoch": 0.16898775586503412,
      "grad_norm": 3.5593295097351074,
      "learning_rate": 0.00013248598130841121,
      "loss": 0.1189,
      "step": 904
    },
    {
      "epoch": 0.1691746892232919,
      "grad_norm": 5.819289207458496,
      "learning_rate": 0.00013241121495327103,
      "loss": 0.2925,
      "step": 905
    },
    {
      "epoch": 0.1693616225815497,
      "grad_norm": 2.8176281452178955,
      "learning_rate": 0.00013233644859813085,
      "loss": 0.2227,
      "step": 906
    },
    {
      "epoch": 0.16954855593980747,
      "grad_norm": 3.418717861175537,
      "learning_rate": 0.00013226168224299067,
      "loss": 0.2704,
      "step": 907
    },
    {
      "epoch": 0.16973548929806523,
      "grad_norm": 3.985682249069214,
      "learning_rate": 0.0001321869158878505,
      "loss": 0.2608,
      "step": 908
    },
    {
      "epoch": 0.169922422656323,
      "grad_norm": 10.383191108703613,
      "learning_rate": 0.00013211214953271028,
      "loss": 0.2022,
      "step": 909
    },
    {
      "epoch": 0.1701093560145808,
      "grad_norm": 3.860751152038574,
      "learning_rate": 0.0001320373831775701,
      "loss": 0.4011,
      "step": 910
    },
    {
      "epoch": 0.17029628937283858,
      "grad_norm": 4.3758864402771,
      "learning_rate": 0.00013196261682242992,
      "loss": 0.1332,
      "step": 911
    },
    {
      "epoch": 0.17048322273109637,
      "grad_norm": 3.2386202812194824,
      "learning_rate": 0.00013188785046728974,
      "loss": 0.1981,
      "step": 912
    },
    {
      "epoch": 0.17067015608935415,
      "grad_norm": 3.551698684692383,
      "learning_rate": 0.00013181308411214953,
      "loss": 0.1368,
      "step": 913
    },
    {
      "epoch": 0.17085708944761194,
      "grad_norm": 3.901181697845459,
      "learning_rate": 0.00013173831775700935,
      "loss": 0.2546,
      "step": 914
    },
    {
      "epoch": 0.1710440228058697,
      "grad_norm": 5.9671244621276855,
      "learning_rate": 0.00013166355140186917,
      "loss": 0.3036,
      "step": 915
    },
    {
      "epoch": 0.17123095616412748,
      "grad_norm": 2.733487129211426,
      "learning_rate": 0.000131588785046729,
      "loss": 0.2664,
      "step": 916
    },
    {
      "epoch": 0.17141788952238526,
      "grad_norm": 4.217400074005127,
      "learning_rate": 0.00013151401869158878,
      "loss": 0.263,
      "step": 917
    },
    {
      "epoch": 0.17160482288064305,
      "grad_norm": 5.718862533569336,
      "learning_rate": 0.0001314392523364486,
      "loss": 0.2386,
      "step": 918
    },
    {
      "epoch": 0.17179175623890083,
      "grad_norm": 8.214094161987305,
      "learning_rate": 0.00013136448598130842,
      "loss": 0.3008,
      "step": 919
    },
    {
      "epoch": 0.17197868959715862,
      "grad_norm": 6.091806888580322,
      "learning_rate": 0.00013128971962616824,
      "loss": 0.1848,
      "step": 920
    },
    {
      "epoch": 0.1721656229554164,
      "grad_norm": 3.1432480812072754,
      "learning_rate": 0.00013121495327102805,
      "loss": 0.3109,
      "step": 921
    },
    {
      "epoch": 0.1723525563136742,
      "grad_norm": 2.6247665882110596,
      "learning_rate": 0.00013114018691588785,
      "loss": 0.2232,
      "step": 922
    },
    {
      "epoch": 0.17253948967193194,
      "grad_norm": 2.321455955505371,
      "learning_rate": 0.00013106542056074766,
      "loss": 0.1346,
      "step": 923
    },
    {
      "epoch": 0.17272642303018973,
      "grad_norm": 1.9457379579544067,
      "learning_rate": 0.00013099065420560748,
      "loss": 0.2195,
      "step": 924
    },
    {
      "epoch": 0.17291335638844751,
      "grad_norm": 2.5969061851501465,
      "learning_rate": 0.0001309158878504673,
      "loss": 0.1788,
      "step": 925
    },
    {
      "epoch": 0.1731002897467053,
      "grad_norm": 3.0008726119995117,
      "learning_rate": 0.0001308411214953271,
      "loss": 0.2205,
      "step": 926
    },
    {
      "epoch": 0.17328722310496308,
      "grad_norm": 2.446674346923828,
      "learning_rate": 0.0001307663551401869,
      "loss": 0.3303,
      "step": 927
    },
    {
      "epoch": 0.17347415646322087,
      "grad_norm": 1.7075082063674927,
      "learning_rate": 0.00013069158878504673,
      "loss": 0.1413,
      "step": 928
    },
    {
      "epoch": 0.17366108982147865,
      "grad_norm": 4.784101486206055,
      "learning_rate": 0.00013061682242990655,
      "loss": 0.1435,
      "step": 929
    },
    {
      "epoch": 0.1738480231797364,
      "grad_norm": 4.989909648895264,
      "learning_rate": 0.00013054205607476637,
      "loss": 0.344,
      "step": 930
    },
    {
      "epoch": 0.1740349565379942,
      "grad_norm": 2.1897284984588623,
      "learning_rate": 0.0001304672897196262,
      "loss": 0.2211,
      "step": 931
    },
    {
      "epoch": 0.17422188989625198,
      "grad_norm": 2.171128273010254,
      "learning_rate": 0.000130392523364486,
      "loss": 0.211,
      "step": 932
    },
    {
      "epoch": 0.17440882325450976,
      "grad_norm": 1.7743228673934937,
      "learning_rate": 0.0001303177570093458,
      "loss": 0.1556,
      "step": 933
    },
    {
      "epoch": 0.17459575661276755,
      "grad_norm": 3.590155601501465,
      "learning_rate": 0.00013024299065420562,
      "loss": 0.2262,
      "step": 934
    },
    {
      "epoch": 0.17478268997102533,
      "grad_norm": 1.3828219175338745,
      "learning_rate": 0.00013016822429906544,
      "loss": 0.1452,
      "step": 935
    },
    {
      "epoch": 0.17496962332928312,
      "grad_norm": 6.545665740966797,
      "learning_rate": 0.00013009345794392526,
      "loss": 0.356,
      "step": 936
    },
    {
      "epoch": 0.1751565566875409,
      "grad_norm": 8.48592472076416,
      "learning_rate": 0.00013001869158878505,
      "loss": 0.2281,
      "step": 937
    },
    {
      "epoch": 0.17534349004579866,
      "grad_norm": 9.929426193237305,
      "learning_rate": 0.00012994392523364487,
      "loss": 0.2432,
      "step": 938
    },
    {
      "epoch": 0.17553042340405645,
      "grad_norm": 2.2931876182556152,
      "learning_rate": 0.00012986915887850469,
      "loss": 0.1678,
      "step": 939
    },
    {
      "epoch": 0.17571735676231423,
      "grad_norm": 1.7848351001739502,
      "learning_rate": 0.0001297943925233645,
      "loss": 0.154,
      "step": 940
    },
    {
      "epoch": 0.17590429012057202,
      "grad_norm": 9.495343208312988,
      "learning_rate": 0.0001297196261682243,
      "loss": 0.456,
      "step": 941
    },
    {
      "epoch": 0.1760912234788298,
      "grad_norm": 3.726196050643921,
      "learning_rate": 0.00012964485981308411,
      "loss": 0.1632,
      "step": 942
    },
    {
      "epoch": 0.17627815683708759,
      "grad_norm": 2.775891065597534,
      "learning_rate": 0.00012957009345794393,
      "loss": 0.1579,
      "step": 943
    },
    {
      "epoch": 0.17646509019534537,
      "grad_norm": 3.8356847763061523,
      "learning_rate": 0.00012949532710280375,
      "loss": 0.2412,
      "step": 944
    },
    {
      "epoch": 0.17665202355360313,
      "grad_norm": 2.5438716411590576,
      "learning_rate": 0.00012942056074766354,
      "loss": 0.2105,
      "step": 945
    },
    {
      "epoch": 0.1768389569118609,
      "grad_norm": 5.11966609954834,
      "learning_rate": 0.00012934579439252336,
      "loss": 0.3129,
      "step": 946
    },
    {
      "epoch": 0.1770258902701187,
      "grad_norm": 8.173371315002441,
      "learning_rate": 0.00012927102803738318,
      "loss": 0.3795,
      "step": 947
    },
    {
      "epoch": 0.17721282362837648,
      "grad_norm": 4.61089563369751,
      "learning_rate": 0.000129196261682243,
      "loss": 0.1567,
      "step": 948
    },
    {
      "epoch": 0.17739975698663427,
      "grad_norm": 1.933635950088501,
      "learning_rate": 0.0001291214953271028,
      "loss": 0.1361,
      "step": 949
    },
    {
      "epoch": 0.17758669034489205,
      "grad_norm": 2.5966765880584717,
      "learning_rate": 0.0001290467289719626,
      "loss": 0.2103,
      "step": 950
    },
    {
      "epoch": 0.17777362370314984,
      "grad_norm": 3.950547695159912,
      "learning_rate": 0.00012897196261682243,
      "loss": 0.1774,
      "step": 951
    },
    {
      "epoch": 0.17796055706140762,
      "grad_norm": 2.785391330718994,
      "learning_rate": 0.00012889719626168225,
      "loss": 0.1721,
      "step": 952
    },
    {
      "epoch": 0.17814749041966538,
      "grad_norm": 7.140584468841553,
      "learning_rate": 0.00012882242990654204,
      "loss": 0.2964,
      "step": 953
    },
    {
      "epoch": 0.17833442377792316,
      "grad_norm": 4.563511848449707,
      "learning_rate": 0.00012874766355140186,
      "loss": 0.1557,
      "step": 954
    },
    {
      "epoch": 0.17852135713618095,
      "grad_norm": 6.8560380935668945,
      "learning_rate": 0.0001286728971962617,
      "loss": 0.2128,
      "step": 955
    },
    {
      "epoch": 0.17870829049443873,
      "grad_norm": 2.590745210647583,
      "learning_rate": 0.00012859813084112152,
      "loss": 0.2243,
      "step": 956
    },
    {
      "epoch": 0.17889522385269652,
      "grad_norm": 3.6339917182922363,
      "learning_rate": 0.00012852336448598132,
      "loss": 0.212,
      "step": 957
    },
    {
      "epoch": 0.1790821572109543,
      "grad_norm": 6.055475234985352,
      "learning_rate": 0.00012844859813084114,
      "loss": 0.2276,
      "step": 958
    },
    {
      "epoch": 0.1792690905692121,
      "grad_norm": 5.933411598205566,
      "learning_rate": 0.00012837383177570095,
      "loss": 0.1861,
      "step": 959
    },
    {
      "epoch": 0.17945602392746984,
      "grad_norm": 2.0307531356811523,
      "learning_rate": 0.00012829906542056077,
      "loss": 0.1422,
      "step": 960
    },
    {
      "epoch": 0.17964295728572763,
      "grad_norm": 3.3756346702575684,
      "learning_rate": 0.00012822429906542056,
      "loss": 0.2052,
      "step": 961
    },
    {
      "epoch": 0.1798298906439854,
      "grad_norm": 1.6388731002807617,
      "learning_rate": 0.00012814953271028038,
      "loss": 0.1673,
      "step": 962
    },
    {
      "epoch": 0.1800168240022432,
      "grad_norm": 3.0558550357818604,
      "learning_rate": 0.0001280747663551402,
      "loss": 0.1454,
      "step": 963
    },
    {
      "epoch": 0.18020375736050098,
      "grad_norm": 3.3675131797790527,
      "learning_rate": 0.00012800000000000002,
      "loss": 0.3746,
      "step": 964
    },
    {
      "epoch": 0.18039069071875877,
      "grad_norm": 3.2787203788757324,
      "learning_rate": 0.0001279252336448598,
      "loss": 0.2357,
      "step": 965
    },
    {
      "epoch": 0.18057762407701655,
      "grad_norm": 3.3669581413269043,
      "learning_rate": 0.00012785046728971963,
      "loss": 0.2357,
      "step": 966
    },
    {
      "epoch": 0.18076455743527434,
      "grad_norm": 6.087366580963135,
      "learning_rate": 0.00012777570093457945,
      "loss": 0.3413,
      "step": 967
    },
    {
      "epoch": 0.1809514907935321,
      "grad_norm": 6.310611724853516,
      "learning_rate": 0.00012770093457943927,
      "loss": 0.2101,
      "step": 968
    },
    {
      "epoch": 0.18113842415178988,
      "grad_norm": 2.5683176517486572,
      "learning_rate": 0.00012762616822429906,
      "loss": 0.2588,
      "step": 969
    },
    {
      "epoch": 0.18132535751004766,
      "grad_norm": 1.7357516288757324,
      "learning_rate": 0.00012755140186915888,
      "loss": 0.2111,
      "step": 970
    },
    {
      "epoch": 0.18151229086830545,
      "grad_norm": 2.874171018600464,
      "learning_rate": 0.0001274766355140187,
      "loss": 0.2626,
      "step": 971
    },
    {
      "epoch": 0.18169922422656323,
      "grad_norm": 8.592766761779785,
      "learning_rate": 0.00012740186915887852,
      "loss": 0.3089,
      "step": 972
    },
    {
      "epoch": 0.18188615758482102,
      "grad_norm": 2.408158779144287,
      "learning_rate": 0.0001273271028037383,
      "loss": 0.1852,
      "step": 973
    },
    {
      "epoch": 0.1820730909430788,
      "grad_norm": 2.5685465335845947,
      "learning_rate": 0.00012725233644859813,
      "loss": 0.1392,
      "step": 974
    },
    {
      "epoch": 0.1822600243013366,
      "grad_norm": 1.9015638828277588,
      "learning_rate": 0.00012717757009345795,
      "loss": 0.2778,
      "step": 975
    },
    {
      "epoch": 0.18244695765959434,
      "grad_norm": 4.901943206787109,
      "learning_rate": 0.00012710280373831777,
      "loss": 0.2885,
      "step": 976
    },
    {
      "epoch": 0.18263389101785213,
      "grad_norm": 1.689988136291504,
      "learning_rate": 0.00012702803738317756,
      "loss": 0.223,
      "step": 977
    },
    {
      "epoch": 0.18282082437610991,
      "grad_norm": 5.820496082305908,
      "learning_rate": 0.00012695327102803738,
      "loss": 0.2483,
      "step": 978
    },
    {
      "epoch": 0.1830077577343677,
      "grad_norm": 4.210005760192871,
      "learning_rate": 0.0001268785046728972,
      "loss": 0.3119,
      "step": 979
    },
    {
      "epoch": 0.18319469109262548,
      "grad_norm": 3.0010101795196533,
      "learning_rate": 0.00012680373831775701,
      "loss": 0.1977,
      "step": 980
    },
    {
      "epoch": 0.18338162445088327,
      "grad_norm": 4.168623447418213,
      "learning_rate": 0.00012672897196261683,
      "loss": 0.269,
      "step": 981
    },
    {
      "epoch": 0.18356855780914105,
      "grad_norm": 2.1520848274230957,
      "learning_rate": 0.00012665420560747665,
      "loss": 0.2911,
      "step": 982
    },
    {
      "epoch": 0.1837554911673988,
      "grad_norm": 2.5883822441101074,
      "learning_rate": 0.00012657943925233647,
      "loss": 0.2756,
      "step": 983
    },
    {
      "epoch": 0.1839424245256566,
      "grad_norm": 2.7756423950195312,
      "learning_rate": 0.00012650467289719626,
      "loss": 0.1843,
      "step": 984
    },
    {
      "epoch": 0.18412935788391438,
      "grad_norm": 2.732259750366211,
      "learning_rate": 0.00012642990654205608,
      "loss": 0.1019,
      "step": 985
    },
    {
      "epoch": 0.18431629124217216,
      "grad_norm": 2.901228904724121,
      "learning_rate": 0.0001263551401869159,
      "loss": 0.2889,
      "step": 986
    },
    {
      "epoch": 0.18450322460042995,
      "grad_norm": 3.124490737915039,
      "learning_rate": 0.00012628037383177572,
      "loss": 0.218,
      "step": 987
    },
    {
      "epoch": 0.18469015795868773,
      "grad_norm": 9.26938247680664,
      "learning_rate": 0.0001262056074766355,
      "loss": 0.5964,
      "step": 988
    },
    {
      "epoch": 0.18487709131694552,
      "grad_norm": 4.048557281494141,
      "learning_rate": 0.00012613084112149533,
      "loss": 0.1851,
      "step": 989
    },
    {
      "epoch": 0.1850640246752033,
      "grad_norm": 3.402533769607544,
      "learning_rate": 0.00012605607476635515,
      "loss": 0.1283,
      "step": 990
    },
    {
      "epoch": 0.18525095803346106,
      "grad_norm": 3.2713398933410645,
      "learning_rate": 0.00012598130841121497,
      "loss": 0.188,
      "step": 991
    },
    {
      "epoch": 0.18543789139171885,
      "grad_norm": 2.2751123905181885,
      "learning_rate": 0.00012590654205607476,
      "loss": 0.2198,
      "step": 992
    },
    {
      "epoch": 0.18562482474997663,
      "grad_norm": 1.4858864545822144,
      "learning_rate": 0.00012583177570093458,
      "loss": 0.175,
      "step": 993
    },
    {
      "epoch": 0.18581175810823441,
      "grad_norm": 1.5846078395843506,
      "learning_rate": 0.0001257570093457944,
      "loss": 0.1094,
      "step": 994
    },
    {
      "epoch": 0.1859986914664922,
      "grad_norm": 3.0573172569274902,
      "learning_rate": 0.00012568224299065422,
      "loss": 0.2295,
      "step": 995
    },
    {
      "epoch": 0.18618562482474998,
      "grad_norm": 5.208429336547852,
      "learning_rate": 0.000125607476635514,
      "loss": 0.1612,
      "step": 996
    },
    {
      "epoch": 0.18637255818300777,
      "grad_norm": 4.279448509216309,
      "learning_rate": 0.00012553271028037383,
      "loss": 0.1816,
      "step": 997
    },
    {
      "epoch": 0.18655949154126553,
      "grad_norm": 6.849042892456055,
      "learning_rate": 0.00012545794392523365,
      "loss": 0.2146,
      "step": 998
    },
    {
      "epoch": 0.1867464248995233,
      "grad_norm": 1.962110996246338,
      "learning_rate": 0.00012538317757009346,
      "loss": 0.3169,
      "step": 999
    },
    {
      "epoch": 0.1869333582577811,
      "grad_norm": 3.0877914428710938,
      "learning_rate": 0.00012530841121495326,
      "loss": 0.3188,
      "step": 1000
    },
    {
      "epoch": 0.18712029161603888,
      "grad_norm": 3.2375411987304688,
      "learning_rate": 0.00012523364485981308,
      "loss": 0.309,
      "step": 1001
    },
    {
      "epoch": 0.18730722497429667,
      "grad_norm": 3.2650485038757324,
      "learning_rate": 0.0001251588785046729,
      "loss": 0.2868,
      "step": 1002
    },
    {
      "epoch": 0.18749415833255445,
      "grad_norm": 3.899991273880005,
      "learning_rate": 0.0001250841121495327,
      "loss": 0.32,
      "step": 1003
    },
    {
      "epoch": 0.18768109169081224,
      "grad_norm": 1.736607313156128,
      "learning_rate": 0.0001250093457943925,
      "loss": 0.2111,
      "step": 1004
    },
    {
      "epoch": 0.18786802504907002,
      "grad_norm": 2.3783493041992188,
      "learning_rate": 0.00012493457943925235,
      "loss": 0.1956,
      "step": 1005
    },
    {
      "epoch": 0.18805495840732778,
      "grad_norm": 5.88011360168457,
      "learning_rate": 0.00012485981308411217,
      "loss": 0.2535,
      "step": 1006
    },
    {
      "epoch": 0.18824189176558556,
      "grad_norm": 1.0087711811065674,
      "learning_rate": 0.000124785046728972,
      "loss": 0.1046,
      "step": 1007
    },
    {
      "epoch": 0.18842882512384335,
      "grad_norm": 3.1063120365142822,
      "learning_rate": 0.00012471028037383178,
      "loss": 0.3144,
      "step": 1008
    },
    {
      "epoch": 0.18861575848210113,
      "grad_norm": 1.7706542015075684,
      "learning_rate": 0.0001246355140186916,
      "loss": 0.2188,
      "step": 1009
    },
    {
      "epoch": 0.18880269184035892,
      "grad_norm": 1.5062175989151,
      "learning_rate": 0.00012456074766355142,
      "loss": 0.1612,
      "step": 1010
    },
    {
      "epoch": 0.1889896251986167,
      "grad_norm": 2.6269474029541016,
      "learning_rate": 0.00012448598130841124,
      "loss": 0.1723,
      "step": 1011
    },
    {
      "epoch": 0.18917655855687449,
      "grad_norm": 1.8469674587249756,
      "learning_rate": 0.00012441121495327103,
      "loss": 0.2749,
      "step": 1012
    },
    {
      "epoch": 0.18936349191513224,
      "grad_norm": 3.58357572555542,
      "learning_rate": 0.00012433644859813085,
      "loss": 0.1503,
      "step": 1013
    },
    {
      "epoch": 0.18955042527339003,
      "grad_norm": 4.281968116760254,
      "learning_rate": 0.00012426168224299067,
      "loss": 0.112,
      "step": 1014
    },
    {
      "epoch": 0.1897373586316478,
      "grad_norm": 3.758493423461914,
      "learning_rate": 0.00012418691588785049,
      "loss": 0.235,
      "step": 1015
    },
    {
      "epoch": 0.1899242919899056,
      "grad_norm": 5.374390602111816,
      "learning_rate": 0.00012411214953271028,
      "loss": 0.2322,
      "step": 1016
    },
    {
      "epoch": 0.19011122534816338,
      "grad_norm": 2.439850091934204,
      "learning_rate": 0.0001240373831775701,
      "loss": 0.1822,
      "step": 1017
    },
    {
      "epoch": 0.19029815870642117,
      "grad_norm": 2.8066787719726562,
      "learning_rate": 0.00012396261682242991,
      "loss": 0.1747,
      "step": 1018
    },
    {
      "epoch": 0.19048509206467895,
      "grad_norm": 3.0641610622406006,
      "learning_rate": 0.00012388785046728973,
      "loss": 0.2023,
      "step": 1019
    },
    {
      "epoch": 0.19067202542293674,
      "grad_norm": 1.3964799642562866,
      "learning_rate": 0.00012381308411214953,
      "loss": 0.1223,
      "step": 1020
    },
    {
      "epoch": 0.1908589587811945,
      "grad_norm": 5.462515354156494,
      "learning_rate": 0.00012373831775700934,
      "loss": 0.16,
      "step": 1021
    },
    {
      "epoch": 0.19104589213945228,
      "grad_norm": 16.120466232299805,
      "learning_rate": 0.00012366355140186916,
      "loss": 0.3512,
      "step": 1022
    },
    {
      "epoch": 0.19123282549771006,
      "grad_norm": 4.786281108856201,
      "learning_rate": 0.00012358878504672898,
      "loss": 0.2383,
      "step": 1023
    },
    {
      "epoch": 0.19141975885596785,
      "grad_norm": 2.9814531803131104,
      "learning_rate": 0.00012351401869158877,
      "loss": 0.2654,
      "step": 1024
    },
    {
      "epoch": 0.19160669221422563,
      "grad_norm": 5.851796627044678,
      "learning_rate": 0.0001234392523364486,
      "loss": 0.3997,
      "step": 1025
    },
    {
      "epoch": 0.19179362557248342,
      "grad_norm": 6.682172775268555,
      "learning_rate": 0.0001233644859813084,
      "loss": 0.4491,
      "step": 1026
    },
    {
      "epoch": 0.1919805589307412,
      "grad_norm": 5.5275750160217285,
      "learning_rate": 0.00012328971962616823,
      "loss": 0.3882,
      "step": 1027
    },
    {
      "epoch": 0.19216749228899896,
      "grad_norm": 11.503844261169434,
      "learning_rate": 0.00012321495327102802,
      "loss": 0.312,
      "step": 1028
    },
    {
      "epoch": 0.19235442564725674,
      "grad_norm": 2.1949410438537598,
      "learning_rate": 0.00012314018691588787,
      "loss": 0.2721,
      "step": 1029
    },
    {
      "epoch": 0.19254135900551453,
      "grad_norm": 3.5068492889404297,
      "learning_rate": 0.0001230654205607477,
      "loss": 0.2342,
      "step": 1030
    },
    {
      "epoch": 0.1927282923637723,
      "grad_norm": 5.062213897705078,
      "learning_rate": 0.00012299065420560748,
      "loss": 0.217,
      "step": 1031
    },
    {
      "epoch": 0.1929152257220301,
      "grad_norm": 3.1327738761901855,
      "learning_rate": 0.0001229158878504673,
      "loss": 0.236,
      "step": 1032
    },
    {
      "epoch": 0.19310215908028788,
      "grad_norm": 4.8202338218688965,
      "learning_rate": 0.00012284112149532712,
      "loss": 0.2991,
      "step": 1033
    },
    {
      "epoch": 0.19328909243854567,
      "grad_norm": 2.5317294597625732,
      "learning_rate": 0.00012276635514018694,
      "loss": 0.2615,
      "step": 1034
    },
    {
      "epoch": 0.19347602579680345,
      "grad_norm": 0.6569936275482178,
      "learning_rate": 0.00012269158878504673,
      "loss": 0.0764,
      "step": 1035
    },
    {
      "epoch": 0.1936629591550612,
      "grad_norm": 1.8481467962265015,
      "learning_rate": 0.00012261682242990655,
      "loss": 0.1908,
      "step": 1036
    },
    {
      "epoch": 0.193849892513319,
      "grad_norm": 2.483029365539551,
      "learning_rate": 0.00012254205607476636,
      "loss": 0.1899,
      "step": 1037
    },
    {
      "epoch": 0.19403682587157678,
      "grad_norm": 1.1993368864059448,
      "learning_rate": 0.00012246728971962618,
      "loss": 0.177,
      "step": 1038
    },
    {
      "epoch": 0.19422375922983456,
      "grad_norm": 1.9943945407867432,
      "learning_rate": 0.000122392523364486,
      "loss": 0.2245,
      "step": 1039
    },
    {
      "epoch": 0.19441069258809235,
      "grad_norm": 2.254851818084717,
      "learning_rate": 0.0001223177570093458,
      "loss": 0.2556,
      "step": 1040
    },
    {
      "epoch": 0.19459762594635013,
      "grad_norm": 2.714324951171875,
      "learning_rate": 0.0001222429906542056,
      "loss": 0.2025,
      "step": 1041
    },
    {
      "epoch": 0.19478455930460792,
      "grad_norm": 3.2570154666900635,
      "learning_rate": 0.00012216822429906543,
      "loss": 0.3632,
      "step": 1042
    },
    {
      "epoch": 0.19497149266286568,
      "grad_norm": 2.564685344696045,
      "learning_rate": 0.00012209345794392525,
      "loss": 0.2924,
      "step": 1043
    },
    {
      "epoch": 0.19515842602112346,
      "grad_norm": 17.024761199951172,
      "learning_rate": 0.00012201869158878506,
      "loss": 0.2654,
      "step": 1044
    },
    {
      "epoch": 0.19534535937938124,
      "grad_norm": 2.285663604736328,
      "learning_rate": 0.00012194392523364486,
      "loss": 0.1552,
      "step": 1045
    },
    {
      "epoch": 0.19553229273763903,
      "grad_norm": 3.9532713890075684,
      "learning_rate": 0.00012186915887850468,
      "loss": 0.2904,
      "step": 1046
    },
    {
      "epoch": 0.19571922609589681,
      "grad_norm": 3.658085823059082,
      "learning_rate": 0.00012179439252336449,
      "loss": 0.1592,
      "step": 1047
    },
    {
      "epoch": 0.1959061594541546,
      "grad_norm": 8.149148941040039,
      "learning_rate": 0.0001217196261682243,
      "loss": 0.2446,
      "step": 1048
    },
    {
      "epoch": 0.19609309281241238,
      "grad_norm": 3.5870182514190674,
      "learning_rate": 0.00012164485981308411,
      "loss": 0.2341,
      "step": 1049
    },
    {
      "epoch": 0.19628002617067017,
      "grad_norm": 8.943353652954102,
      "learning_rate": 0.00012157009345794393,
      "loss": 0.2808,
      "step": 1050
    },
    {
      "epoch": 0.19646695952892793,
      "grad_norm": 5.024774551391602,
      "learning_rate": 0.00012149532710280373,
      "loss": 0.3248,
      "step": 1051
    },
    {
      "epoch": 0.1966538928871857,
      "grad_norm": 4.466785430908203,
      "learning_rate": 0.00012142056074766355,
      "loss": 0.157,
      "step": 1052
    },
    {
      "epoch": 0.1968408262454435,
      "grad_norm": 4.576449394226074,
      "learning_rate": 0.00012134579439252336,
      "loss": 0.2606,
      "step": 1053
    },
    {
      "epoch": 0.19702775960370128,
      "grad_norm": 2.544327974319458,
      "learning_rate": 0.00012127102803738319,
      "loss": 0.2693,
      "step": 1054
    },
    {
      "epoch": 0.19721469296195906,
      "grad_norm": 1.2216836214065552,
      "learning_rate": 0.00012119626168224301,
      "loss": 0.1435,
      "step": 1055
    },
    {
      "epoch": 0.19740162632021685,
      "grad_norm": 3.25557017326355,
      "learning_rate": 0.00012112149532710281,
      "loss": 0.3077,
      "step": 1056
    },
    {
      "epoch": 0.19758855967847463,
      "grad_norm": 7.855559825897217,
      "learning_rate": 0.00012104672897196263,
      "loss": 0.1999,
      "step": 1057
    },
    {
      "epoch": 0.1977754930367324,
      "grad_norm": 4.18308687210083,
      "learning_rate": 0.00012097196261682244,
      "loss": 0.2445,
      "step": 1058
    },
    {
      "epoch": 0.19796242639499018,
      "grad_norm": 2.1484644412994385,
      "learning_rate": 0.00012089719626168226,
      "loss": 0.1896,
      "step": 1059
    },
    {
      "epoch": 0.19814935975324796,
      "grad_norm": 2.511073112487793,
      "learning_rate": 0.00012082242990654206,
      "loss": 0.1697,
      "step": 1060
    },
    {
      "epoch": 0.19833629311150575,
      "grad_norm": 1.532273530960083,
      "learning_rate": 0.00012074766355140188,
      "loss": 0.2185,
      "step": 1061
    },
    {
      "epoch": 0.19852322646976353,
      "grad_norm": 2.4454503059387207,
      "learning_rate": 0.00012067289719626169,
      "loss": 0.3991,
      "step": 1062
    },
    {
      "epoch": 0.19871015982802132,
      "grad_norm": 2.6843678951263428,
      "learning_rate": 0.0001205981308411215,
      "loss": 0.1182,
      "step": 1063
    },
    {
      "epoch": 0.1988970931862791,
      "grad_norm": 3.8198747634887695,
      "learning_rate": 0.00012052336448598131,
      "loss": 0.1957,
      "step": 1064
    },
    {
      "epoch": 0.19908402654453689,
      "grad_norm": 4.008790016174316,
      "learning_rate": 0.00012044859813084113,
      "loss": 0.082,
      "step": 1065
    },
    {
      "epoch": 0.19927095990279464,
      "grad_norm": 2.7227206230163574,
      "learning_rate": 0.00012037383177570094,
      "loss": 0.2747,
      "step": 1066
    },
    {
      "epoch": 0.19945789326105243,
      "grad_norm": 2.2342262268066406,
      "learning_rate": 0.00012029906542056075,
      "loss": 0.1139,
      "step": 1067
    },
    {
      "epoch": 0.1996448266193102,
      "grad_norm": 5.013533592224121,
      "learning_rate": 0.00012022429906542056,
      "loss": 0.2563,
      "step": 1068
    },
    {
      "epoch": 0.199831759977568,
      "grad_norm": 3.0236027240753174,
      "learning_rate": 0.00012014953271028038,
      "loss": 0.2467,
      "step": 1069
    },
    {
      "epoch": 0.20001869333582578,
      "grad_norm": 1.9630695581436157,
      "learning_rate": 0.00012007476635514018,
      "loss": 0.1515,
      "step": 1070
    },
    {
      "epoch": 0.20020562669408357,
      "grad_norm": 10.193901062011719,
      "learning_rate": 0.00012,
      "loss": 0.2496,
      "step": 1071
    },
    {
      "epoch": 0.20039256005234135,
      "grad_norm": 6.201713562011719,
      "learning_rate": 0.00011992523364485981,
      "loss": 0.2469,
      "step": 1072
    },
    {
      "epoch": 0.2005794934105991,
      "grad_norm": 11.818572044372559,
      "learning_rate": 0.00011985046728971963,
      "loss": 0.3011,
      "step": 1073
    },
    {
      "epoch": 0.2007664267688569,
      "grad_norm": 5.195681571960449,
      "learning_rate": 0.00011977570093457943,
      "loss": 0.3458,
      "step": 1074
    },
    {
      "epoch": 0.20095336012711468,
      "grad_norm": 1.482431411743164,
      "learning_rate": 0.00011970093457943925,
      "loss": 0.1436,
      "step": 1075
    },
    {
      "epoch": 0.20114029348537246,
      "grad_norm": 1.7835330963134766,
      "learning_rate": 0.00011962616822429906,
      "loss": 0.1996,
      "step": 1076
    },
    {
      "epoch": 0.20132722684363025,
      "grad_norm": 10.063875198364258,
      "learning_rate": 0.00011955140186915888,
      "loss": 0.3985,
      "step": 1077
    },
    {
      "epoch": 0.20151416020188803,
      "grad_norm": 9.955543518066406,
      "learning_rate": 0.00011947663551401868,
      "loss": 0.2502,
      "step": 1078
    },
    {
      "epoch": 0.20170109356014582,
      "grad_norm": 5.427773952484131,
      "learning_rate": 0.00011940186915887853,
      "loss": 0.2832,
      "step": 1079
    },
    {
      "epoch": 0.2018880269184036,
      "grad_norm": 4.539680480957031,
      "learning_rate": 0.00011932710280373833,
      "loss": 0.2604,
      "step": 1080
    },
    {
      "epoch": 0.20207496027666136,
      "grad_norm": 2.3991854190826416,
      "learning_rate": 0.00011925233644859815,
      "loss": 0.2635,
      "step": 1081
    },
    {
      "epoch": 0.20226189363491914,
      "grad_norm": 4.374992370605469,
      "learning_rate": 0.00011917757009345796,
      "loss": 0.3638,
      "step": 1082
    },
    {
      "epoch": 0.20244882699317693,
      "grad_norm": 3.866307020187378,
      "learning_rate": 0.00011910280373831778,
      "loss": 0.172,
      "step": 1083
    },
    {
      "epoch": 0.2026357603514347,
      "grad_norm": 1.4688057899475098,
      "learning_rate": 0.00011902803738317758,
      "loss": 0.1685,
      "step": 1084
    },
    {
      "epoch": 0.2028226937096925,
      "grad_norm": 3.1081960201263428,
      "learning_rate": 0.0001189532710280374,
      "loss": 0.2834,
      "step": 1085
    },
    {
      "epoch": 0.20300962706795028,
      "grad_norm": 2.502210855484009,
      "learning_rate": 0.0001188785046728972,
      "loss": 0.2107,
      "step": 1086
    },
    {
      "epoch": 0.20319656042620807,
      "grad_norm": 2.1743509769439697,
      "learning_rate": 0.00011880373831775702,
      "loss": 0.2186,
      "step": 1087
    },
    {
      "epoch": 0.20338349378446582,
      "grad_norm": 1.46580970287323,
      "learning_rate": 0.00011872897196261683,
      "loss": 0.2479,
      "step": 1088
    },
    {
      "epoch": 0.2035704271427236,
      "grad_norm": 6.2502217292785645,
      "learning_rate": 0.00011865420560747665,
      "loss": 0.1861,
      "step": 1089
    },
    {
      "epoch": 0.2037573605009814,
      "grad_norm": 4.668541431427002,
      "learning_rate": 0.00011857943925233645,
      "loss": 0.1962,
      "step": 1090
    },
    {
      "epoch": 0.20394429385923918,
      "grad_norm": 1.662613034248352,
      "learning_rate": 0.00011850467289719627,
      "loss": 0.1816,
      "step": 1091
    },
    {
      "epoch": 0.20413122721749696,
      "grad_norm": 2.6010632514953613,
      "learning_rate": 0.00011842990654205608,
      "loss": 0.2214,
      "step": 1092
    },
    {
      "epoch": 0.20431816057575475,
      "grad_norm": 3.654625654220581,
      "learning_rate": 0.0001183551401869159,
      "loss": 0.2429,
      "step": 1093
    },
    {
      "epoch": 0.20450509393401253,
      "grad_norm": 2.701164484024048,
      "learning_rate": 0.0001182803738317757,
      "loss": 0.2753,
      "step": 1094
    },
    {
      "epoch": 0.20469202729227032,
      "grad_norm": 1.9101264476776123,
      "learning_rate": 0.00011820560747663552,
      "loss": 0.0835,
      "step": 1095
    },
    {
      "epoch": 0.20487896065052807,
      "grad_norm": 5.7314677238464355,
      "learning_rate": 0.00011813084112149533,
      "loss": 0.2197,
      "step": 1096
    },
    {
      "epoch": 0.20506589400878586,
      "grad_norm": 3.242879629135132,
      "learning_rate": 0.00011805607476635514,
      "loss": 0.2018,
      "step": 1097
    },
    {
      "epoch": 0.20525282736704364,
      "grad_norm": 8.72826862335205,
      "learning_rate": 0.00011798130841121495,
      "loss": 0.2207,
      "step": 1098
    },
    {
      "epoch": 0.20543976072530143,
      "grad_norm": 4.412341117858887,
      "learning_rate": 0.00011790654205607477,
      "loss": 0.1663,
      "step": 1099
    },
    {
      "epoch": 0.2056266940835592,
      "grad_norm": 3.1213464736938477,
      "learning_rate": 0.00011783177570093457,
      "loss": 0.1138,
      "step": 1100
    },
    {
      "epoch": 0.205813627441817,
      "grad_norm": 2.2283804416656494,
      "learning_rate": 0.00011775700934579439,
      "loss": 0.1894,
      "step": 1101
    },
    {
      "epoch": 0.20600056080007478,
      "grad_norm": 4.156409740447998,
      "learning_rate": 0.0001176822429906542,
      "loss": 0.212,
      "step": 1102
    },
    {
      "epoch": 0.20618749415833257,
      "grad_norm": 4.226345539093018,
      "learning_rate": 0.00011760747663551402,
      "loss": 0.2197,
      "step": 1103
    },
    {
      "epoch": 0.20637442751659033,
      "grad_norm": 3.0316593647003174,
      "learning_rate": 0.00011753271028037385,
      "loss": 0.2803,
      "step": 1104
    },
    {
      "epoch": 0.2065613608748481,
      "grad_norm": 4.820520877838135,
      "learning_rate": 0.00011745794392523365,
      "loss": 0.2086,
      "step": 1105
    },
    {
      "epoch": 0.2067482942331059,
      "grad_norm": 1.1106269359588623,
      "learning_rate": 0.00011738317757009347,
      "loss": 0.0968,
      "step": 1106
    },
    {
      "epoch": 0.20693522759136368,
      "grad_norm": 3.581394672393799,
      "learning_rate": 0.00011730841121495328,
      "loss": 0.1981,
      "step": 1107
    },
    {
      "epoch": 0.20712216094962146,
      "grad_norm": 4.428853988647461,
      "learning_rate": 0.0001172336448598131,
      "loss": 0.2539,
      "step": 1108
    },
    {
      "epoch": 0.20730909430787925,
      "grad_norm": 4.6242194175720215,
      "learning_rate": 0.0001171588785046729,
      "loss": 0.314,
      "step": 1109
    },
    {
      "epoch": 0.20749602766613703,
      "grad_norm": 2.626207113265991,
      "learning_rate": 0.00011708411214953272,
      "loss": 0.1934,
      "step": 1110
    },
    {
      "epoch": 0.2076829610243948,
      "grad_norm": 2.761115312576294,
      "learning_rate": 0.00011700934579439253,
      "loss": 0.167,
      "step": 1111
    },
    {
      "epoch": 0.20786989438265258,
      "grad_norm": 3.4149649143218994,
      "learning_rate": 0.00011693457943925235,
      "loss": 0.1472,
      "step": 1112
    },
    {
      "epoch": 0.20805682774091036,
      "grad_norm": 2.1226985454559326,
      "learning_rate": 0.00011685981308411215,
      "loss": 0.1749,
      "step": 1113
    },
    {
      "epoch": 0.20824376109916815,
      "grad_norm": 2.471534013748169,
      "learning_rate": 0.00011678504672897197,
      "loss": 0.1452,
      "step": 1114
    },
    {
      "epoch": 0.20843069445742593,
      "grad_norm": 1.3646600246429443,
      "learning_rate": 0.00011671028037383178,
      "loss": 0.1175,
      "step": 1115
    },
    {
      "epoch": 0.20861762781568371,
      "grad_norm": 2.5278258323669434,
      "learning_rate": 0.0001166355140186916,
      "loss": 0.2175,
      "step": 1116
    },
    {
      "epoch": 0.2088045611739415,
      "grad_norm": 5.097423076629639,
      "learning_rate": 0.0001165607476635514,
      "loss": 0.3092,
      "step": 1117
    },
    {
      "epoch": 0.20899149453219928,
      "grad_norm": 1.4741272926330566,
      "learning_rate": 0.00011648598130841122,
      "loss": 0.1512,
      "step": 1118
    },
    {
      "epoch": 0.20917842789045704,
      "grad_norm": 4.724228858947754,
      "learning_rate": 0.00011641121495327102,
      "loss": 0.1234,
      "step": 1119
    },
    {
      "epoch": 0.20936536124871483,
      "grad_norm": 2.739447593688965,
      "learning_rate": 0.00011633644859813084,
      "loss": 0.121,
      "step": 1120
    },
    {
      "epoch": 0.2095522946069726,
      "grad_norm": 3.1868534088134766,
      "learning_rate": 0.00011626168224299065,
      "loss": 0.2003,
      "step": 1121
    },
    {
      "epoch": 0.2097392279652304,
      "grad_norm": 2.9606375694274902,
      "learning_rate": 0.00011618691588785047,
      "loss": 0.2855,
      "step": 1122
    },
    {
      "epoch": 0.20992616132348818,
      "grad_norm": 1.57218599319458,
      "learning_rate": 0.00011611214953271029,
      "loss": 0.0971,
      "step": 1123
    },
    {
      "epoch": 0.21011309468174597,
      "grad_norm": 4.050980091094971,
      "learning_rate": 0.00011603738317757009,
      "loss": 0.2459,
      "step": 1124
    },
    {
      "epoch": 0.21030002804000375,
      "grad_norm": 5.773434638977051,
      "learning_rate": 0.00011596261682242991,
      "loss": 0.2877,
      "step": 1125
    },
    {
      "epoch": 0.2104869613982615,
      "grad_norm": 4.602896690368652,
      "learning_rate": 0.00011588785046728972,
      "loss": 0.2343,
      "step": 1126
    },
    {
      "epoch": 0.2106738947565193,
      "grad_norm": 3.9136619567871094,
      "learning_rate": 0.00011581308411214953,
      "loss": 0.3317,
      "step": 1127
    },
    {
      "epoch": 0.21086082811477708,
      "grad_norm": 3.364647388458252,
      "learning_rate": 0.00011573831775700937,
      "loss": 0.199,
      "step": 1128
    },
    {
      "epoch": 0.21104776147303486,
      "grad_norm": 2.6044297218322754,
      "learning_rate": 0.00011566355140186917,
      "loss": 0.1333,
      "step": 1129
    },
    {
      "epoch": 0.21123469483129265,
      "grad_norm": 1.4637094736099243,
      "learning_rate": 0.00011558878504672899,
      "loss": 0.1545,
      "step": 1130
    },
    {
      "epoch": 0.21142162818955043,
      "grad_norm": 1.8837701082229614,
      "learning_rate": 0.0001155140186915888,
      "loss": 0.2468,
      "step": 1131
    },
    {
      "epoch": 0.21160856154780822,
      "grad_norm": 3.0408079624176025,
      "learning_rate": 0.00011543925233644862,
      "loss": 0.2177,
      "step": 1132
    },
    {
      "epoch": 0.211795494906066,
      "grad_norm": 2.133220672607422,
      "learning_rate": 0.00011536448598130842,
      "loss": 0.1721,
      "step": 1133
    },
    {
      "epoch": 0.21198242826432376,
      "grad_norm": 1.1284161806106567,
      "learning_rate": 0.00011528971962616824,
      "loss": 0.1332,
      "step": 1134
    },
    {
      "epoch": 0.21216936162258154,
      "grad_norm": 4.358639240264893,
      "learning_rate": 0.00011521495327102804,
      "loss": 0.4068,
      "step": 1135
    },
    {
      "epoch": 0.21235629498083933,
      "grad_norm": 1.9509634971618652,
      "learning_rate": 0.00011514018691588786,
      "loss": 0.271,
      "step": 1136
    },
    {
      "epoch": 0.2125432283390971,
      "grad_norm": 4.148954391479492,
      "learning_rate": 0.00011506542056074767,
      "loss": 0.2506,
      "step": 1137
    },
    {
      "epoch": 0.2127301616973549,
      "grad_norm": 4.206488132476807,
      "learning_rate": 0.00011499065420560749,
      "loss": 0.2796,
      "step": 1138
    },
    {
      "epoch": 0.21291709505561268,
      "grad_norm": 3.159616231918335,
      "learning_rate": 0.00011491588785046729,
      "loss": 0.2737,
      "step": 1139
    },
    {
      "epoch": 0.21310402841387047,
      "grad_norm": 2.844503879547119,
      "learning_rate": 0.00011484112149532711,
      "loss": 0.2259,
      "step": 1140
    },
    {
      "epoch": 0.21329096177212822,
      "grad_norm": 2.3523001670837402,
      "learning_rate": 0.00011476635514018692,
      "loss": 0.231,
      "step": 1141
    },
    {
      "epoch": 0.213477895130386,
      "grad_norm": 2.1709461212158203,
      "learning_rate": 0.00011469158878504674,
      "loss": 0.1424,
      "step": 1142
    },
    {
      "epoch": 0.2136648284886438,
      "grad_norm": 2.7558090686798096,
      "learning_rate": 0.00011461682242990654,
      "loss": 0.1958,
      "step": 1143
    },
    {
      "epoch": 0.21385176184690158,
      "grad_norm": 1.8441039323806763,
      "learning_rate": 0.00011454205607476636,
      "loss": 0.1364,
      "step": 1144
    },
    {
      "epoch": 0.21403869520515936,
      "grad_norm": 4.474987506866455,
      "learning_rate": 0.00011446728971962617,
      "loss": 0.1652,
      "step": 1145
    },
    {
      "epoch": 0.21422562856341715,
      "grad_norm": 3.897064208984375,
      "learning_rate": 0.00011439252336448598,
      "loss": 0.3365,
      "step": 1146
    },
    {
      "epoch": 0.21441256192167493,
      "grad_norm": 2.5156497955322266,
      "learning_rate": 0.00011431775700934579,
      "loss": 0.292,
      "step": 1147
    },
    {
      "epoch": 0.21459949527993272,
      "grad_norm": 2.1160051822662354,
      "learning_rate": 0.00011424299065420561,
      "loss": 0.1703,
      "step": 1148
    },
    {
      "epoch": 0.21478642863819047,
      "grad_norm": 3.670098304748535,
      "learning_rate": 0.00011416822429906541,
      "loss": 0.2037,
      "step": 1149
    },
    {
      "epoch": 0.21497336199644826,
      "grad_norm": 2.018491268157959,
      "learning_rate": 0.00011409345794392523,
      "loss": 0.2306,
      "step": 1150
    },
    {
      "epoch": 0.21516029535470604,
      "grad_norm": 4.278860092163086,
      "learning_rate": 0.00011401869158878504,
      "loss": 0.1127,
      "step": 1151
    },
    {
      "epoch": 0.21534722871296383,
      "grad_norm": 4.319744110107422,
      "learning_rate": 0.00011394392523364486,
      "loss": 0.249,
      "step": 1152
    },
    {
      "epoch": 0.2155341620712216,
      "grad_norm": 3.3803038597106934,
      "learning_rate": 0.00011386915887850469,
      "loss": 0.1937,
      "step": 1153
    },
    {
      "epoch": 0.2157210954294794,
      "grad_norm": 5.295698165893555,
      "learning_rate": 0.00011379439252336451,
      "loss": 0.2463,
      "step": 1154
    },
    {
      "epoch": 0.21590802878773718,
      "grad_norm": 4.553648948669434,
      "learning_rate": 0.00011371962616822431,
      "loss": 0.3128,
      "step": 1155
    },
    {
      "epoch": 0.21609496214599494,
      "grad_norm": 2.492940902709961,
      "learning_rate": 0.00011364485981308413,
      "loss": 0.0824,
      "step": 1156
    },
    {
      "epoch": 0.21628189550425272,
      "grad_norm": 3.561039447784424,
      "learning_rate": 0.00011357009345794394,
      "loss": 0.0875,
      "step": 1157
    },
    {
      "epoch": 0.2164688288625105,
      "grad_norm": 1.239122986793518,
      "learning_rate": 0.00011349532710280376,
      "loss": 0.1103,
      "step": 1158
    },
    {
      "epoch": 0.2166557622207683,
      "grad_norm": 2.1552319526672363,
      "learning_rate": 0.00011342056074766356,
      "loss": 0.1569,
      "step": 1159
    },
    {
      "epoch": 0.21684269557902608,
      "grad_norm": 1.5766875743865967,
      "learning_rate": 0.00011334579439252338,
      "loss": 0.1004,
      "step": 1160
    },
    {
      "epoch": 0.21702962893728386,
      "grad_norm": 5.680238246917725,
      "learning_rate": 0.00011327102803738319,
      "loss": 0.2419,
      "step": 1161
    },
    {
      "epoch": 0.21721656229554165,
      "grad_norm": 2.6816701889038086,
      "learning_rate": 0.000113196261682243,
      "loss": 0.315,
      "step": 1162
    },
    {
      "epoch": 0.21740349565379943,
      "grad_norm": 3.9063944816589355,
      "learning_rate": 0.00011312149532710281,
      "loss": 0.2698,
      "step": 1163
    },
    {
      "epoch": 0.2175904290120572,
      "grad_norm": 4.187464237213135,
      "learning_rate": 0.00011304672897196263,
      "loss": 0.4673,
      "step": 1164
    },
    {
      "epoch": 0.21777736237031498,
      "grad_norm": 1.753517746925354,
      "learning_rate": 0.00011297196261682243,
      "loss": 0.2599,
      "step": 1165
    },
    {
      "epoch": 0.21796429572857276,
      "grad_norm": 3.4479827880859375,
      "learning_rate": 0.00011289719626168225,
      "loss": 0.3351,
      "step": 1166
    },
    {
      "epoch": 0.21815122908683054,
      "grad_norm": 3.5024359226226807,
      "learning_rate": 0.00011282242990654206,
      "loss": 0.2146,
      "step": 1167
    },
    {
      "epoch": 0.21833816244508833,
      "grad_norm": 4.870087146759033,
      "learning_rate": 0.00011274766355140188,
      "loss": 0.3252,
      "step": 1168
    },
    {
      "epoch": 0.21852509580334611,
      "grad_norm": 2.0228965282440186,
      "learning_rate": 0.00011267289719626168,
      "loss": 0.2593,
      "step": 1169
    },
    {
      "epoch": 0.2187120291616039,
      "grad_norm": 2.9057536125183105,
      "learning_rate": 0.0001125981308411215,
      "loss": 0.2287,
      "step": 1170
    },
    {
      "epoch": 0.21889896251986166,
      "grad_norm": 4.974540710449219,
      "learning_rate": 0.00011252336448598131,
      "loss": 0.4052,
      "step": 1171
    },
    {
      "epoch": 0.21908589587811944,
      "grad_norm": 2.8323769569396973,
      "learning_rate": 0.00011244859813084113,
      "loss": 0.236,
      "step": 1172
    },
    {
      "epoch": 0.21927282923637723,
      "grad_norm": 2.3036656379699707,
      "learning_rate": 0.00011237383177570093,
      "loss": 0.236,
      "step": 1173
    },
    {
      "epoch": 0.219459762594635,
      "grad_norm": 1.2218852043151855,
      "learning_rate": 0.00011229906542056075,
      "loss": 0.1485,
      "step": 1174
    },
    {
      "epoch": 0.2196466959528928,
      "grad_norm": 2.051013231277466,
      "learning_rate": 0.00011222429906542056,
      "loss": 0.2831,
      "step": 1175
    },
    {
      "epoch": 0.21983362931115058,
      "grad_norm": 1.2756034135818481,
      "learning_rate": 0.00011214953271028037,
      "loss": 0.1609,
      "step": 1176
    },
    {
      "epoch": 0.22002056266940836,
      "grad_norm": 1.6097065210342407,
      "learning_rate": 0.00011207476635514018,
      "loss": 0.2602,
      "step": 1177
    },
    {
      "epoch": 0.22020749602766615,
      "grad_norm": 4.023465633392334,
      "learning_rate": 0.00011200000000000001,
      "loss": 0.3003,
      "step": 1178
    },
    {
      "epoch": 0.2203944293859239,
      "grad_norm": 2.9136719703674316,
      "learning_rate": 0.00011192523364485983,
      "loss": 0.1915,
      "step": 1179
    },
    {
      "epoch": 0.2205813627441817,
      "grad_norm": 5.002488613128662,
      "learning_rate": 0.00011185046728971964,
      "loss": 0.3181,
      "step": 1180
    },
    {
      "epoch": 0.22076829610243948,
      "grad_norm": 3.6181905269622803,
      "learning_rate": 0.00011177570093457945,
      "loss": 0.145,
      "step": 1181
    },
    {
      "epoch": 0.22095522946069726,
      "grad_norm": 2.599745750427246,
      "learning_rate": 0.00011170093457943926,
      "loss": 0.205,
      "step": 1182
    },
    {
      "epoch": 0.22114216281895505,
      "grad_norm": 4.718517780303955,
      "learning_rate": 0.00011162616822429908,
      "loss": 0.2297,
      "step": 1183
    },
    {
      "epoch": 0.22132909617721283,
      "grad_norm": 1.2218009233474731,
      "learning_rate": 0.00011155140186915888,
      "loss": 0.1123,
      "step": 1184
    },
    {
      "epoch": 0.22151602953547062,
      "grad_norm": 3.6213738918304443,
      "learning_rate": 0.0001114766355140187,
      "loss": 0.2078,
      "step": 1185
    },
    {
      "epoch": 0.22170296289372837,
      "grad_norm": 6.000616550445557,
      "learning_rate": 0.00011140186915887851,
      "loss": 0.2416,
      "step": 1186
    },
    {
      "epoch": 0.22188989625198616,
      "grad_norm": 4.894949913024902,
      "learning_rate": 0.00011132710280373833,
      "loss": 0.2368,
      "step": 1187
    },
    {
      "epoch": 0.22207682961024394,
      "grad_norm": 6.172778606414795,
      "learning_rate": 0.00011125233644859813,
      "loss": 0.2435,
      "step": 1188
    },
    {
      "epoch": 0.22226376296850173,
      "grad_norm": 3.8798813819885254,
      "learning_rate": 0.00011117757009345795,
      "loss": 0.1688,
      "step": 1189
    },
    {
      "epoch": 0.2224506963267595,
      "grad_norm": 1.8698097467422485,
      "learning_rate": 0.00011110280373831776,
      "loss": 0.2056,
      "step": 1190
    },
    {
      "epoch": 0.2226376296850173,
      "grad_norm": 5.00175142288208,
      "learning_rate": 0.00011102803738317758,
      "loss": 0.1304,
      "step": 1191
    },
    {
      "epoch": 0.22282456304327508,
      "grad_norm": 2.7732067108154297,
      "learning_rate": 0.00011095327102803738,
      "loss": 0.2589,
      "step": 1192
    },
    {
      "epoch": 0.22301149640153287,
      "grad_norm": 3.3780694007873535,
      "learning_rate": 0.0001108785046728972,
      "loss": 0.1591,
      "step": 1193
    },
    {
      "epoch": 0.22319842975979062,
      "grad_norm": 3.7740182876586914,
      "learning_rate": 0.000110803738317757,
      "loss": 0.257,
      "step": 1194
    },
    {
      "epoch": 0.2233853631180484,
      "grad_norm": 1.5986196994781494,
      "learning_rate": 0.00011072897196261682,
      "loss": 0.1456,
      "step": 1195
    },
    {
      "epoch": 0.2235722964763062,
      "grad_norm": 2.5626776218414307,
      "learning_rate": 0.00011065420560747663,
      "loss": 0.2328,
      "step": 1196
    },
    {
      "epoch": 0.22375922983456398,
      "grad_norm": 2.3180694580078125,
      "learning_rate": 0.00011057943925233645,
      "loss": 0.2318,
      "step": 1197
    },
    {
      "epoch": 0.22394616319282176,
      "grad_norm": 2.611578941345215,
      "learning_rate": 0.00011050467289719625,
      "loss": 0.1401,
      "step": 1198
    },
    {
      "epoch": 0.22413309655107955,
      "grad_norm": 2.1842281818389893,
      "learning_rate": 0.00011042990654205607,
      "loss": 0.2375,
      "step": 1199
    },
    {
      "epoch": 0.22432002990933733,
      "grad_norm": 2.6122424602508545,
      "learning_rate": 0.00011035514018691588,
      "loss": 0.3252,
      "step": 1200
    },
    {
      "epoch": 0.2245069632675951,
      "grad_norm": 4.837222099304199,
      "learning_rate": 0.0001102803738317757,
      "loss": 0.2051,
      "step": 1201
    },
    {
      "epoch": 0.22469389662585287,
      "grad_norm": 5.128841876983643,
      "learning_rate": 0.0001102056074766355,
      "loss": 0.2695,
      "step": 1202
    },
    {
      "epoch": 0.22488082998411066,
      "grad_norm": 1.5118499994277954,
      "learning_rate": 0.00011013084112149535,
      "loss": 0.2124,
      "step": 1203
    },
    {
      "epoch": 0.22506776334236844,
      "grad_norm": 2.4777305126190186,
      "learning_rate": 0.00011005607476635515,
      "loss": 0.1813,
      "step": 1204
    },
    {
      "epoch": 0.22525469670062623,
      "grad_norm": 4.146044731140137,
      "learning_rate": 0.00010998130841121497,
      "loss": 0.2883,
      "step": 1205
    },
    {
      "epoch": 0.225441630058884,
      "grad_norm": 1.8382444381713867,
      "learning_rate": 0.00010990654205607478,
      "loss": 0.2308,
      "step": 1206
    },
    {
      "epoch": 0.2256285634171418,
      "grad_norm": 3.8713016510009766,
      "learning_rate": 0.0001098317757009346,
      "loss": 0.3002,
      "step": 1207
    },
    {
      "epoch": 0.22581549677539958,
      "grad_norm": 1.6483514308929443,
      "learning_rate": 0.0001097570093457944,
      "loss": 0.1287,
      "step": 1208
    },
    {
      "epoch": 0.22600243013365734,
      "grad_norm": 2.508382797241211,
      "learning_rate": 0.00010968224299065422,
      "loss": 0.175,
      "step": 1209
    },
    {
      "epoch": 0.22618936349191512,
      "grad_norm": 2.9326822757720947,
      "learning_rate": 0.00010960747663551403,
      "loss": 0.2873,
      "step": 1210
    },
    {
      "epoch": 0.2263762968501729,
      "grad_norm": 3.5246288776397705,
      "learning_rate": 0.00010953271028037384,
      "loss": 0.1585,
      "step": 1211
    },
    {
      "epoch": 0.2265632302084307,
      "grad_norm": 1.4149588346481323,
      "learning_rate": 0.00010945794392523365,
      "loss": 0.1467,
      "step": 1212
    },
    {
      "epoch": 0.22675016356668848,
      "grad_norm": 1.8563003540039062,
      "learning_rate": 0.00010938317757009347,
      "loss": 0.21,
      "step": 1213
    },
    {
      "epoch": 0.22693709692494626,
      "grad_norm": 2.025228261947632,
      "learning_rate": 0.00010930841121495327,
      "loss": 0.2817,
      "step": 1214
    },
    {
      "epoch": 0.22712403028320405,
      "grad_norm": 2.6843178272247314,
      "learning_rate": 0.00010923364485981309,
      "loss": 0.1715,
      "step": 1215
    },
    {
      "epoch": 0.2273109636414618,
      "grad_norm": 2.4443511962890625,
      "learning_rate": 0.0001091588785046729,
      "loss": 0.1421,
      "step": 1216
    },
    {
      "epoch": 0.2274978969997196,
      "grad_norm": 1.0317511558532715,
      "learning_rate": 0.00010908411214953272,
      "loss": 0.1266,
      "step": 1217
    },
    {
      "epoch": 0.22768483035797737,
      "grad_norm": 1.603319525718689,
      "learning_rate": 0.00010900934579439252,
      "loss": 0.1651,
      "step": 1218
    },
    {
      "epoch": 0.22787176371623516,
      "grad_norm": 4.2445855140686035,
      "learning_rate": 0.00010893457943925234,
      "loss": 0.2165,
      "step": 1219
    },
    {
      "epoch": 0.22805869707449294,
      "grad_norm": 2.974715232849121,
      "learning_rate": 0.00010885981308411215,
      "loss": 0.2883,
      "step": 1220
    },
    {
      "epoch": 0.22824563043275073,
      "grad_norm": 1.6637414693832397,
      "learning_rate": 0.00010878504672897197,
      "loss": 0.1743,
      "step": 1221
    },
    {
      "epoch": 0.2284325637910085,
      "grad_norm": 4.027160167694092,
      "learning_rate": 0.00010871028037383177,
      "loss": 0.1392,
      "step": 1222
    },
    {
      "epoch": 0.2286194971492663,
      "grad_norm": 1.395436406135559,
      "learning_rate": 0.00010863551401869159,
      "loss": 0.0893,
      "step": 1223
    },
    {
      "epoch": 0.22880643050752406,
      "grad_norm": 1.6467605829238892,
      "learning_rate": 0.0001085607476635514,
      "loss": 0.1319,
      "step": 1224
    },
    {
      "epoch": 0.22899336386578184,
      "grad_norm": 1.811649203300476,
      "learning_rate": 0.00010848598130841121,
      "loss": 0.1156,
      "step": 1225
    },
    {
      "epoch": 0.22918029722403963,
      "grad_norm": 4.896510124206543,
      "learning_rate": 0.00010841121495327102,
      "loss": 0.2682,
      "step": 1226
    },
    {
      "epoch": 0.2293672305822974,
      "grad_norm": 1.6187622547149658,
      "learning_rate": 0.00010833644859813084,
      "loss": 0.1986,
      "step": 1227
    },
    {
      "epoch": 0.2295541639405552,
      "grad_norm": 7.863338470458984,
      "learning_rate": 0.00010826168224299067,
      "loss": 0.1935,
      "step": 1228
    },
    {
      "epoch": 0.22974109729881298,
      "grad_norm": 14.865132331848145,
      "learning_rate": 0.00010818691588785048,
      "loss": 0.3772,
      "step": 1229
    },
    {
      "epoch": 0.22992803065707076,
      "grad_norm": 4.594022750854492,
      "learning_rate": 0.0001081121495327103,
      "loss": 0.32,
      "step": 1230
    },
    {
      "epoch": 0.23011496401532855,
      "grad_norm": 2.5925309658050537,
      "learning_rate": 0.0001080373831775701,
      "loss": 0.221,
      "step": 1231
    },
    {
      "epoch": 0.2303018973735863,
      "grad_norm": 3.3611361980438232,
      "learning_rate": 0.00010796261682242992,
      "loss": 0.2378,
      "step": 1232
    },
    {
      "epoch": 0.2304888307318441,
      "grad_norm": 3.009890079498291,
      "learning_rate": 0.00010788785046728972,
      "loss": 0.1167,
      "step": 1233
    },
    {
      "epoch": 0.23067576409010188,
      "grad_norm": 3.3102335929870605,
      "learning_rate": 0.00010781308411214954,
      "loss": 0.3553,
      "step": 1234
    },
    {
      "epoch": 0.23086269744835966,
      "grad_norm": 4.9518303871154785,
      "learning_rate": 0.00010773831775700935,
      "loss": 0.2159,
      "step": 1235
    },
    {
      "epoch": 0.23104963080661745,
      "grad_norm": 5.671279430389404,
      "learning_rate": 0.00010766355140186917,
      "loss": 0.3478,
      "step": 1236
    },
    {
      "epoch": 0.23123656416487523,
      "grad_norm": 2.560891628265381,
      "learning_rate": 0.00010758878504672897,
      "loss": 0.2222,
      "step": 1237
    },
    {
      "epoch": 0.23142349752313301,
      "grad_norm": 2.5827317237854004,
      "learning_rate": 0.00010751401869158879,
      "loss": 0.1752,
      "step": 1238
    },
    {
      "epoch": 0.23161043088139077,
      "grad_norm": 1.302910566329956,
      "learning_rate": 0.00010743925233644861,
      "loss": 0.0583,
      "step": 1239
    },
    {
      "epoch": 0.23179736423964856,
      "grad_norm": 2.469918966293335,
      "learning_rate": 0.00010736448598130842,
      "loss": 0.2099,
      "step": 1240
    },
    {
      "epoch": 0.23198429759790634,
      "grad_norm": 2.799238920211792,
      "learning_rate": 0.00010728971962616823,
      "loss": 0.3207,
      "step": 1241
    },
    {
      "epoch": 0.23217123095616413,
      "grad_norm": 2.2870397567749023,
      "learning_rate": 0.00010721495327102804,
      "loss": 0.3086,
      "step": 1242
    },
    {
      "epoch": 0.2323581643144219,
      "grad_norm": 3.078911781311035,
      "learning_rate": 0.00010714018691588786,
      "loss": 0.1754,
      "step": 1243
    },
    {
      "epoch": 0.2325450976726797,
      "grad_norm": 7.241899013519287,
      "learning_rate": 0.00010706542056074766,
      "loss": 0.2876,
      "step": 1244
    },
    {
      "epoch": 0.23273203103093748,
      "grad_norm": 2.4483437538146973,
      "learning_rate": 0.00010699065420560748,
      "loss": 0.2583,
      "step": 1245
    },
    {
      "epoch": 0.23291896438919527,
      "grad_norm": 4.424984931945801,
      "learning_rate": 0.00010691588785046729,
      "loss": 0.3208,
      "step": 1246
    },
    {
      "epoch": 0.23310589774745302,
      "grad_norm": 1.9468058347702026,
      "learning_rate": 0.00010684112149532711,
      "loss": 0.1811,
      "step": 1247
    },
    {
      "epoch": 0.2332928311057108,
      "grad_norm": 4.1555705070495605,
      "learning_rate": 0.00010676635514018691,
      "loss": 0.1753,
      "step": 1248
    },
    {
      "epoch": 0.2334797644639686,
      "grad_norm": 1.885825276374817,
      "learning_rate": 0.00010669158878504673,
      "loss": 0.16,
      "step": 1249
    },
    {
      "epoch": 0.23366669782222638,
      "grad_norm": 2.981257438659668,
      "learning_rate": 0.00010661682242990654,
      "loss": 0.1593,
      "step": 1250
    },
    {
      "epoch": 0.23385363118048416,
      "grad_norm": 3.6588685512542725,
      "learning_rate": 0.00010654205607476636,
      "loss": 0.3065,
      "step": 1251
    },
    {
      "epoch": 0.23404056453874195,
      "grad_norm": 1.6507376432418823,
      "learning_rate": 0.00010646728971962619,
      "loss": 0.1599,
      "step": 1252
    },
    {
      "epoch": 0.23422749789699973,
      "grad_norm": 2.874293804168701,
      "learning_rate": 0.00010639252336448599,
      "loss": 0.282,
      "step": 1253
    },
    {
      "epoch": 0.2344144312552575,
      "grad_norm": 2.4330556392669678,
      "learning_rate": 0.00010631775700934581,
      "loss": 0.2412,
      "step": 1254
    },
    {
      "epoch": 0.23460136461351527,
      "grad_norm": 3.1040356159210205,
      "learning_rate": 0.00010624299065420562,
      "loss": 0.2053,
      "step": 1255
    },
    {
      "epoch": 0.23478829797177306,
      "grad_norm": 7.3984761238098145,
      "learning_rate": 0.00010616822429906544,
      "loss": 0.1388,
      "step": 1256
    },
    {
      "epoch": 0.23497523133003084,
      "grad_norm": 2.3175997734069824,
      "learning_rate": 0.00010609345794392524,
      "loss": 0.1711,
      "step": 1257
    },
    {
      "epoch": 0.23516216468828863,
      "grad_norm": 2.1030666828155518,
      "learning_rate": 0.00010601869158878506,
      "loss": 0.2038,
      "step": 1258
    },
    {
      "epoch": 0.2353490980465464,
      "grad_norm": 2.157639980316162,
      "learning_rate": 0.00010594392523364487,
      "loss": 0.2807,
      "step": 1259
    },
    {
      "epoch": 0.2355360314048042,
      "grad_norm": 3.0055599212646484,
      "learning_rate": 0.00010586915887850468,
      "loss": 0.1743,
      "step": 1260
    },
    {
      "epoch": 0.23572296476306198,
      "grad_norm": 2.6669559478759766,
      "learning_rate": 0.00010579439252336449,
      "loss": 0.253,
      "step": 1261
    },
    {
      "epoch": 0.23590989812131974,
      "grad_norm": 1.623125433921814,
      "learning_rate": 0.00010571962616822431,
      "loss": 0.218,
      "step": 1262
    },
    {
      "epoch": 0.23609683147957752,
      "grad_norm": 4.090718746185303,
      "learning_rate": 0.00010564485981308411,
      "loss": 0.1462,
      "step": 1263
    },
    {
      "epoch": 0.2362837648378353,
      "grad_norm": 2.4341156482696533,
      "learning_rate": 0.00010557009345794393,
      "loss": 0.172,
      "step": 1264
    },
    {
      "epoch": 0.2364706981960931,
      "grad_norm": 8.636808395385742,
      "learning_rate": 0.00010549532710280374,
      "loss": 0.2633,
      "step": 1265
    },
    {
      "epoch": 0.23665763155435088,
      "grad_norm": 2.889085292816162,
      "learning_rate": 0.00010542056074766356,
      "loss": 0.207,
      "step": 1266
    },
    {
      "epoch": 0.23684456491260866,
      "grad_norm": 3.0110528469085693,
      "learning_rate": 0.00010534579439252336,
      "loss": 0.2863,
      "step": 1267
    },
    {
      "epoch": 0.23703149827086645,
      "grad_norm": 3.0800249576568604,
      "learning_rate": 0.00010527102803738318,
      "loss": 0.1945,
      "step": 1268
    },
    {
      "epoch": 0.2372184316291242,
      "grad_norm": 2.169684410095215,
      "learning_rate": 0.00010519626168224299,
      "loss": 0.288,
      "step": 1269
    },
    {
      "epoch": 0.237405364987382,
      "grad_norm": 1.7556354999542236,
      "learning_rate": 0.0001051214953271028,
      "loss": 0.2121,
      "step": 1270
    },
    {
      "epoch": 0.23759229834563977,
      "grad_norm": 2.9316914081573486,
      "learning_rate": 0.00010504672897196261,
      "loss": 0.1282,
      "step": 1271
    },
    {
      "epoch": 0.23777923170389756,
      "grad_norm": 4.648540019989014,
      "learning_rate": 0.00010497196261682243,
      "loss": 0.3944,
      "step": 1272
    },
    {
      "epoch": 0.23796616506215534,
      "grad_norm": 1.1146049499511719,
      "learning_rate": 0.00010489719626168223,
      "loss": 0.1339,
      "step": 1273
    },
    {
      "epoch": 0.23815309842041313,
      "grad_norm": 2.917144298553467,
      "learning_rate": 0.00010482242990654205,
      "loss": 0.3568,
      "step": 1274
    },
    {
      "epoch": 0.2383400317786709,
      "grad_norm": 2.566847562789917,
      "learning_rate": 0.00010474766355140186,
      "loss": 0.268,
      "step": 1275
    },
    {
      "epoch": 0.2385269651369287,
      "grad_norm": 3.792433023452759,
      "learning_rate": 0.00010467289719626168,
      "loss": 0.2184,
      "step": 1276
    },
    {
      "epoch": 0.23871389849518646,
      "grad_norm": 4.686139106750488,
      "learning_rate": 0.00010459813084112151,
      "loss": 0.2572,
      "step": 1277
    },
    {
      "epoch": 0.23890083185344424,
      "grad_norm": 1.6524443626403809,
      "learning_rate": 0.00010452336448598133,
      "loss": 0.1857,
      "step": 1278
    },
    {
      "epoch": 0.23908776521170202,
      "grad_norm": 5.621034622192383,
      "learning_rate": 0.00010444859813084113,
      "loss": 0.2325,
      "step": 1279
    },
    {
      "epoch": 0.2392746985699598,
      "grad_norm": 1.7451001405715942,
      "learning_rate": 0.00010437383177570095,
      "loss": 0.1198,
      "step": 1280
    },
    {
      "epoch": 0.2394616319282176,
      "grad_norm": 1.7308963537216187,
      "learning_rate": 0.00010429906542056076,
      "loss": 0.1658,
      "step": 1281
    },
    {
      "epoch": 0.23964856528647538,
      "grad_norm": 4.798209190368652,
      "learning_rate": 0.00010422429906542058,
      "loss": 0.2567,
      "step": 1282
    },
    {
      "epoch": 0.23983549864473316,
      "grad_norm": 3.8077805042266846,
      "learning_rate": 0.00010414953271028038,
      "loss": 0.2557,
      "step": 1283
    },
    {
      "epoch": 0.24002243200299092,
      "grad_norm": 4.319705486297607,
      "learning_rate": 0.0001040747663551402,
      "loss": 0.237,
      "step": 1284
    },
    {
      "epoch": 0.2402093653612487,
      "grad_norm": 3.1458027362823486,
      "learning_rate": 0.00010400000000000001,
      "loss": 0.1829,
      "step": 1285
    },
    {
      "epoch": 0.2403962987195065,
      "grad_norm": 3.947490692138672,
      "learning_rate": 0.00010392523364485983,
      "loss": 0.2334,
      "step": 1286
    },
    {
      "epoch": 0.24058323207776428,
      "grad_norm": 1.646897792816162,
      "learning_rate": 0.00010385046728971963,
      "loss": 0.223,
      "step": 1287
    },
    {
      "epoch": 0.24077016543602206,
      "grad_norm": 1.9791408777236938,
      "learning_rate": 0.00010377570093457945,
      "loss": 0.2436,
      "step": 1288
    },
    {
      "epoch": 0.24095709879427984,
      "grad_norm": 1.59511137008667,
      "learning_rate": 0.00010370093457943926,
      "loss": 0.1784,
      "step": 1289
    },
    {
      "epoch": 0.24114403215253763,
      "grad_norm": 2.5764238834381104,
      "learning_rate": 0.00010362616822429907,
      "loss": 0.2282,
      "step": 1290
    },
    {
      "epoch": 0.24133096551079541,
      "grad_norm": 4.149435043334961,
      "learning_rate": 0.00010355140186915888,
      "loss": 0.2114,
      "step": 1291
    },
    {
      "epoch": 0.24151789886905317,
      "grad_norm": 1.7825236320495605,
      "learning_rate": 0.0001034766355140187,
      "loss": 0.2311,
      "step": 1292
    },
    {
      "epoch": 0.24170483222731096,
      "grad_norm": 1.1218342781066895,
      "learning_rate": 0.0001034018691588785,
      "loss": 0.1313,
      "step": 1293
    },
    {
      "epoch": 0.24189176558556874,
      "grad_norm": 1.8634177446365356,
      "learning_rate": 0.00010332710280373832,
      "loss": 0.2197,
      "step": 1294
    },
    {
      "epoch": 0.24207869894382653,
      "grad_norm": 0.9776652455329895,
      "learning_rate": 0.00010325233644859813,
      "loss": 0.1078,
      "step": 1295
    },
    {
      "epoch": 0.2422656323020843,
      "grad_norm": 2.112715482711792,
      "learning_rate": 0.00010317757009345795,
      "loss": 0.0868,
      "step": 1296
    },
    {
      "epoch": 0.2424525656603421,
      "grad_norm": 2.6612515449523926,
      "learning_rate": 0.00010310280373831775,
      "loss": 0.2931,
      "step": 1297
    },
    {
      "epoch": 0.24263949901859988,
      "grad_norm": 1.352564811706543,
      "learning_rate": 0.00010302803738317757,
      "loss": 0.1787,
      "step": 1298
    },
    {
      "epoch": 0.24282643237685764,
      "grad_norm": 11.117291450500488,
      "learning_rate": 0.00010295327102803738,
      "loss": 0.3518,
      "step": 1299
    },
    {
      "epoch": 0.24301336573511542,
      "grad_norm": 1.3561948537826538,
      "learning_rate": 0.0001028785046728972,
      "loss": 0.1623,
      "step": 1300
    },
    {
      "epoch": 0.2432002990933732,
      "grad_norm": 4.244334697723389,
      "learning_rate": 0.000102803738317757,
      "loss": 0.2314,
      "step": 1301
    },
    {
      "epoch": 0.243387232451631,
      "grad_norm": 6.669711589813232,
      "learning_rate": 0.00010272897196261683,
      "loss": 0.2585,
      "step": 1302
    },
    {
      "epoch": 0.24357416580988878,
      "grad_norm": 2.176131010055542,
      "learning_rate": 0.00010265420560747665,
      "loss": 0.1658,
      "step": 1303
    },
    {
      "epoch": 0.24376109916814656,
      "grad_norm": 4.296248912811279,
      "learning_rate": 0.00010257943925233646,
      "loss": 0.2974,
      "step": 1304
    },
    {
      "epoch": 0.24394803252640435,
      "grad_norm": 2.8917157649993896,
      "learning_rate": 0.00010250467289719628,
      "loss": 0.3568,
      "step": 1305
    },
    {
      "epoch": 0.24413496588466213,
      "grad_norm": 2.8476970195770264,
      "learning_rate": 0.00010242990654205608,
      "loss": 0.0864,
      "step": 1306
    },
    {
      "epoch": 0.2443218992429199,
      "grad_norm": 4.982353210449219,
      "learning_rate": 0.0001023551401869159,
      "loss": 0.2722,
      "step": 1307
    },
    {
      "epoch": 0.24450883260117767,
      "grad_norm": 2.903909921646118,
      "learning_rate": 0.0001022803738317757,
      "loss": 0.2894,
      "step": 1308
    },
    {
      "epoch": 0.24469576595943546,
      "grad_norm": 1.233439326286316,
      "learning_rate": 0.00010220560747663552,
      "loss": 0.0557,
      "step": 1309
    },
    {
      "epoch": 0.24488269931769324,
      "grad_norm": 3.8253860473632812,
      "learning_rate": 0.00010213084112149533,
      "loss": 0.2088,
      "step": 1310
    },
    {
      "epoch": 0.24506963267595103,
      "grad_norm": 1.556631326675415,
      "learning_rate": 0.00010205607476635515,
      "loss": 0.1671,
      "step": 1311
    },
    {
      "epoch": 0.2452565660342088,
      "grad_norm": 3.59405517578125,
      "learning_rate": 0.00010198130841121495,
      "loss": 0.2572,
      "step": 1312
    },
    {
      "epoch": 0.2454434993924666,
      "grad_norm": 4.998990535736084,
      "learning_rate": 0.00010190654205607477,
      "loss": 0.3562,
      "step": 1313
    },
    {
      "epoch": 0.24563043275072435,
      "grad_norm": 1.6713422536849976,
      "learning_rate": 0.00010183177570093458,
      "loss": 0.2418,
      "step": 1314
    },
    {
      "epoch": 0.24581736610898214,
      "grad_norm": 2.4797871112823486,
      "learning_rate": 0.0001017570093457944,
      "loss": 0.1186,
      "step": 1315
    },
    {
      "epoch": 0.24600429946723992,
      "grad_norm": 1.9250218868255615,
      "learning_rate": 0.0001016822429906542,
      "loss": 0.244,
      "step": 1316
    },
    {
      "epoch": 0.2461912328254977,
      "grad_norm": 2.362128257751465,
      "learning_rate": 0.00010160747663551402,
      "loss": 0.2658,
      "step": 1317
    },
    {
      "epoch": 0.2463781661837555,
      "grad_norm": 1.6092512607574463,
      "learning_rate": 0.00010153271028037383,
      "loss": 0.2271,
      "step": 1318
    },
    {
      "epoch": 0.24656509954201328,
      "grad_norm": 1.7594667673110962,
      "learning_rate": 0.00010145794392523365,
      "loss": 0.1494,
      "step": 1319
    },
    {
      "epoch": 0.24675203290027106,
      "grad_norm": 1.4733480215072632,
      "learning_rate": 0.00010138317757009345,
      "loss": 0.1991,
      "step": 1320
    },
    {
      "epoch": 0.24693896625852885,
      "grad_norm": 3.5174357891082764,
      "learning_rate": 0.00010130841121495327,
      "loss": 0.1056,
      "step": 1321
    },
    {
      "epoch": 0.2471258996167866,
      "grad_norm": 2.1860570907592773,
      "learning_rate": 0.00010123364485981307,
      "loss": 0.196,
      "step": 1322
    },
    {
      "epoch": 0.2473128329750444,
      "grad_norm": 2.8065731525421143,
      "learning_rate": 0.0001011588785046729,
      "loss": 0.2497,
      "step": 1323
    },
    {
      "epoch": 0.24749976633330217,
      "grad_norm": 3.1183695793151855,
      "learning_rate": 0.00010108411214953271,
      "loss": 0.1951,
      "step": 1324
    },
    {
      "epoch": 0.24768669969155996,
      "grad_norm": 1.6565200090408325,
      "learning_rate": 0.00010100934579439252,
      "loss": 0.1685,
      "step": 1325
    },
    {
      "epoch": 0.24787363304981774,
      "grad_norm": 1.7548573017120361,
      "learning_rate": 0.00010093457943925234,
      "loss": 0.2101,
      "step": 1326
    },
    {
      "epoch": 0.24806056640807553,
      "grad_norm": 2.869776725769043,
      "learning_rate": 0.00010085981308411217,
      "loss": 0.2226,
      "step": 1327
    },
    {
      "epoch": 0.2482474997663333,
      "grad_norm": 3.83286452293396,
      "learning_rate": 0.00010078504672897197,
      "loss": 0.2091,
      "step": 1328
    },
    {
      "epoch": 0.24843443312459107,
      "grad_norm": 3.5176947116851807,
      "learning_rate": 0.0001007102803738318,
      "loss": 0.2804,
      "step": 1329
    },
    {
      "epoch": 0.24862136648284885,
      "grad_norm": 3.7798514366149902,
      "learning_rate": 0.0001006355140186916,
      "loss": 0.146,
      "step": 1330
    },
    {
      "epoch": 0.24880829984110664,
      "grad_norm": 2.9645161628723145,
      "learning_rate": 0.00010056074766355142,
      "loss": 0.2319,
      "step": 1331
    },
    {
      "epoch": 0.24899523319936442,
      "grad_norm": 1.5373951196670532,
      "learning_rate": 0.00010048598130841122,
      "loss": 0.1176,
      "step": 1332
    },
    {
      "epoch": 0.2491821665576222,
      "grad_norm": 1.096556305885315,
      "learning_rate": 0.00010041121495327104,
      "loss": 0.1497,
      "step": 1333
    },
    {
      "epoch": 0.24936909991588,
      "grad_norm": 2.905374765396118,
      "learning_rate": 0.00010033644859813085,
      "loss": 0.3397,
      "step": 1334
    },
    {
      "epoch": 0.24955603327413778,
      "grad_norm": 5.081762313842773,
      "learning_rate": 0.00010026168224299067,
      "loss": 0.2897,
      "step": 1335
    },
    {
      "epoch": 0.24974296663239556,
      "grad_norm": 0.6808121800422668,
      "learning_rate": 0.00010018691588785047,
      "loss": 0.0799,
      "step": 1336
    },
    {
      "epoch": 0.24992989999065332,
      "grad_norm": 1.0190482139587402,
      "learning_rate": 0.00010011214953271029,
      "loss": 0.1562,
      "step": 1337
    },
    {
      "epoch": 0.2501168333489111,
      "grad_norm": 2.3039703369140625,
      "learning_rate": 0.0001000373831775701,
      "loss": 0.1432,
      "step": 1338
    },
    {
      "epoch": 0.2503037667071689,
      "grad_norm": 1.0912524461746216,
      "learning_rate": 9.996261682242991e-05,
      "loss": 0.1742,
      "step": 1339
    },
    {
      "epoch": 0.2504907000654267,
      "grad_norm": 2.7098207473754883,
      "learning_rate": 9.988785046728972e-05,
      "loss": 0.2755,
      "step": 1340
    },
    {
      "epoch": 0.25067763342368443,
      "grad_norm": 1.662813663482666,
      "learning_rate": 9.981308411214954e-05,
      "loss": 0.2418,
      "step": 1341
    },
    {
      "epoch": 0.25086456678194224,
      "grad_norm": 2.0585663318634033,
      "learning_rate": 9.973831775700934e-05,
      "loss": 0.094,
      "step": 1342
    },
    {
      "epoch": 0.2510515001402,
      "grad_norm": 1.608244776725769,
      "learning_rate": 9.966355140186916e-05,
      "loss": 0.1789,
      "step": 1343
    },
    {
      "epoch": 0.2512384334984578,
      "grad_norm": 5.11346960067749,
      "learning_rate": 9.958878504672897e-05,
      "loss": 0.2327,
      "step": 1344
    },
    {
      "epoch": 0.25142536685671557,
      "grad_norm": 1.5331352949142456,
      "learning_rate": 9.95140186915888e-05,
      "loss": 0.2069,
      "step": 1345
    },
    {
      "epoch": 0.2516123002149734,
      "grad_norm": 1.557662844657898,
      "learning_rate": 9.94392523364486e-05,
      "loss": 0.1065,
      "step": 1346
    },
    {
      "epoch": 0.25179923357323114,
      "grad_norm": 2.9585468769073486,
      "learning_rate": 9.936448598130842e-05,
      "loss": 0.1851,
      "step": 1347
    },
    {
      "epoch": 0.2519861669314889,
      "grad_norm": 2.2609598636627197,
      "learning_rate": 9.928971962616823e-05,
      "loss": 0.2785,
      "step": 1348
    },
    {
      "epoch": 0.2521731002897467,
      "grad_norm": 1.8471969366073608,
      "learning_rate": 9.921495327102805e-05,
      "loss": 0.2174,
      "step": 1349
    },
    {
      "epoch": 0.25236003364800447,
      "grad_norm": 0.9861489534378052,
      "learning_rate": 9.914018691588785e-05,
      "loss": 0.1868,
      "step": 1350
    },
    {
      "epoch": 0.2525469670062623,
      "grad_norm": 3.229973554611206,
      "learning_rate": 9.906542056074767e-05,
      "loss": 0.171,
      "step": 1351
    },
    {
      "epoch": 0.25273390036452004,
      "grad_norm": 3.4182469844818115,
      "learning_rate": 9.899065420560748e-05,
      "loss": 0.1323,
      "step": 1352
    },
    {
      "epoch": 0.25292083372277785,
      "grad_norm": 3.28309965133667,
      "learning_rate": 9.89158878504673e-05,
      "loss": 0.2371,
      "step": 1353
    },
    {
      "epoch": 0.2531077670810356,
      "grad_norm": 1.9854273796081543,
      "learning_rate": 9.88411214953271e-05,
      "loss": 0.1353,
      "step": 1354
    },
    {
      "epoch": 0.2532947004392934,
      "grad_norm": 9.316061973571777,
      "learning_rate": 9.876635514018692e-05,
      "loss": 0.2042,
      "step": 1355
    },
    {
      "epoch": 0.2534816337975512,
      "grad_norm": 2.421410083770752,
      "learning_rate": 9.869158878504673e-05,
      "loss": 0.1409,
      "step": 1356
    },
    {
      "epoch": 0.25366856715580893,
      "grad_norm": 2.46199893951416,
      "learning_rate": 9.861682242990655e-05,
      "loss": 0.11,
      "step": 1357
    },
    {
      "epoch": 0.25385550051406675,
      "grad_norm": 3.502270460128784,
      "learning_rate": 9.854205607476636e-05,
      "loss": 0.2947,
      "step": 1358
    },
    {
      "epoch": 0.2540424338723245,
      "grad_norm": 1.9454628229141235,
      "learning_rate": 9.846728971962618e-05,
      "loss": 0.1592,
      "step": 1359
    },
    {
      "epoch": 0.2542293672305823,
      "grad_norm": 1.7400497198104858,
      "learning_rate": 9.839252336448599e-05,
      "loss": 0.171,
      "step": 1360
    },
    {
      "epoch": 0.25441630058884007,
      "grad_norm": 15.759803771972656,
      "learning_rate": 9.831775700934581e-05,
      "loss": 0.2283,
      "step": 1361
    },
    {
      "epoch": 0.2546032339470979,
      "grad_norm": 1.155495047569275,
      "learning_rate": 9.824299065420561e-05,
      "loss": 0.1259,
      "step": 1362
    },
    {
      "epoch": 0.25479016730535564,
      "grad_norm": 2.912628650665283,
      "learning_rate": 9.816822429906543e-05,
      "loss": 0.1913,
      "step": 1363
    },
    {
      "epoch": 0.2549771006636134,
      "grad_norm": 1.5211303234100342,
      "learning_rate": 9.809345794392524e-05,
      "loss": 0.0944,
      "step": 1364
    },
    {
      "epoch": 0.2551640340218712,
      "grad_norm": 2.471590757369995,
      "learning_rate": 9.801869158878506e-05,
      "loss": 0.2337,
      "step": 1365
    },
    {
      "epoch": 0.25535096738012897,
      "grad_norm": 1.4614906311035156,
      "learning_rate": 9.794392523364486e-05,
      "loss": 0.1339,
      "step": 1366
    },
    {
      "epoch": 0.2555379007383868,
      "grad_norm": 3.5515737533569336,
      "learning_rate": 9.786915887850468e-05,
      "loss": 0.2464,
      "step": 1367
    },
    {
      "epoch": 0.25572483409664454,
      "grad_norm": 1.2345550060272217,
      "learning_rate": 9.779439252336449e-05,
      "loss": 0.1733,
      "step": 1368
    },
    {
      "epoch": 0.25591176745490235,
      "grad_norm": 1.99653959274292,
      "learning_rate": 9.77196261682243e-05,
      "loss": 0.1213,
      "step": 1369
    },
    {
      "epoch": 0.2560987008131601,
      "grad_norm": 2.3207948207855225,
      "learning_rate": 9.764485981308412e-05,
      "loss": 0.1928,
      "step": 1370
    },
    {
      "epoch": 0.25628563417141786,
      "grad_norm": 1.7899463176727295,
      "learning_rate": 9.757009345794393e-05,
      "loss": 0.196,
      "step": 1371
    },
    {
      "epoch": 0.2564725675296757,
      "grad_norm": 1.7276939153671265,
      "learning_rate": 9.749532710280375e-05,
      "loss": 0.2638,
      "step": 1372
    },
    {
      "epoch": 0.25665950088793343,
      "grad_norm": 2.6673245429992676,
      "learning_rate": 9.742056074766355e-05,
      "loss": 0.1601,
      "step": 1373
    },
    {
      "epoch": 0.25684643424619125,
      "grad_norm": 2.6053223609924316,
      "learning_rate": 9.734579439252337e-05,
      "loss": 0.2229,
      "step": 1374
    },
    {
      "epoch": 0.257033367604449,
      "grad_norm": 0.8256664276123047,
      "learning_rate": 9.727102803738318e-05,
      "loss": 0.1465,
      "step": 1375
    },
    {
      "epoch": 0.2572203009627068,
      "grad_norm": 0.9707236289978027,
      "learning_rate": 9.7196261682243e-05,
      "loss": 0.1575,
      "step": 1376
    },
    {
      "epoch": 0.2574072343209646,
      "grad_norm": 2.050535202026367,
      "learning_rate": 9.71214953271028e-05,
      "loss": 0.2749,
      "step": 1377
    },
    {
      "epoch": 0.25759416767922233,
      "grad_norm": 1.7251811027526855,
      "learning_rate": 9.704672897196262e-05,
      "loss": 0.1451,
      "step": 1378
    },
    {
      "epoch": 0.25778110103748014,
      "grad_norm": 1.593077301979065,
      "learning_rate": 9.697196261682242e-05,
      "loss": 0.2705,
      "step": 1379
    },
    {
      "epoch": 0.2579680343957379,
      "grad_norm": 1.0359832048416138,
      "learning_rate": 9.689719626168224e-05,
      "loss": 0.1106,
      "step": 1380
    },
    {
      "epoch": 0.2581549677539957,
      "grad_norm": 1.8313210010528564,
      "learning_rate": 9.682242990654206e-05,
      "loss": 0.3157,
      "step": 1381
    },
    {
      "epoch": 0.25834190111225347,
      "grad_norm": 1.6505509614944458,
      "learning_rate": 9.674766355140188e-05,
      "loss": 0.1368,
      "step": 1382
    },
    {
      "epoch": 0.2585288344705113,
      "grad_norm": 1.6241837739944458,
      "learning_rate": 9.667289719626169e-05,
      "loss": 0.1456,
      "step": 1383
    },
    {
      "epoch": 0.25871576782876904,
      "grad_norm": 2.9527881145477295,
      "learning_rate": 9.65981308411215e-05,
      "loss": 0.2018,
      "step": 1384
    },
    {
      "epoch": 0.25890270118702685,
      "grad_norm": 4.375796794891357,
      "learning_rate": 9.652336448598131e-05,
      "loss": 0.3084,
      "step": 1385
    },
    {
      "epoch": 0.2590896345452846,
      "grad_norm": 1.709782600402832,
      "learning_rate": 9.644859813084113e-05,
      "loss": 0.1837,
      "step": 1386
    },
    {
      "epoch": 0.25927656790354237,
      "grad_norm": 1.6245684623718262,
      "learning_rate": 9.637383177570094e-05,
      "loss": 0.1956,
      "step": 1387
    },
    {
      "epoch": 0.2594635012618002,
      "grad_norm": 2.3602514266967773,
      "learning_rate": 9.629906542056075e-05,
      "loss": 0.1801,
      "step": 1388
    },
    {
      "epoch": 0.25965043462005793,
      "grad_norm": 3.200284481048584,
      "learning_rate": 9.622429906542056e-05,
      "loss": 0.2626,
      "step": 1389
    },
    {
      "epoch": 0.25983736797831575,
      "grad_norm": 3.1700170040130615,
      "learning_rate": 9.614953271028038e-05,
      "loss": 0.3063,
      "step": 1390
    },
    {
      "epoch": 0.2600243013365735,
      "grad_norm": 6.2173686027526855,
      "learning_rate": 9.607476635514018e-05,
      "loss": 0.2239,
      "step": 1391
    },
    {
      "epoch": 0.2602112346948313,
      "grad_norm": 5.370689392089844,
      "learning_rate": 9.6e-05,
      "loss": 0.1935,
      "step": 1392
    },
    {
      "epoch": 0.2603981680530891,
      "grad_norm": 3.154676914215088,
      "learning_rate": 9.592523364485981e-05,
      "loss": 0.116,
      "step": 1393
    },
    {
      "epoch": 0.26058510141134683,
      "grad_norm": 5.2865400314331055,
      "learning_rate": 9.585046728971963e-05,
      "loss": 0.3027,
      "step": 1394
    },
    {
      "epoch": 0.26077203476960464,
      "grad_norm": 1.0871572494506836,
      "learning_rate": 9.577570093457945e-05,
      "loss": 0.1143,
      "step": 1395
    },
    {
      "epoch": 0.2609589681278624,
      "grad_norm": 2.223916530609131,
      "learning_rate": 9.570093457943926e-05,
      "loss": 0.25,
      "step": 1396
    },
    {
      "epoch": 0.2611459014861202,
      "grad_norm": 2.557378053665161,
      "learning_rate": 9.562616822429907e-05,
      "loss": 0.1903,
      "step": 1397
    },
    {
      "epoch": 0.26133283484437797,
      "grad_norm": 1.5850800275802612,
      "learning_rate": 9.555140186915889e-05,
      "loss": 0.2037,
      "step": 1398
    },
    {
      "epoch": 0.2615197682026358,
      "grad_norm": 3.4698429107666016,
      "learning_rate": 9.54766355140187e-05,
      "loss": 0.3026,
      "step": 1399
    },
    {
      "epoch": 0.26170670156089354,
      "grad_norm": 3.3095264434814453,
      "learning_rate": 9.540186915887851e-05,
      "loss": 0.216,
      "step": 1400
    },
    {
      "epoch": 0.2618936349191513,
      "grad_norm": 3.68733811378479,
      "learning_rate": 9.532710280373832e-05,
      "loss": 0.1487,
      "step": 1401
    },
    {
      "epoch": 0.2620805682774091,
      "grad_norm": 3.518178701400757,
      "learning_rate": 9.525233644859814e-05,
      "loss": 0.184,
      "step": 1402
    },
    {
      "epoch": 0.26226750163566687,
      "grad_norm": 2.936110258102417,
      "learning_rate": 9.517757009345794e-05,
      "loss": 0.1292,
      "step": 1403
    },
    {
      "epoch": 0.2624544349939247,
      "grad_norm": 1.126947045326233,
      "learning_rate": 9.510280373831776e-05,
      "loss": 0.101,
      "step": 1404
    },
    {
      "epoch": 0.26264136835218244,
      "grad_norm": 2.298124074935913,
      "learning_rate": 9.502803738317757e-05,
      "loss": 0.1306,
      "step": 1405
    },
    {
      "epoch": 0.26282830171044025,
      "grad_norm": 3.404705047607422,
      "learning_rate": 9.495327102803739e-05,
      "loss": 0.2372,
      "step": 1406
    },
    {
      "epoch": 0.263015235068698,
      "grad_norm": 1.3417001962661743,
      "learning_rate": 9.48785046728972e-05,
      "loss": 0.1567,
      "step": 1407
    },
    {
      "epoch": 0.26320216842695576,
      "grad_norm": 0.8092033267021179,
      "learning_rate": 9.480373831775702e-05,
      "loss": 0.1211,
      "step": 1408
    },
    {
      "epoch": 0.2633891017852136,
      "grad_norm": 4.736337661743164,
      "learning_rate": 9.472897196261683e-05,
      "loss": 0.2551,
      "step": 1409
    },
    {
      "epoch": 0.26357603514347133,
      "grad_norm": 2.439685583114624,
      "learning_rate": 9.465420560747665e-05,
      "loss": 0.1697,
      "step": 1410
    },
    {
      "epoch": 0.26376296850172914,
      "grad_norm": 2.302555561065674,
      "learning_rate": 9.457943925233645e-05,
      "loss": 0.2488,
      "step": 1411
    },
    {
      "epoch": 0.2639499018599869,
      "grad_norm": 1.6484678983688354,
      "learning_rate": 9.450467289719627e-05,
      "loss": 0.1254,
      "step": 1412
    },
    {
      "epoch": 0.2641368352182447,
      "grad_norm": 1.5821106433868408,
      "learning_rate": 9.442990654205608e-05,
      "loss": 0.1058,
      "step": 1413
    },
    {
      "epoch": 0.26432376857650247,
      "grad_norm": 1.254935622215271,
      "learning_rate": 9.43551401869159e-05,
      "loss": 0.1593,
      "step": 1414
    },
    {
      "epoch": 0.2645107019347603,
      "grad_norm": 1.2930625677108765,
      "learning_rate": 9.42803738317757e-05,
      "loss": 0.2521,
      "step": 1415
    },
    {
      "epoch": 0.26469763529301804,
      "grad_norm": 3.4051833152770996,
      "learning_rate": 9.420560747663552e-05,
      "loss": 0.1941,
      "step": 1416
    },
    {
      "epoch": 0.2648845686512758,
      "grad_norm": 2.3780324459075928,
      "learning_rate": 9.413084112149533e-05,
      "loss": 0.1018,
      "step": 1417
    },
    {
      "epoch": 0.2650715020095336,
      "grad_norm": 3.2114851474761963,
      "learning_rate": 9.405607476635514e-05,
      "loss": 0.3847,
      "step": 1418
    },
    {
      "epoch": 0.26525843536779137,
      "grad_norm": 4.499588966369629,
      "learning_rate": 9.398130841121496e-05,
      "loss": 0.3382,
      "step": 1419
    },
    {
      "epoch": 0.2654453687260492,
      "grad_norm": 3.6909995079040527,
      "learning_rate": 9.390654205607478e-05,
      "loss": 0.2519,
      "step": 1420
    },
    {
      "epoch": 0.26563230208430694,
      "grad_norm": 4.711140155792236,
      "learning_rate": 9.383177570093459e-05,
      "loss": 0.2333,
      "step": 1421
    },
    {
      "epoch": 0.26581923544256475,
      "grad_norm": 2.831569194793701,
      "learning_rate": 9.37570093457944e-05,
      "loss": 0.2343,
      "step": 1422
    },
    {
      "epoch": 0.2660061688008225,
      "grad_norm": 5.095237731933594,
      "learning_rate": 9.368224299065421e-05,
      "loss": 0.2943,
      "step": 1423
    },
    {
      "epoch": 0.26619310215908026,
      "grad_norm": 8.913406372070312,
      "learning_rate": 9.360747663551403e-05,
      "loss": 0.2624,
      "step": 1424
    },
    {
      "epoch": 0.2663800355173381,
      "grad_norm": 3.0012764930725098,
      "learning_rate": 9.353271028037384e-05,
      "loss": 0.1206,
      "step": 1425
    },
    {
      "epoch": 0.26656696887559583,
      "grad_norm": 2.524146318435669,
      "learning_rate": 9.345794392523365e-05,
      "loss": 0.275,
      "step": 1426
    },
    {
      "epoch": 0.26675390223385365,
      "grad_norm": 1.6858152151107788,
      "learning_rate": 9.338317757009346e-05,
      "loss": 0.1825,
      "step": 1427
    },
    {
      "epoch": 0.2669408355921114,
      "grad_norm": 2.4395592212677,
      "learning_rate": 9.330841121495328e-05,
      "loss": 0.125,
      "step": 1428
    },
    {
      "epoch": 0.2671277689503692,
      "grad_norm": 1.9061716794967651,
      "learning_rate": 9.323364485981308e-05,
      "loss": 0.2157,
      "step": 1429
    },
    {
      "epoch": 0.267314702308627,
      "grad_norm": 4.629698753356934,
      "learning_rate": 9.31588785046729e-05,
      "loss": 0.2355,
      "step": 1430
    },
    {
      "epoch": 0.26750163566688473,
      "grad_norm": 3.0965073108673096,
      "learning_rate": 9.308411214953271e-05,
      "loss": 0.1645,
      "step": 1431
    },
    {
      "epoch": 0.26768856902514254,
      "grad_norm": 1.5836530923843384,
      "learning_rate": 9.300934579439253e-05,
      "loss": 0.131,
      "step": 1432
    },
    {
      "epoch": 0.2678755023834003,
      "grad_norm": 1.346874713897705,
      "learning_rate": 9.293457943925235e-05,
      "loss": 0.1051,
      "step": 1433
    },
    {
      "epoch": 0.2680624357416581,
      "grad_norm": 3.2173690795898438,
      "learning_rate": 9.285981308411215e-05,
      "loss": 0.1399,
      "step": 1434
    },
    {
      "epoch": 0.26824936909991587,
      "grad_norm": 1.7144265174865723,
      "learning_rate": 9.278504672897197e-05,
      "loss": 0.2325,
      "step": 1435
    },
    {
      "epoch": 0.2684363024581737,
      "grad_norm": 1.6888574361801147,
      "learning_rate": 9.271028037383178e-05,
      "loss": 0.2975,
      "step": 1436
    },
    {
      "epoch": 0.26862323581643144,
      "grad_norm": 1.1643282175064087,
      "learning_rate": 9.26355140186916e-05,
      "loss": 0.1651,
      "step": 1437
    },
    {
      "epoch": 0.26881016917468925,
      "grad_norm": 1.379728078842163,
      "learning_rate": 9.25607476635514e-05,
      "loss": 0.1258,
      "step": 1438
    },
    {
      "epoch": 0.268997102532947,
      "grad_norm": 3.5746161937713623,
      "learning_rate": 9.248598130841122e-05,
      "loss": 0.3455,
      "step": 1439
    },
    {
      "epoch": 0.26918403589120476,
      "grad_norm": 1.6157550811767578,
      "learning_rate": 9.241121495327104e-05,
      "loss": 0.1734,
      "step": 1440
    },
    {
      "epoch": 0.2693709692494626,
      "grad_norm": 3.340005874633789,
      "learning_rate": 9.233644859813084e-05,
      "loss": 0.1652,
      "step": 1441
    },
    {
      "epoch": 0.26955790260772033,
      "grad_norm": 1.2558971643447876,
      "learning_rate": 9.226168224299066e-05,
      "loss": 0.1695,
      "step": 1442
    },
    {
      "epoch": 0.26974483596597815,
      "grad_norm": 1.6539299488067627,
      "learning_rate": 9.218691588785047e-05,
      "loss": 0.1901,
      "step": 1443
    },
    {
      "epoch": 0.2699317693242359,
      "grad_norm": 1.1039488315582275,
      "learning_rate": 9.211214953271029e-05,
      "loss": 0.1323,
      "step": 1444
    },
    {
      "epoch": 0.2701187026824937,
      "grad_norm": 1.7645751237869263,
      "learning_rate": 9.20373831775701e-05,
      "loss": 0.1411,
      "step": 1445
    },
    {
      "epoch": 0.2703056360407515,
      "grad_norm": 3.664477825164795,
      "learning_rate": 9.196261682242991e-05,
      "loss": 0.2026,
      "step": 1446
    },
    {
      "epoch": 0.27049256939900923,
      "grad_norm": 2.168548345565796,
      "learning_rate": 9.188785046728973e-05,
      "loss": 0.1103,
      "step": 1447
    },
    {
      "epoch": 0.27067950275726704,
      "grad_norm": 3.0211358070373535,
      "learning_rate": 9.181308411214953e-05,
      "loss": 0.3088,
      "step": 1448
    },
    {
      "epoch": 0.2708664361155248,
      "grad_norm": 4.059067249298096,
      "learning_rate": 9.173831775700935e-05,
      "loss": 0.2987,
      "step": 1449
    },
    {
      "epoch": 0.2710533694737826,
      "grad_norm": 1.2321395874023438,
      "learning_rate": 9.166355140186916e-05,
      "loss": 0.177,
      "step": 1450
    },
    {
      "epoch": 0.27124030283204037,
      "grad_norm": 1.3882732391357422,
      "learning_rate": 9.158878504672898e-05,
      "loss": 0.171,
      "step": 1451
    },
    {
      "epoch": 0.2714272361902982,
      "grad_norm": 2.7544586658477783,
      "learning_rate": 9.151401869158878e-05,
      "loss": 0.1227,
      "step": 1452
    },
    {
      "epoch": 0.27161416954855594,
      "grad_norm": 1.4505858421325684,
      "learning_rate": 9.14392523364486e-05,
      "loss": 0.163,
      "step": 1453
    },
    {
      "epoch": 0.2718011029068137,
      "grad_norm": 1.5400872230529785,
      "learning_rate": 9.13644859813084e-05,
      "loss": 0.265,
      "step": 1454
    },
    {
      "epoch": 0.2719880362650715,
      "grad_norm": 2.3581526279449463,
      "learning_rate": 9.128971962616823e-05,
      "loss": 0.179,
      "step": 1455
    },
    {
      "epoch": 0.27217496962332927,
      "grad_norm": 4.718128204345703,
      "learning_rate": 9.121495327102803e-05,
      "loss": 0.237,
      "step": 1456
    },
    {
      "epoch": 0.2723619029815871,
      "grad_norm": 1.750299334526062,
      "learning_rate": 9.114018691588786e-05,
      "loss": 0.2525,
      "step": 1457
    },
    {
      "epoch": 0.27254883633984484,
      "grad_norm": 2.1212832927703857,
      "learning_rate": 9.106542056074767e-05,
      "loss": 0.1219,
      "step": 1458
    },
    {
      "epoch": 0.27273576969810265,
      "grad_norm": 1.8786842823028564,
      "learning_rate": 9.099065420560749e-05,
      "loss": 0.2551,
      "step": 1459
    },
    {
      "epoch": 0.2729227030563604,
      "grad_norm": 1.8496941328048706,
      "learning_rate": 9.091588785046729e-05,
      "loss": 0.1539,
      "step": 1460
    },
    {
      "epoch": 0.27310963641461816,
      "grad_norm": 15.055534362792969,
      "learning_rate": 9.084112149532711e-05,
      "loss": 0.2206,
      "step": 1461
    },
    {
      "epoch": 0.273296569772876,
      "grad_norm": 1.3082948923110962,
      "learning_rate": 9.076635514018692e-05,
      "loss": 0.1099,
      "step": 1462
    },
    {
      "epoch": 0.27348350313113373,
      "grad_norm": 1.7217514514923096,
      "learning_rate": 9.069158878504674e-05,
      "loss": 0.2281,
      "step": 1463
    },
    {
      "epoch": 0.27367043648939154,
      "grad_norm": 6.332208156585693,
      "learning_rate": 9.061682242990654e-05,
      "loss": 0.2838,
      "step": 1464
    },
    {
      "epoch": 0.2738573698476493,
      "grad_norm": 3.347280263900757,
      "learning_rate": 9.054205607476636e-05,
      "loss": 0.1199,
      "step": 1465
    },
    {
      "epoch": 0.2740443032059071,
      "grad_norm": 2.492361068725586,
      "learning_rate": 9.046728971962616e-05,
      "loss": 0.1527,
      "step": 1466
    },
    {
      "epoch": 0.27423123656416487,
      "grad_norm": 2.714434862136841,
      "learning_rate": 9.039252336448598e-05,
      "loss": 0.2454,
      "step": 1467
    },
    {
      "epoch": 0.2744181699224227,
      "grad_norm": 1.287350058555603,
      "learning_rate": 9.031775700934579e-05,
      "loss": 0.1282,
      "step": 1468
    },
    {
      "epoch": 0.27460510328068044,
      "grad_norm": 1.6559749841690063,
      "learning_rate": 9.024299065420562e-05,
      "loss": 0.1603,
      "step": 1469
    },
    {
      "epoch": 0.2747920366389382,
      "grad_norm": 9.538351058959961,
      "learning_rate": 9.016822429906543e-05,
      "loss": 0.2121,
      "step": 1470
    },
    {
      "epoch": 0.274978969997196,
      "grad_norm": 1.9876419305801392,
      "learning_rate": 9.009345794392525e-05,
      "loss": 0.18,
      "step": 1471
    },
    {
      "epoch": 0.27516590335545377,
      "grad_norm": 1.742715835571289,
      "learning_rate": 9.001869158878505e-05,
      "loss": 0.1965,
      "step": 1472
    },
    {
      "epoch": 0.2753528367137116,
      "grad_norm": 1.7292414903640747,
      "learning_rate": 8.994392523364487e-05,
      "loss": 0.2018,
      "step": 1473
    },
    {
      "epoch": 0.27553977007196934,
      "grad_norm": 2.879655361175537,
      "learning_rate": 8.986915887850468e-05,
      "loss": 0.1954,
      "step": 1474
    },
    {
      "epoch": 0.27572670343022715,
      "grad_norm": 1.554348349571228,
      "learning_rate": 8.97943925233645e-05,
      "loss": 0.1748,
      "step": 1475
    },
    {
      "epoch": 0.2759136367884849,
      "grad_norm": 1.862476110458374,
      "learning_rate": 8.97196261682243e-05,
      "loss": 0.1098,
      "step": 1476
    },
    {
      "epoch": 0.27610057014674266,
      "grad_norm": 1.520072102546692,
      "learning_rate": 8.964485981308412e-05,
      "loss": 0.1233,
      "step": 1477
    },
    {
      "epoch": 0.2762875035050005,
      "grad_norm": 1.4167559146881104,
      "learning_rate": 8.957009345794392e-05,
      "loss": 0.1565,
      "step": 1478
    },
    {
      "epoch": 0.27647443686325823,
      "grad_norm": 3.5479586124420166,
      "learning_rate": 8.949532710280374e-05,
      "loss": 0.0963,
      "step": 1479
    },
    {
      "epoch": 0.27666137022151605,
      "grad_norm": 2.8156380653381348,
      "learning_rate": 8.942056074766355e-05,
      "loss": 0.1814,
      "step": 1480
    },
    {
      "epoch": 0.2768483035797738,
      "grad_norm": 2.2663567066192627,
      "learning_rate": 8.934579439252338e-05,
      "loss": 0.2739,
      "step": 1481
    },
    {
      "epoch": 0.2770352369380316,
      "grad_norm": 5.350846290588379,
      "learning_rate": 8.927102803738319e-05,
      "loss": 0.2629,
      "step": 1482
    },
    {
      "epoch": 0.27722217029628937,
      "grad_norm": 1.6660521030426025,
      "learning_rate": 8.9196261682243e-05,
      "loss": 0.2102,
      "step": 1483
    },
    {
      "epoch": 0.27740910365454713,
      "grad_norm": 1.4708102941513062,
      "learning_rate": 8.912149532710281e-05,
      "loss": 0.1727,
      "step": 1484
    },
    {
      "epoch": 0.27759603701280494,
      "grad_norm": 2.5431737899780273,
      "learning_rate": 8.904672897196263e-05,
      "loss": 0.1747,
      "step": 1485
    },
    {
      "epoch": 0.2777829703710627,
      "grad_norm": 1.7372286319732666,
      "learning_rate": 8.897196261682243e-05,
      "loss": 0.1567,
      "step": 1486
    },
    {
      "epoch": 0.2779699037293205,
      "grad_norm": 9.277551651000977,
      "learning_rate": 8.889719626168225e-05,
      "loss": 0.3888,
      "step": 1487
    },
    {
      "epoch": 0.27815683708757827,
      "grad_norm": 4.128966331481934,
      "learning_rate": 8.882242990654206e-05,
      "loss": 0.2813,
      "step": 1488
    },
    {
      "epoch": 0.2783437704458361,
      "grad_norm": 8.045367240905762,
      "learning_rate": 8.874766355140188e-05,
      "loss": 0.3085,
      "step": 1489
    },
    {
      "epoch": 0.27853070380409384,
      "grad_norm": 5.212996482849121,
      "learning_rate": 8.867289719626168e-05,
      "loss": 0.1373,
      "step": 1490
    },
    {
      "epoch": 0.2787176371623516,
      "grad_norm": 2.434601068496704,
      "learning_rate": 8.85981308411215e-05,
      "loss": 0.1486,
      "step": 1491
    },
    {
      "epoch": 0.2789045705206094,
      "grad_norm": 3.6864893436431885,
      "learning_rate": 8.85233644859813e-05,
      "loss": 0.1947,
      "step": 1492
    },
    {
      "epoch": 0.27909150387886716,
      "grad_norm": 2.6942832469940186,
      "learning_rate": 8.844859813084113e-05,
      "loss": 0.1761,
      "step": 1493
    },
    {
      "epoch": 0.279278437237125,
      "grad_norm": 3.0177910327911377,
      "learning_rate": 8.837383177570094e-05,
      "loss": 0.3046,
      "step": 1494
    },
    {
      "epoch": 0.27946537059538273,
      "grad_norm": 1.905702829360962,
      "learning_rate": 8.829906542056075e-05,
      "loss": 0.1988,
      "step": 1495
    },
    {
      "epoch": 0.27965230395364055,
      "grad_norm": 1.7287380695343018,
      "learning_rate": 8.822429906542057e-05,
      "loss": 0.2047,
      "step": 1496
    },
    {
      "epoch": 0.2798392373118983,
      "grad_norm": 2.8784701824188232,
      "learning_rate": 8.814953271028037e-05,
      "loss": 0.2873,
      "step": 1497
    },
    {
      "epoch": 0.2800261706701561,
      "grad_norm": 6.852260589599609,
      "learning_rate": 8.807476635514019e-05,
      "loss": 0.4475,
      "step": 1498
    },
    {
      "epoch": 0.2802131040284139,
      "grad_norm": 2.4920504093170166,
      "learning_rate": 8.800000000000001e-05,
      "loss": 0.2323,
      "step": 1499
    },
    {
      "epoch": 0.28040003738667163,
      "grad_norm": 5.92142391204834,
      "learning_rate": 8.792523364485982e-05,
      "loss": 0.2927,
      "step": 1500
    },
    {
      "epoch": 0.28058697074492944,
      "grad_norm": 1.81569242477417,
      "learning_rate": 8.785046728971964e-05,
      "loss": 0.1633,
      "step": 1501
    },
    {
      "epoch": 0.2807739041031872,
      "grad_norm": 1.629634141921997,
      "learning_rate": 8.777570093457944e-05,
      "loss": 0.2604,
      "step": 1502
    },
    {
      "epoch": 0.280960837461445,
      "grad_norm": 0.9794480800628662,
      "learning_rate": 8.770093457943926e-05,
      "loss": 0.1503,
      "step": 1503
    },
    {
      "epoch": 0.28114777081970277,
      "grad_norm": 6.956924915313721,
      "learning_rate": 8.762616822429906e-05,
      "loss": 0.3238,
      "step": 1504
    },
    {
      "epoch": 0.2813347041779606,
      "grad_norm": 2.418184280395508,
      "learning_rate": 8.755140186915888e-05,
      "loss": 0.1952,
      "step": 1505
    },
    {
      "epoch": 0.28152163753621834,
      "grad_norm": 2.2539191246032715,
      "learning_rate": 8.74766355140187e-05,
      "loss": 0.2023,
      "step": 1506
    },
    {
      "epoch": 0.2817085708944761,
      "grad_norm": 2.029662847518921,
      "learning_rate": 8.740186915887851e-05,
      "loss": 0.1794,
      "step": 1507
    },
    {
      "epoch": 0.2818955042527339,
      "grad_norm": 1.6680914163589478,
      "learning_rate": 8.732710280373833e-05,
      "loss": 0.1159,
      "step": 1508
    },
    {
      "epoch": 0.28208243761099167,
      "grad_norm": 1.0829623937606812,
      "learning_rate": 8.725233644859813e-05,
      "loss": 0.1043,
      "step": 1509
    },
    {
      "epoch": 0.2822693709692495,
      "grad_norm": 2.7042016983032227,
      "learning_rate": 8.717757009345795e-05,
      "loss": 0.1324,
      "step": 1510
    },
    {
      "epoch": 0.28245630432750723,
      "grad_norm": 1.478716254234314,
      "learning_rate": 8.710280373831776e-05,
      "loss": 0.2658,
      "step": 1511
    },
    {
      "epoch": 0.28264323768576505,
      "grad_norm": 1.83195161819458,
      "learning_rate": 8.702803738317758e-05,
      "loss": 0.2549,
      "step": 1512
    },
    {
      "epoch": 0.2828301710440228,
      "grad_norm": 2.329286813735962,
      "learning_rate": 8.695327102803738e-05,
      "loss": 0.201,
      "step": 1513
    },
    {
      "epoch": 0.28301710440228056,
      "grad_norm": 2.256565570831299,
      "learning_rate": 8.68785046728972e-05,
      "loss": 0.2597,
      "step": 1514
    },
    {
      "epoch": 0.2832040377605384,
      "grad_norm": 1.1528520584106445,
      "learning_rate": 8.6803738317757e-05,
      "loss": 0.1701,
      "step": 1515
    },
    {
      "epoch": 0.28339097111879613,
      "grad_norm": 2.128166913986206,
      "learning_rate": 8.672897196261682e-05,
      "loss": 0.1796,
      "step": 1516
    },
    {
      "epoch": 0.28357790447705394,
      "grad_norm": 1.8271843194961548,
      "learning_rate": 8.665420560747663e-05,
      "loss": 0.2618,
      "step": 1517
    },
    {
      "epoch": 0.2837648378353117,
      "grad_norm": 3.0965826511383057,
      "learning_rate": 8.657943925233645e-05,
      "loss": 0.1974,
      "step": 1518
    },
    {
      "epoch": 0.2839517711935695,
      "grad_norm": 1.3028430938720703,
      "learning_rate": 8.650467289719627e-05,
      "loss": 0.1497,
      "step": 1519
    },
    {
      "epoch": 0.28413870455182727,
      "grad_norm": 1.7832978963851929,
      "learning_rate": 8.642990654205609e-05,
      "loss": 0.1301,
      "step": 1520
    },
    {
      "epoch": 0.284325637910085,
      "grad_norm": 2.340855836868286,
      "learning_rate": 8.635514018691589e-05,
      "loss": 0.143,
      "step": 1521
    },
    {
      "epoch": 0.28451257126834284,
      "grad_norm": 2.455657720565796,
      "learning_rate": 8.628037383177571e-05,
      "loss": 0.2071,
      "step": 1522
    },
    {
      "epoch": 0.2846995046266006,
      "grad_norm": 7.324656009674072,
      "learning_rate": 8.620560747663551e-05,
      "loss": 0.2561,
      "step": 1523
    },
    {
      "epoch": 0.2848864379848584,
      "grad_norm": 3.4305951595306396,
      "learning_rate": 8.613084112149533e-05,
      "loss": 0.229,
      "step": 1524
    },
    {
      "epoch": 0.28507337134311617,
      "grad_norm": 4.075995445251465,
      "learning_rate": 8.605607476635514e-05,
      "loss": 0.2067,
      "step": 1525
    },
    {
      "epoch": 0.285260304701374,
      "grad_norm": 2.1446146965026855,
      "learning_rate": 8.598130841121496e-05,
      "loss": 0.1243,
      "step": 1526
    },
    {
      "epoch": 0.28544723805963174,
      "grad_norm": 1.9239553213119507,
      "learning_rate": 8.590654205607476e-05,
      "loss": 0.0934,
      "step": 1527
    },
    {
      "epoch": 0.28563417141788955,
      "grad_norm": 4.865071773529053,
      "learning_rate": 8.583177570093458e-05,
      "loss": 0.1775,
      "step": 1528
    },
    {
      "epoch": 0.2858211047761473,
      "grad_norm": 3.740368604660034,
      "learning_rate": 8.575700934579439e-05,
      "loss": 0.1611,
      "step": 1529
    },
    {
      "epoch": 0.28600803813440506,
      "grad_norm": 4.339230060577393,
      "learning_rate": 8.56822429906542e-05,
      "loss": 0.3458,
      "step": 1530
    },
    {
      "epoch": 0.2861949714926629,
      "grad_norm": 4.639553070068359,
      "learning_rate": 8.560747663551403e-05,
      "loss": 0.4177,
      "step": 1531
    },
    {
      "epoch": 0.28638190485092063,
      "grad_norm": 2.4329018592834473,
      "learning_rate": 8.553271028037384e-05,
      "loss": 0.2142,
      "step": 1532
    },
    {
      "epoch": 0.28656883820917844,
      "grad_norm": 2.082254648208618,
      "learning_rate": 8.545794392523365e-05,
      "loss": 0.1862,
      "step": 1533
    },
    {
      "epoch": 0.2867557715674362,
      "grad_norm": 3.1170215606689453,
      "learning_rate": 8.538317757009347e-05,
      "loss": 0.1061,
      "step": 1534
    },
    {
      "epoch": 0.286942704925694,
      "grad_norm": 1.759351372718811,
      "learning_rate": 8.530841121495327e-05,
      "loss": 0.0873,
      "step": 1535
    },
    {
      "epoch": 0.28712963828395177,
      "grad_norm": 3.0460681915283203,
      "learning_rate": 8.523364485981309e-05,
      "loss": 0.2326,
      "step": 1536
    },
    {
      "epoch": 0.28731657164220953,
      "grad_norm": 3.370413064956665,
      "learning_rate": 8.51588785046729e-05,
      "loss": 0.1733,
      "step": 1537
    },
    {
      "epoch": 0.28750350500046734,
      "grad_norm": 0.7187445163726807,
      "learning_rate": 8.508411214953272e-05,
      "loss": 0.114,
      "step": 1538
    },
    {
      "epoch": 0.2876904383587251,
      "grad_norm": 2.7639787197113037,
      "learning_rate": 8.500934579439252e-05,
      "loss": 0.1852,
      "step": 1539
    },
    {
      "epoch": 0.2878773717169829,
      "grad_norm": 2.589198589324951,
      "learning_rate": 8.493457943925234e-05,
      "loss": 0.2381,
      "step": 1540
    },
    {
      "epoch": 0.28806430507524067,
      "grad_norm": 1.8585782051086426,
      "learning_rate": 8.485981308411215e-05,
      "loss": 0.2052,
      "step": 1541
    },
    {
      "epoch": 0.2882512384334985,
      "grad_norm": 4.3354291915893555,
      "learning_rate": 8.478504672897197e-05,
      "loss": 0.1673,
      "step": 1542
    },
    {
      "epoch": 0.28843817179175624,
      "grad_norm": 4.688167095184326,
      "learning_rate": 8.471028037383178e-05,
      "loss": 0.2205,
      "step": 1543
    },
    {
      "epoch": 0.288625105150014,
      "grad_norm": 1.5239872932434082,
      "learning_rate": 8.46355140186916e-05,
      "loss": 0.2455,
      "step": 1544
    },
    {
      "epoch": 0.2888120385082718,
      "grad_norm": 1.4748177528381348,
      "learning_rate": 8.456074766355141e-05,
      "loss": 0.2003,
      "step": 1545
    },
    {
      "epoch": 0.28899897186652956,
      "grad_norm": 1.1355845928192139,
      "learning_rate": 8.448598130841123e-05,
      "loss": 0.0825,
      "step": 1546
    },
    {
      "epoch": 0.2891859052247874,
      "grad_norm": 1.542519450187683,
      "learning_rate": 8.441121495327103e-05,
      "loss": 0.2052,
      "step": 1547
    },
    {
      "epoch": 0.28937283858304513,
      "grad_norm": 1.7088608741760254,
      "learning_rate": 8.433644859813085e-05,
      "loss": 0.2397,
      "step": 1548
    },
    {
      "epoch": 0.28955977194130295,
      "grad_norm": 1.5925604104995728,
      "learning_rate": 8.426168224299066e-05,
      "loss": 0.2256,
      "step": 1549
    },
    {
      "epoch": 0.2897467052995607,
      "grad_norm": 1.1311078071594238,
      "learning_rate": 8.418691588785048e-05,
      "loss": 0.1935,
      "step": 1550
    },
    {
      "epoch": 0.2899336386578185,
      "grad_norm": 1.7796319723129272,
      "learning_rate": 8.411214953271028e-05,
      "loss": 0.1108,
      "step": 1551
    },
    {
      "epoch": 0.2901205720160763,
      "grad_norm": 1.9430882930755615,
      "learning_rate": 8.40373831775701e-05,
      "loss": 0.1932,
      "step": 1552
    },
    {
      "epoch": 0.29030750537433403,
      "grad_norm": 2.151945114135742,
      "learning_rate": 8.39626168224299e-05,
      "loss": 0.1255,
      "step": 1553
    },
    {
      "epoch": 0.29049443873259184,
      "grad_norm": 1.1503924131393433,
      "learning_rate": 8.388785046728972e-05,
      "loss": 0.1579,
      "step": 1554
    },
    {
      "epoch": 0.2906813720908496,
      "grad_norm": 1.9054116010665894,
      "learning_rate": 8.381308411214953e-05,
      "loss": 0.1313,
      "step": 1555
    },
    {
      "epoch": 0.2908683054491074,
      "grad_norm": 5.714905738830566,
      "learning_rate": 8.373831775700936e-05,
      "loss": 0.2866,
      "step": 1556
    },
    {
      "epoch": 0.29105523880736517,
      "grad_norm": 1.500449299812317,
      "learning_rate": 8.366355140186917e-05,
      "loss": 0.2497,
      "step": 1557
    },
    {
      "epoch": 0.291242172165623,
      "grad_norm": 2.2468130588531494,
      "learning_rate": 8.358878504672899e-05,
      "loss": 0.2272,
      "step": 1558
    },
    {
      "epoch": 0.29142910552388074,
      "grad_norm": 2.0472800731658936,
      "learning_rate": 8.351401869158879e-05,
      "loss": 0.2068,
      "step": 1559
    },
    {
      "epoch": 0.2916160388821385,
      "grad_norm": 4.774343013763428,
      "learning_rate": 8.343925233644861e-05,
      "loss": 0.3664,
      "step": 1560
    },
    {
      "epoch": 0.2918029722403963,
      "grad_norm": 1.5064704418182373,
      "learning_rate": 8.336448598130842e-05,
      "loss": 0.1479,
      "step": 1561
    },
    {
      "epoch": 0.29198990559865406,
      "grad_norm": 1.4232697486877441,
      "learning_rate": 8.328971962616823e-05,
      "loss": 0.152,
      "step": 1562
    },
    {
      "epoch": 0.2921768389569119,
      "grad_norm": 2.0240743160247803,
      "learning_rate": 8.321495327102804e-05,
      "loss": 0.1961,
      "step": 1563
    },
    {
      "epoch": 0.29236377231516963,
      "grad_norm": 3.1269371509552,
      "learning_rate": 8.314018691588786e-05,
      "loss": 0.2111,
      "step": 1564
    },
    {
      "epoch": 0.29255070567342745,
      "grad_norm": 1.7986886501312256,
      "learning_rate": 8.306542056074766e-05,
      "loss": 0.1847,
      "step": 1565
    },
    {
      "epoch": 0.2927376390316852,
      "grad_norm": 7.620277404785156,
      "learning_rate": 8.299065420560748e-05,
      "loss": 0.2608,
      "step": 1566
    },
    {
      "epoch": 0.29292457238994296,
      "grad_norm": 2.337916612625122,
      "learning_rate": 8.291588785046729e-05,
      "loss": 0.2306,
      "step": 1567
    },
    {
      "epoch": 0.2931115057482008,
      "grad_norm": 2.991610288619995,
      "learning_rate": 8.28411214953271e-05,
      "loss": 0.2849,
      "step": 1568
    },
    {
      "epoch": 0.29329843910645853,
      "grad_norm": 2.9189045429229736,
      "learning_rate": 8.276635514018693e-05,
      "loss": 0.2359,
      "step": 1569
    },
    {
      "epoch": 0.29348537246471634,
      "grad_norm": 4.185079097747803,
      "learning_rate": 8.269158878504673e-05,
      "loss": 0.2764,
      "step": 1570
    },
    {
      "epoch": 0.2936723058229741,
      "grad_norm": 3.768144130706787,
      "learning_rate": 8.261682242990655e-05,
      "loss": 0.1255,
      "step": 1571
    },
    {
      "epoch": 0.2938592391812319,
      "grad_norm": 3.8150298595428467,
      "learning_rate": 8.254205607476635e-05,
      "loss": 0.1636,
      "step": 1572
    },
    {
      "epoch": 0.29404617253948967,
      "grad_norm": 2.180269241333008,
      "learning_rate": 8.246728971962617e-05,
      "loss": 0.1391,
      "step": 1573
    },
    {
      "epoch": 0.2942331058977474,
      "grad_norm": 3.044935703277588,
      "learning_rate": 8.239252336448598e-05,
      "loss": 0.1954,
      "step": 1574
    },
    {
      "epoch": 0.29442003925600524,
      "grad_norm": 1.5587847232818604,
      "learning_rate": 8.23177570093458e-05,
      "loss": 0.1842,
      "step": 1575
    },
    {
      "epoch": 0.294606972614263,
      "grad_norm": 1.4365363121032715,
      "learning_rate": 8.22429906542056e-05,
      "loss": 0.1965,
      "step": 1576
    },
    {
      "epoch": 0.2947939059725208,
      "grad_norm": 3.793173313140869,
      "learning_rate": 8.216822429906542e-05,
      "loss": 0.2014,
      "step": 1577
    },
    {
      "epoch": 0.29498083933077857,
      "grad_norm": 2.6285955905914307,
      "learning_rate": 8.209345794392523e-05,
      "loss": 0.2741,
      "step": 1578
    },
    {
      "epoch": 0.2951677726890364,
      "grad_norm": 2.2373440265655518,
      "learning_rate": 8.201869158878505e-05,
      "loss": 0.1837,
      "step": 1579
    },
    {
      "epoch": 0.29535470604729414,
      "grad_norm": 1.7939503192901611,
      "learning_rate": 8.194392523364487e-05,
      "loss": 0.1457,
      "step": 1580
    },
    {
      "epoch": 0.29554163940555195,
      "grad_norm": 1.4057543277740479,
      "learning_rate": 8.186915887850468e-05,
      "loss": 0.1657,
      "step": 1581
    },
    {
      "epoch": 0.2957285727638097,
      "grad_norm": 1.9416346549987793,
      "learning_rate": 8.179439252336449e-05,
      "loss": 0.3146,
      "step": 1582
    },
    {
      "epoch": 0.29591550612206746,
      "grad_norm": 2.262242078781128,
      "learning_rate": 8.171962616822431e-05,
      "loss": 0.1522,
      "step": 1583
    },
    {
      "epoch": 0.2961024394803253,
      "grad_norm": 1.7465451955795288,
      "learning_rate": 8.164485981308411e-05,
      "loss": 0.1842,
      "step": 1584
    },
    {
      "epoch": 0.29628937283858303,
      "grad_norm": 1.6784968376159668,
      "learning_rate": 8.157009345794393e-05,
      "loss": 0.1915,
      "step": 1585
    },
    {
      "epoch": 0.29647630619684084,
      "grad_norm": 1.9427120685577393,
      "learning_rate": 8.149532710280374e-05,
      "loss": 0.1051,
      "step": 1586
    },
    {
      "epoch": 0.2966632395550986,
      "grad_norm": 1.2599424123764038,
      "learning_rate": 8.142056074766356e-05,
      "loss": 0.1327,
      "step": 1587
    },
    {
      "epoch": 0.2968501729133564,
      "grad_norm": 3.5214526653289795,
      "learning_rate": 8.134579439252336e-05,
      "loss": 0.3087,
      "step": 1588
    },
    {
      "epoch": 0.29703710627161417,
      "grad_norm": 3.6531143188476562,
      "learning_rate": 8.127102803738318e-05,
      "loss": 0.2621,
      "step": 1589
    },
    {
      "epoch": 0.29722403962987193,
      "grad_norm": 4.521125316619873,
      "learning_rate": 8.119626168224299e-05,
      "loss": 0.2305,
      "step": 1590
    },
    {
      "epoch": 0.29741097298812974,
      "grad_norm": 1.8954676389694214,
      "learning_rate": 8.11214953271028e-05,
      "loss": 0.1617,
      "step": 1591
    },
    {
      "epoch": 0.2975979063463875,
      "grad_norm": 1.0803797245025635,
      "learning_rate": 8.104672897196261e-05,
      "loss": 0.2497,
      "step": 1592
    },
    {
      "epoch": 0.2977848397046453,
      "grad_norm": 1.3205264806747437,
      "learning_rate": 8.097196261682244e-05,
      "loss": 0.1615,
      "step": 1593
    },
    {
      "epoch": 0.29797177306290307,
      "grad_norm": 5.080068588256836,
      "learning_rate": 8.089719626168225e-05,
      "loss": 0.1405,
      "step": 1594
    },
    {
      "epoch": 0.2981587064211609,
      "grad_norm": 5.29209566116333,
      "learning_rate": 8.082242990654207e-05,
      "loss": 0.2589,
      "step": 1595
    },
    {
      "epoch": 0.29834563977941864,
      "grad_norm": 1.3659363985061646,
      "learning_rate": 8.074766355140187e-05,
      "loss": 0.2051,
      "step": 1596
    },
    {
      "epoch": 0.2985325731376764,
      "grad_norm": 1.235745906829834,
      "learning_rate": 8.067289719626169e-05,
      "loss": 0.1322,
      "step": 1597
    },
    {
      "epoch": 0.2987195064959342,
      "grad_norm": 3.640453338623047,
      "learning_rate": 8.05981308411215e-05,
      "loss": 0.3883,
      "step": 1598
    },
    {
      "epoch": 0.29890643985419196,
      "grad_norm": 2.516012668609619,
      "learning_rate": 8.052336448598132e-05,
      "loss": 0.2418,
      "step": 1599
    },
    {
      "epoch": 0.2990933732124498,
      "grad_norm": 1.1162240505218506,
      "learning_rate": 8.044859813084112e-05,
      "loss": 0.1738,
      "step": 1600
    },
    {
      "epoch": 0.29928030657070753,
      "grad_norm": 1.4641106128692627,
      "learning_rate": 8.037383177570094e-05,
      "loss": 0.2775,
      "step": 1601
    },
    {
      "epoch": 0.29946723992896535,
      "grad_norm": 1.8343077898025513,
      "learning_rate": 8.029906542056074e-05,
      "loss": 0.1469,
      "step": 1602
    },
    {
      "epoch": 0.2996541732872231,
      "grad_norm": 2.4303195476531982,
      "learning_rate": 8.022429906542056e-05,
      "loss": 0.2637,
      "step": 1603
    },
    {
      "epoch": 0.29984110664548086,
      "grad_norm": 1.2762147188186646,
      "learning_rate": 8.014953271028037e-05,
      "loss": 0.2724,
      "step": 1604
    },
    {
      "epoch": 0.30002804000373867,
      "grad_norm": 2.1215004920959473,
      "learning_rate": 8.00747663551402e-05,
      "loss": 0.1672,
      "step": 1605
    },
    {
      "epoch": 0.30021497336199643,
      "grad_norm": 1.3573989868164062,
      "learning_rate": 8e-05,
      "loss": 0.1549,
      "step": 1606
    },
    {
      "epoch": 0.30040190672025424,
      "grad_norm": 1.8578647375106812,
      "learning_rate": 7.992523364485983e-05,
      "loss": 0.2848,
      "step": 1607
    },
    {
      "epoch": 0.300588840078512,
      "grad_norm": 0.8595573902130127,
      "learning_rate": 7.985046728971963e-05,
      "loss": 0.1455,
      "step": 1608
    },
    {
      "epoch": 0.3007757734367698,
      "grad_norm": 1.8103753328323364,
      "learning_rate": 7.977570093457945e-05,
      "loss": 0.1916,
      "step": 1609
    },
    {
      "epoch": 0.30096270679502757,
      "grad_norm": 2.134777545928955,
      "learning_rate": 7.970093457943925e-05,
      "loss": 0.193,
      "step": 1610
    },
    {
      "epoch": 0.3011496401532854,
      "grad_norm": 1.6682709455490112,
      "learning_rate": 7.962616822429907e-05,
      "loss": 0.2153,
      "step": 1611
    },
    {
      "epoch": 0.30133657351154314,
      "grad_norm": 1.847677230834961,
      "learning_rate": 7.955140186915888e-05,
      "loss": 0.1597,
      "step": 1612
    },
    {
      "epoch": 0.3015235068698009,
      "grad_norm": 0.9392816424369812,
      "learning_rate": 7.94766355140187e-05,
      "loss": 0.1619,
      "step": 1613
    },
    {
      "epoch": 0.3017104402280587,
      "grad_norm": 1.8872383832931519,
      "learning_rate": 7.94018691588785e-05,
      "loss": 0.2128,
      "step": 1614
    },
    {
      "epoch": 0.30189737358631646,
      "grad_norm": 2.7282676696777344,
      "learning_rate": 7.932710280373832e-05,
      "loss": 0.3114,
      "step": 1615
    },
    {
      "epoch": 0.3020843069445743,
      "grad_norm": 2.315150022506714,
      "learning_rate": 7.925233644859813e-05,
      "loss": 0.1596,
      "step": 1616
    },
    {
      "epoch": 0.30227124030283203,
      "grad_norm": 2.5647926330566406,
      "learning_rate": 7.917757009345795e-05,
      "loss": 0.1712,
      "step": 1617
    },
    {
      "epoch": 0.30245817366108985,
      "grad_norm": 1.5058714151382446,
      "learning_rate": 7.910280373831777e-05,
      "loss": 0.2522,
      "step": 1618
    },
    {
      "epoch": 0.3026451070193476,
      "grad_norm": 1.2630939483642578,
      "learning_rate": 7.902803738317758e-05,
      "loss": 0.2242,
      "step": 1619
    },
    {
      "epoch": 0.30283204037760536,
      "grad_norm": 0.6982828974723816,
      "learning_rate": 7.895327102803739e-05,
      "loss": 0.1018,
      "step": 1620
    },
    {
      "epoch": 0.3030189737358632,
      "grad_norm": 1.6587812900543213,
      "learning_rate": 7.887850467289721e-05,
      "loss": 0.249,
      "step": 1621
    },
    {
      "epoch": 0.30320590709412093,
      "grad_norm": 1.5788969993591309,
      "learning_rate": 7.880373831775701e-05,
      "loss": 0.1906,
      "step": 1622
    },
    {
      "epoch": 0.30339284045237874,
      "grad_norm": 1.3976455926895142,
      "learning_rate": 7.872897196261683e-05,
      "loss": 0.2414,
      "step": 1623
    },
    {
      "epoch": 0.3035797738106365,
      "grad_norm": 1.9039392471313477,
      "learning_rate": 7.865420560747664e-05,
      "loss": 0.2162,
      "step": 1624
    },
    {
      "epoch": 0.3037667071688943,
      "grad_norm": 2.9306528568267822,
      "learning_rate": 7.857943925233646e-05,
      "loss": 0.1893,
      "step": 1625
    },
    {
      "epoch": 0.30395364052715207,
      "grad_norm": 2.158698320388794,
      "learning_rate": 7.850467289719626e-05,
      "loss": 0.2809,
      "step": 1626
    },
    {
      "epoch": 0.3041405738854098,
      "grad_norm": 1.0944640636444092,
      "learning_rate": 7.842990654205608e-05,
      "loss": 0.1347,
      "step": 1627
    },
    {
      "epoch": 0.30432750724366764,
      "grad_norm": 4.094513893127441,
      "learning_rate": 7.835514018691589e-05,
      "loss": 0.172,
      "step": 1628
    },
    {
      "epoch": 0.3045144406019254,
      "grad_norm": 2.129864454269409,
      "learning_rate": 7.82803738317757e-05,
      "loss": 0.2551,
      "step": 1629
    },
    {
      "epoch": 0.3047013739601832,
      "grad_norm": 1.2377328872680664,
      "learning_rate": 7.820560747663552e-05,
      "loss": 0.1145,
      "step": 1630
    },
    {
      "epoch": 0.30488830731844097,
      "grad_norm": 1.1930532455444336,
      "learning_rate": 7.813084112149533e-05,
      "loss": 0.1239,
      "step": 1631
    },
    {
      "epoch": 0.3050752406766988,
      "grad_norm": 1.2802903652191162,
      "learning_rate": 7.805607476635515e-05,
      "loss": 0.1168,
      "step": 1632
    },
    {
      "epoch": 0.30526217403495653,
      "grad_norm": 2.9699697494506836,
      "learning_rate": 7.798130841121495e-05,
      "loss": 0.1589,
      "step": 1633
    },
    {
      "epoch": 0.3054491073932143,
      "grad_norm": 1.3982511758804321,
      "learning_rate": 7.790654205607477e-05,
      "loss": 0.1207,
      "step": 1634
    },
    {
      "epoch": 0.3056360407514721,
      "grad_norm": 8.841553688049316,
      "learning_rate": 7.783177570093458e-05,
      "loss": 0.2842,
      "step": 1635
    },
    {
      "epoch": 0.30582297410972986,
      "grad_norm": 3.62738037109375,
      "learning_rate": 7.77570093457944e-05,
      "loss": 0.1739,
      "step": 1636
    },
    {
      "epoch": 0.3060099074679877,
      "grad_norm": 6.755562782287598,
      "learning_rate": 7.76822429906542e-05,
      "loss": 0.2119,
      "step": 1637
    },
    {
      "epoch": 0.30619684082624543,
      "grad_norm": 4.046710968017578,
      "learning_rate": 7.760747663551402e-05,
      "loss": 0.1658,
      "step": 1638
    },
    {
      "epoch": 0.30638377418450324,
      "grad_norm": 2.0903446674346924,
      "learning_rate": 7.753271028037383e-05,
      "loss": 0.2271,
      "step": 1639
    },
    {
      "epoch": 0.306570707542761,
      "grad_norm": 0.8344042897224426,
      "learning_rate": 7.745794392523364e-05,
      "loss": 0.1326,
      "step": 1640
    },
    {
      "epoch": 0.3067576409010188,
      "grad_norm": 3.3816158771514893,
      "learning_rate": 7.738317757009346e-05,
      "loss": 0.1968,
      "step": 1641
    },
    {
      "epoch": 0.30694457425927657,
      "grad_norm": 5.0302653312683105,
      "learning_rate": 7.730841121495328e-05,
      "loss": 0.3125,
      "step": 1642
    },
    {
      "epoch": 0.3071315076175343,
      "grad_norm": 1.0342743396759033,
      "learning_rate": 7.723364485981309e-05,
      "loss": 0.1074,
      "step": 1643
    },
    {
      "epoch": 0.30731844097579214,
      "grad_norm": 2.8861165046691895,
      "learning_rate": 7.71588785046729e-05,
      "loss": 0.1997,
      "step": 1644
    },
    {
      "epoch": 0.3075053743340499,
      "grad_norm": 3.771310567855835,
      "learning_rate": 7.708411214953271e-05,
      "loss": 0.2108,
      "step": 1645
    },
    {
      "epoch": 0.3076923076923077,
      "grad_norm": 1.0085434913635254,
      "learning_rate": 7.700934579439253e-05,
      "loss": 0.0506,
      "step": 1646
    },
    {
      "epoch": 0.30787924105056547,
      "grad_norm": 2.1858718395233154,
      "learning_rate": 7.693457943925234e-05,
      "loss": 0.3202,
      "step": 1647
    },
    {
      "epoch": 0.3080661744088233,
      "grad_norm": 4.069086074829102,
      "learning_rate": 7.685981308411215e-05,
      "loss": 0.1444,
      "step": 1648
    },
    {
      "epoch": 0.30825310776708104,
      "grad_norm": 1.9790009260177612,
      "learning_rate": 7.678504672897196e-05,
      "loss": 0.1099,
      "step": 1649
    },
    {
      "epoch": 0.3084400411253388,
      "grad_norm": 3.204951763153076,
      "learning_rate": 7.671028037383178e-05,
      "loss": 0.2723,
      "step": 1650
    },
    {
      "epoch": 0.3086269744835966,
      "grad_norm": 1.470486044883728,
      "learning_rate": 7.663551401869158e-05,
      "loss": 0.2277,
      "step": 1651
    },
    {
      "epoch": 0.30881390784185436,
      "grad_norm": 1.2585501670837402,
      "learning_rate": 7.65607476635514e-05,
      "loss": 0.1408,
      "step": 1652
    },
    {
      "epoch": 0.3090008412001122,
      "grad_norm": 1.2790913581848145,
      "learning_rate": 7.648598130841121e-05,
      "loss": 0.1895,
      "step": 1653
    },
    {
      "epoch": 0.30918777455836993,
      "grad_norm": 2.3992369174957275,
      "learning_rate": 7.641121495327103e-05,
      "loss": 0.2023,
      "step": 1654
    },
    {
      "epoch": 0.30937470791662774,
      "grad_norm": 2.067119598388672,
      "learning_rate": 7.633644859813085e-05,
      "loss": 0.2159,
      "step": 1655
    },
    {
      "epoch": 0.3095616412748855,
      "grad_norm": 1.5591533184051514,
      "learning_rate": 7.626168224299067e-05,
      "loss": 0.2379,
      "step": 1656
    },
    {
      "epoch": 0.30974857463314326,
      "grad_norm": 1.3457111120224,
      "learning_rate": 7.618691588785047e-05,
      "loss": 0.217,
      "step": 1657
    },
    {
      "epoch": 0.30993550799140107,
      "grad_norm": 4.6837639808654785,
      "learning_rate": 7.611214953271029e-05,
      "loss": 0.2487,
      "step": 1658
    },
    {
      "epoch": 0.31012244134965883,
      "grad_norm": 2.050295829772949,
      "learning_rate": 7.60373831775701e-05,
      "loss": 0.2268,
      "step": 1659
    },
    {
      "epoch": 0.31030937470791664,
      "grad_norm": 1.1471375226974487,
      "learning_rate": 7.596261682242991e-05,
      "loss": 0.1831,
      "step": 1660
    },
    {
      "epoch": 0.3104963080661744,
      "grad_norm": 2.3937439918518066,
      "learning_rate": 7.588785046728972e-05,
      "loss": 0.1483,
      "step": 1661
    },
    {
      "epoch": 0.3106832414244322,
      "grad_norm": 2.670886754989624,
      "learning_rate": 7.581308411214954e-05,
      "loss": 0.1778,
      "step": 1662
    },
    {
      "epoch": 0.31087017478268997,
      "grad_norm": 2.929121732711792,
      "learning_rate": 7.573831775700934e-05,
      "loss": 0.238,
      "step": 1663
    },
    {
      "epoch": 0.3110571081409477,
      "grad_norm": 1.8843271732330322,
      "learning_rate": 7.566355140186916e-05,
      "loss": 0.2817,
      "step": 1664
    },
    {
      "epoch": 0.31124404149920554,
      "grad_norm": 4.039649486541748,
      "learning_rate": 7.558878504672897e-05,
      "loss": 0.2755,
      "step": 1665
    },
    {
      "epoch": 0.3114309748574633,
      "grad_norm": 2.2668380737304688,
      "learning_rate": 7.551401869158879e-05,
      "loss": 0.1228,
      "step": 1666
    },
    {
      "epoch": 0.3116179082157211,
      "grad_norm": 1.8535151481628418,
      "learning_rate": 7.54392523364486e-05,
      "loss": 0.132,
      "step": 1667
    },
    {
      "epoch": 0.31180484157397886,
      "grad_norm": 2.851844310760498,
      "learning_rate": 7.536448598130842e-05,
      "loss": 0.1207,
      "step": 1668
    },
    {
      "epoch": 0.3119917749322367,
      "grad_norm": 3.936202049255371,
      "learning_rate": 7.528971962616823e-05,
      "loss": 0.1272,
      "step": 1669
    },
    {
      "epoch": 0.31217870829049443,
      "grad_norm": 1.9911483526229858,
      "learning_rate": 7.521495327102805e-05,
      "loss": 0.1614,
      "step": 1670
    },
    {
      "epoch": 0.31236564164875225,
      "grad_norm": 1.5300545692443848,
      "learning_rate": 7.514018691588785e-05,
      "loss": 0.2841,
      "step": 1671
    },
    {
      "epoch": 0.31255257500701,
      "grad_norm": 1.433429479598999,
      "learning_rate": 7.506542056074767e-05,
      "loss": 0.1733,
      "step": 1672
    },
    {
      "epoch": 0.31273950836526776,
      "grad_norm": 2.121872901916504,
      "learning_rate": 7.499065420560748e-05,
      "loss": 0.1568,
      "step": 1673
    },
    {
      "epoch": 0.3129264417235256,
      "grad_norm": 0.924599826335907,
      "learning_rate": 7.49158878504673e-05,
      "loss": 0.1256,
      "step": 1674
    },
    {
      "epoch": 0.31311337508178333,
      "grad_norm": 1.3804075717926025,
      "learning_rate": 7.48411214953271e-05,
      "loss": 0.1664,
      "step": 1675
    },
    {
      "epoch": 0.31330030844004114,
      "grad_norm": 1.4802477359771729,
      "learning_rate": 7.476635514018692e-05,
      "loss": 0.2087,
      "step": 1676
    },
    {
      "epoch": 0.3134872417982989,
      "grad_norm": 0.8353661894798279,
      "learning_rate": 7.469158878504673e-05,
      "loss": 0.1614,
      "step": 1677
    },
    {
      "epoch": 0.3136741751565567,
      "grad_norm": 1.5471163988113403,
      "learning_rate": 7.461682242990654e-05,
      "loss": 0.3059,
      "step": 1678
    },
    {
      "epoch": 0.31386110851481447,
      "grad_norm": 1.9030414819717407,
      "learning_rate": 7.454205607476635e-05,
      "loss": 0.2987,
      "step": 1679
    },
    {
      "epoch": 0.3140480418730722,
      "grad_norm": 1.6121325492858887,
      "learning_rate": 7.446728971962618e-05,
      "loss": 0.1569,
      "step": 1680
    },
    {
      "epoch": 0.31423497523133004,
      "grad_norm": 3.154663324356079,
      "learning_rate": 7.439252336448599e-05,
      "loss": 0.2436,
      "step": 1681
    },
    {
      "epoch": 0.3144219085895878,
      "grad_norm": 2.294600248336792,
      "learning_rate": 7.43177570093458e-05,
      "loss": 0.2558,
      "step": 1682
    },
    {
      "epoch": 0.3146088419478456,
      "grad_norm": 7.522895812988281,
      "learning_rate": 7.424299065420561e-05,
      "loss": 0.4567,
      "step": 1683
    },
    {
      "epoch": 0.31479577530610336,
      "grad_norm": 1.284317970275879,
      "learning_rate": 7.416822429906543e-05,
      "loss": 0.1029,
      "step": 1684
    },
    {
      "epoch": 0.3149827086643612,
      "grad_norm": 1.5253299474716187,
      "learning_rate": 7.409345794392524e-05,
      "loss": 0.0892,
      "step": 1685
    },
    {
      "epoch": 0.31516964202261893,
      "grad_norm": 2.951262950897217,
      "learning_rate": 7.401869158878506e-05,
      "loss": 0.2582,
      "step": 1686
    },
    {
      "epoch": 0.3153565753808767,
      "grad_norm": 1.9432393312454224,
      "learning_rate": 7.394392523364486e-05,
      "loss": 0.1511,
      "step": 1687
    },
    {
      "epoch": 0.3155435087391345,
      "grad_norm": 3.2087368965148926,
      "learning_rate": 7.386915887850468e-05,
      "loss": 0.2749,
      "step": 1688
    },
    {
      "epoch": 0.31573044209739226,
      "grad_norm": 1.722404956817627,
      "learning_rate": 7.379439252336448e-05,
      "loss": 0.1964,
      "step": 1689
    },
    {
      "epoch": 0.3159173754556501,
      "grad_norm": 3.367493152618408,
      "learning_rate": 7.37196261682243e-05,
      "loss": 0.1622,
      "step": 1690
    },
    {
      "epoch": 0.31610430881390783,
      "grad_norm": 4.16855001449585,
      "learning_rate": 7.364485981308411e-05,
      "loss": 0.1517,
      "step": 1691
    },
    {
      "epoch": 0.31629124217216564,
      "grad_norm": 4.591294765472412,
      "learning_rate": 7.357009345794393e-05,
      "loss": 0.1281,
      "step": 1692
    },
    {
      "epoch": 0.3164781755304234,
      "grad_norm": 3.6449267864227295,
      "learning_rate": 7.349532710280375e-05,
      "loss": 0.2838,
      "step": 1693
    },
    {
      "epoch": 0.3166651088886812,
      "grad_norm": 1.1345717906951904,
      "learning_rate": 7.342056074766355e-05,
      "loss": 0.1813,
      "step": 1694
    },
    {
      "epoch": 0.31685204224693897,
      "grad_norm": 2.980252981185913,
      "learning_rate": 7.334579439252337e-05,
      "loss": 0.1967,
      "step": 1695
    },
    {
      "epoch": 0.3170389756051967,
      "grad_norm": 1.3208290338516235,
      "learning_rate": 7.327102803738318e-05,
      "loss": 0.1388,
      "step": 1696
    },
    {
      "epoch": 0.31722590896345454,
      "grad_norm": 1.423819661140442,
      "learning_rate": 7.3196261682243e-05,
      "loss": 0.1527,
      "step": 1697
    },
    {
      "epoch": 0.3174128423217123,
      "grad_norm": 1.6845688819885254,
      "learning_rate": 7.312149532710281e-05,
      "loss": 0.2387,
      "step": 1698
    },
    {
      "epoch": 0.3175997756799701,
      "grad_norm": 0.811410129070282,
      "learning_rate": 7.304672897196262e-05,
      "loss": 0.1134,
      "step": 1699
    },
    {
      "epoch": 0.31778670903822787,
      "grad_norm": 3.867779493331909,
      "learning_rate": 7.297196261682244e-05,
      "loss": 0.1433,
      "step": 1700
    },
    {
      "epoch": 0.3179736423964857,
      "grad_norm": 2.0553650856018066,
      "learning_rate": 7.289719626168224e-05,
      "loss": 0.2062,
      "step": 1701
    },
    {
      "epoch": 0.31816057575474344,
      "grad_norm": 1.7156994342803955,
      "learning_rate": 7.282242990654206e-05,
      "loss": 0.2266,
      "step": 1702
    },
    {
      "epoch": 0.3183475091130012,
      "grad_norm": 1.8197238445281982,
      "learning_rate": 7.274766355140187e-05,
      "loss": 0.1755,
      "step": 1703
    },
    {
      "epoch": 0.318534442471259,
      "grad_norm": 2.218161106109619,
      "learning_rate": 7.267289719626169e-05,
      "loss": 0.1401,
      "step": 1704
    },
    {
      "epoch": 0.31872137582951676,
      "grad_norm": 0.7959603071212769,
      "learning_rate": 7.25981308411215e-05,
      "loss": 0.1023,
      "step": 1705
    },
    {
      "epoch": 0.3189083091877746,
      "grad_norm": 1.6598151922225952,
      "learning_rate": 7.252336448598131e-05,
      "loss": 0.1412,
      "step": 1706
    },
    {
      "epoch": 0.31909524254603233,
      "grad_norm": 1.9857800006866455,
      "learning_rate": 7.244859813084113e-05,
      "loss": 0.2303,
      "step": 1707
    },
    {
      "epoch": 0.31928217590429014,
      "grad_norm": 1.341336727142334,
      "learning_rate": 7.237383177570093e-05,
      "loss": 0.2503,
      "step": 1708
    },
    {
      "epoch": 0.3194691092625479,
      "grad_norm": 1.273614764213562,
      "learning_rate": 7.229906542056075e-05,
      "loss": 0.1398,
      "step": 1709
    },
    {
      "epoch": 0.31965604262080566,
      "grad_norm": 1.537294626235962,
      "learning_rate": 7.222429906542056e-05,
      "loss": 0.1703,
      "step": 1710
    },
    {
      "epoch": 0.31984297597906347,
      "grad_norm": 0.9082763195037842,
      "learning_rate": 7.214953271028038e-05,
      "loss": 0.0423,
      "step": 1711
    },
    {
      "epoch": 0.32002990933732123,
      "grad_norm": 2.2633190155029297,
      "learning_rate": 7.207476635514018e-05,
      "loss": 0.2439,
      "step": 1712
    },
    {
      "epoch": 0.32021684269557904,
      "grad_norm": 1.2620881795883179,
      "learning_rate": 7.2e-05,
      "loss": 0.1778,
      "step": 1713
    },
    {
      "epoch": 0.3204037760538368,
      "grad_norm": 2.4969189167022705,
      "learning_rate": 7.192523364485981e-05,
      "loss": 0.2312,
      "step": 1714
    },
    {
      "epoch": 0.3205907094120946,
      "grad_norm": 1.0520941019058228,
      "learning_rate": 7.185046728971963e-05,
      "loss": 0.2321,
      "step": 1715
    },
    {
      "epoch": 0.32077764277035237,
      "grad_norm": 0.9026798009872437,
      "learning_rate": 7.177570093457943e-05,
      "loss": 0.1283,
      "step": 1716
    },
    {
      "epoch": 0.3209645761286101,
      "grad_norm": 2.613842725753784,
      "learning_rate": 7.170093457943926e-05,
      "loss": 0.1578,
      "step": 1717
    },
    {
      "epoch": 0.32115150948686794,
      "grad_norm": 4.887718200683594,
      "learning_rate": 7.162616822429907e-05,
      "loss": 0.3459,
      "step": 1718
    },
    {
      "epoch": 0.3213384428451257,
      "grad_norm": 1.270736813545227,
      "learning_rate": 7.155140186915889e-05,
      "loss": 0.2036,
      "step": 1719
    },
    {
      "epoch": 0.3215253762033835,
      "grad_norm": 2.2404568195343018,
      "learning_rate": 7.14766355140187e-05,
      "loss": 0.1728,
      "step": 1720
    },
    {
      "epoch": 0.32171230956164126,
      "grad_norm": 1.3671931028366089,
      "learning_rate": 7.140186915887851e-05,
      "loss": 0.2135,
      "step": 1721
    },
    {
      "epoch": 0.3218992429198991,
      "grad_norm": 0.9476264119148254,
      "learning_rate": 7.132710280373832e-05,
      "loss": 0.1415,
      "step": 1722
    },
    {
      "epoch": 0.32208617627815683,
      "grad_norm": 1.5193008184432983,
      "learning_rate": 7.125233644859814e-05,
      "loss": 0.1982,
      "step": 1723
    },
    {
      "epoch": 0.32227310963641465,
      "grad_norm": 1.3715078830718994,
      "learning_rate": 7.117757009345794e-05,
      "loss": 0.1184,
      "step": 1724
    },
    {
      "epoch": 0.3224600429946724,
      "grad_norm": 1.4903068542480469,
      "learning_rate": 7.110280373831776e-05,
      "loss": 0.127,
      "step": 1725
    },
    {
      "epoch": 0.32264697635293016,
      "grad_norm": 4.175466537475586,
      "learning_rate": 7.102803738317757e-05,
      "loss": 0.2655,
      "step": 1726
    },
    {
      "epoch": 0.32283390971118797,
      "grad_norm": 1.6110249757766724,
      "learning_rate": 7.095327102803738e-05,
      "loss": 0.1913,
      "step": 1727
    },
    {
      "epoch": 0.32302084306944573,
      "grad_norm": 1.8921167850494385,
      "learning_rate": 7.087850467289719e-05,
      "loss": 0.2678,
      "step": 1728
    },
    {
      "epoch": 0.32320777642770354,
      "grad_norm": 1.2732635736465454,
      "learning_rate": 7.080373831775702e-05,
      "loss": 0.1418,
      "step": 1729
    },
    {
      "epoch": 0.3233947097859613,
      "grad_norm": 1.599778652191162,
      "learning_rate": 7.072897196261683e-05,
      "loss": 0.1805,
      "step": 1730
    },
    {
      "epoch": 0.3235816431442191,
      "grad_norm": 3.9656214714050293,
      "learning_rate": 7.065420560747665e-05,
      "loss": 0.1833,
      "step": 1731
    },
    {
      "epoch": 0.32376857650247687,
      "grad_norm": 1.312942385673523,
      "learning_rate": 7.057943925233645e-05,
      "loss": 0.1551,
      "step": 1732
    },
    {
      "epoch": 0.3239555098607346,
      "grad_norm": 1.4923020601272583,
      "learning_rate": 7.050467289719627e-05,
      "loss": 0.2711,
      "step": 1733
    },
    {
      "epoch": 0.32414244321899244,
      "grad_norm": 1.3846044540405273,
      "learning_rate": 7.042990654205608e-05,
      "loss": 0.2088,
      "step": 1734
    },
    {
      "epoch": 0.3243293765772502,
      "grad_norm": 1.0077710151672363,
      "learning_rate": 7.03551401869159e-05,
      "loss": 0.1659,
      "step": 1735
    },
    {
      "epoch": 0.324516309935508,
      "grad_norm": 2.260458469390869,
      "learning_rate": 7.02803738317757e-05,
      "loss": 0.1757,
      "step": 1736
    },
    {
      "epoch": 0.32470324329376576,
      "grad_norm": 1.5733728408813477,
      "learning_rate": 7.020560747663552e-05,
      "loss": 0.1114,
      "step": 1737
    },
    {
      "epoch": 0.3248901766520236,
      "grad_norm": 2.4888052940368652,
      "learning_rate": 7.013084112149532e-05,
      "loss": 0.219,
      "step": 1738
    },
    {
      "epoch": 0.32507711001028133,
      "grad_norm": 1.855991005897522,
      "learning_rate": 7.005607476635514e-05,
      "loss": 0.0861,
      "step": 1739
    },
    {
      "epoch": 0.3252640433685391,
      "grad_norm": 0.767894446849823,
      "learning_rate": 6.998130841121495e-05,
      "loss": 0.1087,
      "step": 1740
    },
    {
      "epoch": 0.3254509767267969,
      "grad_norm": 1.6807928085327148,
      "learning_rate": 6.990654205607478e-05,
      "loss": 0.1987,
      "step": 1741
    },
    {
      "epoch": 0.32563791008505466,
      "grad_norm": 1.0147305727005005,
      "learning_rate": 6.983177570093459e-05,
      "loss": 0.1911,
      "step": 1742
    },
    {
      "epoch": 0.3258248434433125,
      "grad_norm": 0.6123443841934204,
      "learning_rate": 6.97570093457944e-05,
      "loss": 0.0755,
      "step": 1743
    },
    {
      "epoch": 0.32601177680157023,
      "grad_norm": 3.8843133449554443,
      "learning_rate": 6.968224299065421e-05,
      "loss": 0.1745,
      "step": 1744
    },
    {
      "epoch": 0.32619871015982804,
      "grad_norm": 1.5212243795394897,
      "learning_rate": 6.960747663551403e-05,
      "loss": 0.1492,
      "step": 1745
    },
    {
      "epoch": 0.3263856435180858,
      "grad_norm": 2.5939698219299316,
      "learning_rate": 6.953271028037383e-05,
      "loss": 0.1563,
      "step": 1746
    },
    {
      "epoch": 0.32657257687634356,
      "grad_norm": 2.6305205821990967,
      "learning_rate": 6.945794392523365e-05,
      "loss": 0.133,
      "step": 1747
    },
    {
      "epoch": 0.32675951023460137,
      "grad_norm": 1.3612427711486816,
      "learning_rate": 6.938317757009346e-05,
      "loss": 0.0969,
      "step": 1748
    },
    {
      "epoch": 0.3269464435928591,
      "grad_norm": 0.946611762046814,
      "learning_rate": 6.930841121495328e-05,
      "loss": 0.084,
      "step": 1749
    },
    {
      "epoch": 0.32713337695111694,
      "grad_norm": 1.4780619144439697,
      "learning_rate": 6.923364485981308e-05,
      "loss": 0.0932,
      "step": 1750
    },
    {
      "epoch": 0.3273203103093747,
      "grad_norm": 2.5038089752197266,
      "learning_rate": 6.91588785046729e-05,
      "loss": 0.1771,
      "step": 1751
    },
    {
      "epoch": 0.3275072436676325,
      "grad_norm": 4.635286808013916,
      "learning_rate": 6.908411214953271e-05,
      "loss": 0.3309,
      "step": 1752
    },
    {
      "epoch": 0.32769417702589027,
      "grad_norm": 1.8490424156188965,
      "learning_rate": 6.900934579439253e-05,
      "loss": 0.1597,
      "step": 1753
    },
    {
      "epoch": 0.3278811103841481,
      "grad_norm": 3.0025601387023926,
      "learning_rate": 6.893457943925234e-05,
      "loss": 0.1309,
      "step": 1754
    },
    {
      "epoch": 0.32806804374240583,
      "grad_norm": 2.4277102947235107,
      "learning_rate": 6.885981308411215e-05,
      "loss": 0.2462,
      "step": 1755
    },
    {
      "epoch": 0.3282549771006636,
      "grad_norm": 2.2030553817749023,
      "learning_rate": 6.878504672897197e-05,
      "loss": 0.1697,
      "step": 1756
    },
    {
      "epoch": 0.3284419104589214,
      "grad_norm": 2.019944667816162,
      "learning_rate": 6.871028037383179e-05,
      "loss": 0.2098,
      "step": 1757
    },
    {
      "epoch": 0.32862884381717916,
      "grad_norm": 2.7889702320098877,
      "learning_rate": 6.86355140186916e-05,
      "loss": 0.2432,
      "step": 1758
    },
    {
      "epoch": 0.328815777175437,
      "grad_norm": 5.19071626663208,
      "learning_rate": 6.856074766355141e-05,
      "loss": 0.2194,
      "step": 1759
    },
    {
      "epoch": 0.32900271053369473,
      "grad_norm": 4.461239814758301,
      "learning_rate": 6.848598130841122e-05,
      "loss": 0.2176,
      "step": 1760
    },
    {
      "epoch": 0.32918964389195254,
      "grad_norm": 1.7494430541992188,
      "learning_rate": 6.841121495327104e-05,
      "loss": 0.1555,
      "step": 1761
    },
    {
      "epoch": 0.3293765772502103,
      "grad_norm": 1.548798680305481,
      "learning_rate": 6.833644859813084e-05,
      "loss": 0.1981,
      "step": 1762
    },
    {
      "epoch": 0.32956351060846806,
      "grad_norm": 1.774588942527771,
      "learning_rate": 6.826168224299066e-05,
      "loss": 0.12,
      "step": 1763
    },
    {
      "epoch": 0.32975044396672587,
      "grad_norm": 1.9191491603851318,
      "learning_rate": 6.818691588785047e-05,
      "loss": 0.254,
      "step": 1764
    },
    {
      "epoch": 0.3299373773249836,
      "grad_norm": 1.1308844089508057,
      "learning_rate": 6.811214953271028e-05,
      "loss": 0.1412,
      "step": 1765
    },
    {
      "epoch": 0.33012431068324144,
      "grad_norm": 2.0873541831970215,
      "learning_rate": 6.80373831775701e-05,
      "loss": 0.1663,
      "step": 1766
    },
    {
      "epoch": 0.3303112440414992,
      "grad_norm": 1.7236926555633545,
      "learning_rate": 6.796261682242991e-05,
      "loss": 0.2031,
      "step": 1767
    },
    {
      "epoch": 0.330498177399757,
      "grad_norm": 2.146456718444824,
      "learning_rate": 6.788785046728973e-05,
      "loss": 0.1566,
      "step": 1768
    },
    {
      "epoch": 0.33068511075801477,
      "grad_norm": 2.0385286808013916,
      "learning_rate": 6.781308411214953e-05,
      "loss": 0.2128,
      "step": 1769
    },
    {
      "epoch": 0.3308720441162725,
      "grad_norm": 2.4579615592956543,
      "learning_rate": 6.773831775700935e-05,
      "loss": 0.1928,
      "step": 1770
    },
    {
      "epoch": 0.33105897747453034,
      "grad_norm": 1.2291285991668701,
      "learning_rate": 6.766355140186916e-05,
      "loss": 0.2739,
      "step": 1771
    },
    {
      "epoch": 0.3312459108327881,
      "grad_norm": 1.6722954511642456,
      "learning_rate": 6.758878504672898e-05,
      "loss": 0.1435,
      "step": 1772
    },
    {
      "epoch": 0.3314328441910459,
      "grad_norm": 1.443207025527954,
      "learning_rate": 6.751401869158878e-05,
      "loss": 0.1407,
      "step": 1773
    },
    {
      "epoch": 0.33161977754930366,
      "grad_norm": 1.0653339624404907,
      "learning_rate": 6.74392523364486e-05,
      "loss": 0.1466,
      "step": 1774
    },
    {
      "epoch": 0.3318067109075615,
      "grad_norm": 3.250413179397583,
      "learning_rate": 6.73644859813084e-05,
      "loss": 0.2644,
      "step": 1775
    },
    {
      "epoch": 0.33199364426581923,
      "grad_norm": 1.6920814514160156,
      "learning_rate": 6.728971962616822e-05,
      "loss": 0.3239,
      "step": 1776
    },
    {
      "epoch": 0.332180577624077,
      "grad_norm": 1.889595627784729,
      "learning_rate": 6.721495327102803e-05,
      "loss": 0.2768,
      "step": 1777
    },
    {
      "epoch": 0.3323675109823348,
      "grad_norm": 1.5631908178329468,
      "learning_rate": 6.714018691588785e-05,
      "loss": 0.2765,
      "step": 1778
    },
    {
      "epoch": 0.33255444434059256,
      "grad_norm": 1.9624212980270386,
      "learning_rate": 6.706542056074767e-05,
      "loss": 0.1527,
      "step": 1779
    },
    {
      "epoch": 0.33274137769885037,
      "grad_norm": 0.985239565372467,
      "learning_rate": 6.699065420560749e-05,
      "loss": 0.1355,
      "step": 1780
    },
    {
      "epoch": 0.33292831105710813,
      "grad_norm": 3.0774753093719482,
      "learning_rate": 6.691588785046729e-05,
      "loss": 0.2532,
      "step": 1781
    },
    {
      "epoch": 0.33311524441536594,
      "grad_norm": 1.6296443939208984,
      "learning_rate": 6.684112149532711e-05,
      "loss": 0.1941,
      "step": 1782
    },
    {
      "epoch": 0.3333021777736237,
      "grad_norm": 1.081154465675354,
      "learning_rate": 6.676635514018692e-05,
      "loss": 0.1678,
      "step": 1783
    },
    {
      "epoch": 0.3334891111318815,
      "grad_norm": 1.7832462787628174,
      "learning_rate": 6.669158878504673e-05,
      "loss": 0.235,
      "step": 1784
    },
    {
      "epoch": 0.33367604449013927,
      "grad_norm": 1.4161186218261719,
      "learning_rate": 6.661682242990654e-05,
      "loss": 0.1755,
      "step": 1785
    },
    {
      "epoch": 0.333862977848397,
      "grad_norm": 3.3842790126800537,
      "learning_rate": 6.654205607476636e-05,
      "loss": 0.2212,
      "step": 1786
    },
    {
      "epoch": 0.33404991120665484,
      "grad_norm": 1.4348578453063965,
      "learning_rate": 6.646728971962616e-05,
      "loss": 0.151,
      "step": 1787
    },
    {
      "epoch": 0.3342368445649126,
      "grad_norm": 3.938055992126465,
      "learning_rate": 6.639252336448598e-05,
      "loss": 0.1771,
      "step": 1788
    },
    {
      "epoch": 0.3344237779231704,
      "grad_norm": 8.608002662658691,
      "learning_rate": 6.631775700934579e-05,
      "loss": 0.3,
      "step": 1789
    },
    {
      "epoch": 0.33461071128142816,
      "grad_norm": 2.897308349609375,
      "learning_rate": 6.624299065420561e-05,
      "loss": 0.2153,
      "step": 1790
    },
    {
      "epoch": 0.334797644639686,
      "grad_norm": 1.705618143081665,
      "learning_rate": 6.616822429906543e-05,
      "loss": 0.2471,
      "step": 1791
    },
    {
      "epoch": 0.33498457799794373,
      "grad_norm": 4.2112908363342285,
      "learning_rate": 6.609345794392525e-05,
      "loss": 0.2207,
      "step": 1792
    },
    {
      "epoch": 0.3351715113562015,
      "grad_norm": 6.9181437492370605,
      "learning_rate": 6.601869158878505e-05,
      "loss": 0.5226,
      "step": 1793
    },
    {
      "epoch": 0.3353584447144593,
      "grad_norm": 8.039395332336426,
      "learning_rate": 6.594392523364487e-05,
      "loss": 0.282,
      "step": 1794
    },
    {
      "epoch": 0.33554537807271706,
      "grad_norm": 3.641061544418335,
      "learning_rate": 6.586915887850467e-05,
      "loss": 0.2683,
      "step": 1795
    },
    {
      "epoch": 0.3357323114309749,
      "grad_norm": 5.904903411865234,
      "learning_rate": 6.57943925233645e-05,
      "loss": 0.176,
      "step": 1796
    },
    {
      "epoch": 0.33591924478923263,
      "grad_norm": 11.968759536743164,
      "learning_rate": 6.57196261682243e-05,
      "loss": 0.6029,
      "step": 1797
    },
    {
      "epoch": 0.33610617814749044,
      "grad_norm": 1.8748915195465088,
      "learning_rate": 6.564485981308412e-05,
      "loss": 0.104,
      "step": 1798
    },
    {
      "epoch": 0.3362931115057482,
      "grad_norm": 3.024045705795288,
      "learning_rate": 6.557009345794392e-05,
      "loss": 0.2083,
      "step": 1799
    },
    {
      "epoch": 0.33648004486400596,
      "grad_norm": 1.6019957065582275,
      "learning_rate": 6.549532710280374e-05,
      "loss": 0.0881,
      "step": 1800
    },
    {
      "epoch": 0.33666697822226377,
      "grad_norm": 3.246495008468628,
      "learning_rate": 6.542056074766355e-05,
      "loss": 0.1796,
      "step": 1801
    },
    {
      "epoch": 0.3368539115805215,
      "grad_norm": 4.209509372711182,
      "learning_rate": 6.534579439252337e-05,
      "loss": 0.2351,
      "step": 1802
    },
    {
      "epoch": 0.33704084493877934,
      "grad_norm": 1.2976865768432617,
      "learning_rate": 6.527102803738318e-05,
      "loss": 0.11,
      "step": 1803
    },
    {
      "epoch": 0.3372277782970371,
      "grad_norm": 1.5709259510040283,
      "learning_rate": 6.5196261682243e-05,
      "loss": 0.0842,
      "step": 1804
    },
    {
      "epoch": 0.3374147116552949,
      "grad_norm": 3.508004903793335,
      "learning_rate": 6.512149532710281e-05,
      "loss": 0.3016,
      "step": 1805
    },
    {
      "epoch": 0.33760164501355266,
      "grad_norm": 1.6240943670272827,
      "learning_rate": 6.504672897196263e-05,
      "loss": 0.1435,
      "step": 1806
    },
    {
      "epoch": 0.3377885783718105,
      "grad_norm": 1.9958630800247192,
      "learning_rate": 6.497196261682243e-05,
      "loss": 0.2271,
      "step": 1807
    },
    {
      "epoch": 0.33797551173006823,
      "grad_norm": 1.71764075756073,
      "learning_rate": 6.489719626168225e-05,
      "loss": 0.1673,
      "step": 1808
    },
    {
      "epoch": 0.338162445088326,
      "grad_norm": 2.909655809402466,
      "learning_rate": 6.482242990654206e-05,
      "loss": 0.3056,
      "step": 1809
    },
    {
      "epoch": 0.3383493784465838,
      "grad_norm": 1.7260226011276245,
      "learning_rate": 6.474766355140188e-05,
      "loss": 0.0748,
      "step": 1810
    },
    {
      "epoch": 0.33853631180484156,
      "grad_norm": 1.2180284261703491,
      "learning_rate": 6.467289719626168e-05,
      "loss": 0.1394,
      "step": 1811
    },
    {
      "epoch": 0.3387232451630994,
      "grad_norm": 2.316310167312622,
      "learning_rate": 6.45981308411215e-05,
      "loss": 0.1929,
      "step": 1812
    },
    {
      "epoch": 0.33891017852135713,
      "grad_norm": 1.3749115467071533,
      "learning_rate": 6.45233644859813e-05,
      "loss": 0.2138,
      "step": 1813
    },
    {
      "epoch": 0.33909711187961494,
      "grad_norm": 2.39593243598938,
      "learning_rate": 6.444859813084112e-05,
      "loss": 0.1416,
      "step": 1814
    },
    {
      "epoch": 0.3392840452378727,
      "grad_norm": 2.156883955001831,
      "learning_rate": 6.437383177570093e-05,
      "loss": 0.2424,
      "step": 1815
    },
    {
      "epoch": 0.33947097859613046,
      "grad_norm": 2.482574701309204,
      "learning_rate": 6.429906542056076e-05,
      "loss": 0.1363,
      "step": 1816
    },
    {
      "epoch": 0.33965791195438827,
      "grad_norm": 2.113873243331909,
      "learning_rate": 6.422429906542057e-05,
      "loss": 0.2017,
      "step": 1817
    },
    {
      "epoch": 0.339844845312646,
      "grad_norm": 2.1515984535217285,
      "learning_rate": 6.414953271028039e-05,
      "loss": 0.1759,
      "step": 1818
    },
    {
      "epoch": 0.34003177867090384,
      "grad_norm": 1.3284541368484497,
      "learning_rate": 6.407476635514019e-05,
      "loss": 0.1467,
      "step": 1819
    },
    {
      "epoch": 0.3402187120291616,
      "grad_norm": 1.5720133781433105,
      "learning_rate": 6.400000000000001e-05,
      "loss": 0.2197,
      "step": 1820
    },
    {
      "epoch": 0.3404056453874194,
      "grad_norm": 1.729273796081543,
      "learning_rate": 6.392523364485982e-05,
      "loss": 0.2671,
      "step": 1821
    },
    {
      "epoch": 0.34059257874567717,
      "grad_norm": 1.2820392847061157,
      "learning_rate": 6.385046728971963e-05,
      "loss": 0.0919,
      "step": 1822
    },
    {
      "epoch": 0.3407795121039349,
      "grad_norm": 2.646833658218384,
      "learning_rate": 6.377570093457944e-05,
      "loss": 0.2181,
      "step": 1823
    },
    {
      "epoch": 0.34096644546219274,
      "grad_norm": 2.2207953929901123,
      "learning_rate": 6.370093457943926e-05,
      "loss": 0.2725,
      "step": 1824
    },
    {
      "epoch": 0.3411533788204505,
      "grad_norm": 1.504810094833374,
      "learning_rate": 6.362616822429906e-05,
      "loss": 0.1517,
      "step": 1825
    },
    {
      "epoch": 0.3413403121787083,
      "grad_norm": 2.07246994972229,
      "learning_rate": 6.355140186915888e-05,
      "loss": 0.2121,
      "step": 1826
    },
    {
      "epoch": 0.34152724553696606,
      "grad_norm": 3.1984381675720215,
      "learning_rate": 6.347663551401869e-05,
      "loss": 0.3015,
      "step": 1827
    },
    {
      "epoch": 0.3417141788952239,
      "grad_norm": 1.1313172578811646,
      "learning_rate": 6.340186915887851e-05,
      "loss": 0.155,
      "step": 1828
    },
    {
      "epoch": 0.34190111225348163,
      "grad_norm": 1.2997959852218628,
      "learning_rate": 6.332710280373833e-05,
      "loss": 0.1529,
      "step": 1829
    },
    {
      "epoch": 0.3420880456117394,
      "grad_norm": 2.431913375854492,
      "learning_rate": 6.325233644859813e-05,
      "loss": 0.2527,
      "step": 1830
    },
    {
      "epoch": 0.3422749789699972,
      "grad_norm": 0.926313579082489,
      "learning_rate": 6.317757009345795e-05,
      "loss": 0.1378,
      "step": 1831
    },
    {
      "epoch": 0.34246191232825496,
      "grad_norm": 1.3787002563476562,
      "learning_rate": 6.310280373831776e-05,
      "loss": 0.1931,
      "step": 1832
    },
    {
      "epoch": 0.34264884568651277,
      "grad_norm": 1.6046477556228638,
      "learning_rate": 6.302803738317757e-05,
      "loss": 0.1234,
      "step": 1833
    },
    {
      "epoch": 0.34283577904477053,
      "grad_norm": 1.7132617235183716,
      "learning_rate": 6.295327102803738e-05,
      "loss": 0.277,
      "step": 1834
    },
    {
      "epoch": 0.34302271240302834,
      "grad_norm": 9.648728370666504,
      "learning_rate": 6.28785046728972e-05,
      "loss": 0.2275,
      "step": 1835
    },
    {
      "epoch": 0.3432096457612861,
      "grad_norm": 2.115438938140869,
      "learning_rate": 6.2803738317757e-05,
      "loss": 0.1857,
      "step": 1836
    },
    {
      "epoch": 0.3433965791195439,
      "grad_norm": 3.0020458698272705,
      "learning_rate": 6.272897196261682e-05,
      "loss": 0.1853,
      "step": 1837
    },
    {
      "epoch": 0.34358351247780167,
      "grad_norm": 1.1448134183883667,
      "learning_rate": 6.265420560747663e-05,
      "loss": 0.0541,
      "step": 1838
    },
    {
      "epoch": 0.3437704458360594,
      "grad_norm": 1.500717282295227,
      "learning_rate": 6.257943925233645e-05,
      "loss": 0.1952,
      "step": 1839
    },
    {
      "epoch": 0.34395737919431724,
      "grad_norm": 0.9454162120819092,
      "learning_rate": 6.250467289719625e-05,
      "loss": 0.1297,
      "step": 1840
    },
    {
      "epoch": 0.344144312552575,
      "grad_norm": 1.1269960403442383,
      "learning_rate": 6.242990654205608e-05,
      "loss": 0.1716,
      "step": 1841
    },
    {
      "epoch": 0.3443312459108328,
      "grad_norm": 1.5122243165969849,
      "learning_rate": 6.235514018691589e-05,
      "loss": 0.2752,
      "step": 1842
    },
    {
      "epoch": 0.34451817926909056,
      "grad_norm": 6.682120323181152,
      "learning_rate": 6.228037383177571e-05,
      "loss": 0.3083,
      "step": 1843
    },
    {
      "epoch": 0.3447051126273484,
      "grad_norm": 3.0849149227142334,
      "learning_rate": 6.220560747663551e-05,
      "loss": 0.194,
      "step": 1844
    },
    {
      "epoch": 0.34489204598560613,
      "grad_norm": 3.048957109451294,
      "learning_rate": 6.213084112149533e-05,
      "loss": 0.2206,
      "step": 1845
    },
    {
      "epoch": 0.3450789793438639,
      "grad_norm": 1.2121479511260986,
      "learning_rate": 6.205607476635514e-05,
      "loss": 0.1388,
      "step": 1846
    },
    {
      "epoch": 0.3452659127021217,
      "grad_norm": 2.124545097351074,
      "learning_rate": 6.198130841121496e-05,
      "loss": 0.1968,
      "step": 1847
    },
    {
      "epoch": 0.34545284606037946,
      "grad_norm": 1.0268217325210571,
      "learning_rate": 6.190654205607476e-05,
      "loss": 0.0423,
      "step": 1848
    },
    {
      "epoch": 0.34563977941863727,
      "grad_norm": 1.3337630033493042,
      "learning_rate": 6.183177570093458e-05,
      "loss": 0.1699,
      "step": 1849
    },
    {
      "epoch": 0.34582671277689503,
      "grad_norm": 1.921126365661621,
      "learning_rate": 6.175700934579439e-05,
      "loss": 0.1878,
      "step": 1850
    },
    {
      "epoch": 0.34601364613515284,
      "grad_norm": 1.2849105596542358,
      "learning_rate": 6.16822429906542e-05,
      "loss": 0.1328,
      "step": 1851
    },
    {
      "epoch": 0.3462005794934106,
      "grad_norm": 0.9950743913650513,
      "learning_rate": 6.160747663551401e-05,
      "loss": 0.0939,
      "step": 1852
    },
    {
      "epoch": 0.34638751285166836,
      "grad_norm": 1.7500189542770386,
      "learning_rate": 6.153271028037384e-05,
      "loss": 0.1861,
      "step": 1853
    },
    {
      "epoch": 0.34657444620992617,
      "grad_norm": 1.5498173236846924,
      "learning_rate": 6.145794392523365e-05,
      "loss": 0.136,
      "step": 1854
    },
    {
      "epoch": 0.3467613795681839,
      "grad_norm": 1.39406418800354,
      "learning_rate": 6.138317757009347e-05,
      "loss": 0.1121,
      "step": 1855
    },
    {
      "epoch": 0.34694831292644174,
      "grad_norm": 1.860533595085144,
      "learning_rate": 6.130841121495327e-05,
      "loss": 0.2131,
      "step": 1856
    },
    {
      "epoch": 0.3471352462846995,
      "grad_norm": 3.5251216888427734,
      "learning_rate": 6.123364485981309e-05,
      "loss": 0.226,
      "step": 1857
    },
    {
      "epoch": 0.3473221796429573,
      "grad_norm": 1.0612692832946777,
      "learning_rate": 6.11588785046729e-05,
      "loss": 0.1122,
      "step": 1858
    },
    {
      "epoch": 0.34750911300121506,
      "grad_norm": 1.983152985572815,
      "learning_rate": 6.108411214953272e-05,
      "loss": 0.1431,
      "step": 1859
    },
    {
      "epoch": 0.3476960463594728,
      "grad_norm": 1.559633731842041,
      "learning_rate": 6.100934579439253e-05,
      "loss": 0.1674,
      "step": 1860
    },
    {
      "epoch": 0.34788297971773063,
      "grad_norm": 4.886540412902832,
      "learning_rate": 6.093457943925234e-05,
      "loss": 0.2435,
      "step": 1861
    },
    {
      "epoch": 0.3480699130759884,
      "grad_norm": 6.150725841522217,
      "learning_rate": 6.085981308411215e-05,
      "loss": 0.1854,
      "step": 1862
    },
    {
      "epoch": 0.3482568464342462,
      "grad_norm": 1.9743561744689941,
      "learning_rate": 6.0785046728971964e-05,
      "loss": 0.1584,
      "step": 1863
    },
    {
      "epoch": 0.34844377979250396,
      "grad_norm": 1.0289959907531738,
      "learning_rate": 6.0710280373831777e-05,
      "loss": 0.1204,
      "step": 1864
    },
    {
      "epoch": 0.3486307131507618,
      "grad_norm": 3.2141661643981934,
      "learning_rate": 6.0635514018691595e-05,
      "loss": 0.2065,
      "step": 1865
    },
    {
      "epoch": 0.34881764650901953,
      "grad_norm": 1.0703150033950806,
      "learning_rate": 6.056074766355141e-05,
      "loss": 0.1057,
      "step": 1866
    },
    {
      "epoch": 0.34900457986727734,
      "grad_norm": 2.3961904048919678,
      "learning_rate": 6.048598130841122e-05,
      "loss": 0.2125,
      "step": 1867
    },
    {
      "epoch": 0.3491915132255351,
      "grad_norm": 6.311365604400635,
      "learning_rate": 6.041121495327103e-05,
      "loss": 0.3207,
      "step": 1868
    },
    {
      "epoch": 0.34937844658379286,
      "grad_norm": 1.8381900787353516,
      "learning_rate": 6.0336448598130844e-05,
      "loss": 0.1644,
      "step": 1869
    },
    {
      "epoch": 0.34956537994205067,
      "grad_norm": 2.973813772201538,
      "learning_rate": 6.0261682242990656e-05,
      "loss": 0.1262,
      "step": 1870
    },
    {
      "epoch": 0.3497523133003084,
      "grad_norm": 4.9787774085998535,
      "learning_rate": 6.018691588785047e-05,
      "loss": 0.1356,
      "step": 1871
    },
    {
      "epoch": 0.34993924665856624,
      "grad_norm": 2.1314783096313477,
      "learning_rate": 6.011214953271028e-05,
      "loss": 0.1584,
      "step": 1872
    },
    {
      "epoch": 0.350126180016824,
      "grad_norm": 1.7502939701080322,
      "learning_rate": 6.003738317757009e-05,
      "loss": 0.1503,
      "step": 1873
    },
    {
      "epoch": 0.3503131133750818,
      "grad_norm": 3.0635788440704346,
      "learning_rate": 5.9962616822429904e-05,
      "loss": 0.1915,
      "step": 1874
    },
    {
      "epoch": 0.35050004673333957,
      "grad_norm": 2.899103879928589,
      "learning_rate": 5.9887850467289716e-05,
      "loss": 0.3388,
      "step": 1875
    },
    {
      "epoch": 0.3506869800915973,
      "grad_norm": 1.4959685802459717,
      "learning_rate": 5.981308411214953e-05,
      "loss": 0.1589,
      "step": 1876
    },
    {
      "epoch": 0.35087391344985513,
      "grad_norm": 2.3409268856048584,
      "learning_rate": 5.973831775700934e-05,
      "loss": 0.2497,
      "step": 1877
    },
    {
      "epoch": 0.3510608468081129,
      "grad_norm": 2.284705400466919,
      "learning_rate": 5.9663551401869166e-05,
      "loss": 0.3132,
      "step": 1878
    },
    {
      "epoch": 0.3512477801663707,
      "grad_norm": 2.8007280826568604,
      "learning_rate": 5.958878504672898e-05,
      "loss": 0.2636,
      "step": 1879
    },
    {
      "epoch": 0.35143471352462846,
      "grad_norm": 3.9676525592803955,
      "learning_rate": 5.951401869158879e-05,
      "loss": 0.1428,
      "step": 1880
    },
    {
      "epoch": 0.3516216468828863,
      "grad_norm": 1.9259129762649536,
      "learning_rate": 5.94392523364486e-05,
      "loss": 0.2509,
      "step": 1881
    },
    {
      "epoch": 0.35180858024114403,
      "grad_norm": 2.5995635986328125,
      "learning_rate": 5.9364485981308414e-05,
      "loss": 0.2025,
      "step": 1882
    },
    {
      "epoch": 0.3519955135994018,
      "grad_norm": 1.145751953125,
      "learning_rate": 5.9289719626168227e-05,
      "loss": 0.1702,
      "step": 1883
    },
    {
      "epoch": 0.3521824469576596,
      "grad_norm": 0.9230062365531921,
      "learning_rate": 5.921495327102804e-05,
      "loss": 0.0567,
      "step": 1884
    },
    {
      "epoch": 0.35236938031591736,
      "grad_norm": 2.2422635555267334,
      "learning_rate": 5.914018691588785e-05,
      "loss": 0.3389,
      "step": 1885
    },
    {
      "epoch": 0.35255631367417517,
      "grad_norm": 4.0860161781311035,
      "learning_rate": 5.906542056074766e-05,
      "loss": 0.1053,
      "step": 1886
    },
    {
      "epoch": 0.3527432470324329,
      "grad_norm": 1.6393879652023315,
      "learning_rate": 5.8990654205607475e-05,
      "loss": 0.1264,
      "step": 1887
    },
    {
      "epoch": 0.35293018039069074,
      "grad_norm": 2.8622043132781982,
      "learning_rate": 5.891588785046729e-05,
      "loss": 0.2289,
      "step": 1888
    },
    {
      "epoch": 0.3531171137489485,
      "grad_norm": 4.773760795593262,
      "learning_rate": 5.88411214953271e-05,
      "loss": 0.1771,
      "step": 1889
    },
    {
      "epoch": 0.35330404710720625,
      "grad_norm": 1.5634419918060303,
      "learning_rate": 5.8766355140186925e-05,
      "loss": 0.1546,
      "step": 1890
    },
    {
      "epoch": 0.35349098046546407,
      "grad_norm": 4.3418779373168945,
      "learning_rate": 5.869158878504674e-05,
      "loss": 0.1753,
      "step": 1891
    },
    {
      "epoch": 0.3536779138237218,
      "grad_norm": 13.758840560913086,
      "learning_rate": 5.861682242990655e-05,
      "loss": 0.2095,
      "step": 1892
    },
    {
      "epoch": 0.35386484718197964,
      "grad_norm": 1.5697213411331177,
      "learning_rate": 5.854205607476636e-05,
      "loss": 0.1529,
      "step": 1893
    },
    {
      "epoch": 0.3540517805402374,
      "grad_norm": 0.7976945042610168,
      "learning_rate": 5.846728971962617e-05,
      "loss": 0.15,
      "step": 1894
    },
    {
      "epoch": 0.3542387138984952,
      "grad_norm": 2.6883187294006348,
      "learning_rate": 5.8392523364485985e-05,
      "loss": 0.2221,
      "step": 1895
    },
    {
      "epoch": 0.35442564725675296,
      "grad_norm": 1.5858469009399414,
      "learning_rate": 5.83177570093458e-05,
      "loss": 0.2639,
      "step": 1896
    },
    {
      "epoch": 0.3546125806150108,
      "grad_norm": 1.6611615419387817,
      "learning_rate": 5.824299065420561e-05,
      "loss": 0.168,
      "step": 1897
    },
    {
      "epoch": 0.35479951397326853,
      "grad_norm": 2.283783435821533,
      "learning_rate": 5.816822429906542e-05,
      "loss": 0.2373,
      "step": 1898
    },
    {
      "epoch": 0.3549864473315263,
      "grad_norm": 1.7776862382888794,
      "learning_rate": 5.8093457943925234e-05,
      "loss": 0.1221,
      "step": 1899
    },
    {
      "epoch": 0.3551733806897841,
      "grad_norm": 1.503739833831787,
      "learning_rate": 5.8018691588785046e-05,
      "loss": 0.2367,
      "step": 1900
    },
    {
      "epoch": 0.35536031404804186,
      "grad_norm": 1.9973021745681763,
      "learning_rate": 5.794392523364486e-05,
      "loss": 0.196,
      "step": 1901
    },
    {
      "epoch": 0.35554724740629967,
      "grad_norm": 1.2410327196121216,
      "learning_rate": 5.786915887850468e-05,
      "loss": 0.1214,
      "step": 1902
    },
    {
      "epoch": 0.35573418076455743,
      "grad_norm": 2.178187608718872,
      "learning_rate": 5.7794392523364495e-05,
      "loss": 0.1681,
      "step": 1903
    },
    {
      "epoch": 0.35592111412281524,
      "grad_norm": 4.801835060119629,
      "learning_rate": 5.771962616822431e-05,
      "loss": 0.383,
      "step": 1904
    },
    {
      "epoch": 0.356108047481073,
      "grad_norm": 3.5374200344085693,
      "learning_rate": 5.764485981308412e-05,
      "loss": 0.2179,
      "step": 1905
    },
    {
      "epoch": 0.35629498083933075,
      "grad_norm": 1.1948318481445312,
      "learning_rate": 5.757009345794393e-05,
      "loss": 0.1321,
      "step": 1906
    },
    {
      "epoch": 0.35648191419758857,
      "grad_norm": 2.256401300430298,
      "learning_rate": 5.7495327102803744e-05,
      "loss": 0.205,
      "step": 1907
    },
    {
      "epoch": 0.3566688475558463,
      "grad_norm": 0.6892870664596558,
      "learning_rate": 5.7420560747663556e-05,
      "loss": 0.0597,
      "step": 1908
    },
    {
      "epoch": 0.35685578091410414,
      "grad_norm": 1.6802856922149658,
      "learning_rate": 5.734579439252337e-05,
      "loss": 0.2058,
      "step": 1909
    },
    {
      "epoch": 0.3570427142723619,
      "grad_norm": 2.9774296283721924,
      "learning_rate": 5.727102803738318e-05,
      "loss": 0.1292,
      "step": 1910
    },
    {
      "epoch": 0.3572296476306197,
      "grad_norm": 2.993034839630127,
      "learning_rate": 5.719626168224299e-05,
      "loss": 0.1862,
      "step": 1911
    },
    {
      "epoch": 0.35741658098887746,
      "grad_norm": 1.2977875471115112,
      "learning_rate": 5.7121495327102804e-05,
      "loss": 0.1431,
      "step": 1912
    },
    {
      "epoch": 0.3576035143471352,
      "grad_norm": 1.1010867357254028,
      "learning_rate": 5.7046728971962616e-05,
      "loss": 0.2625,
      "step": 1913
    },
    {
      "epoch": 0.35779044770539303,
      "grad_norm": 3.809391975402832,
      "learning_rate": 5.697196261682243e-05,
      "loss": 0.2531,
      "step": 1914
    },
    {
      "epoch": 0.3579773810636508,
      "grad_norm": 1.1685106754302979,
      "learning_rate": 5.6897196261682254e-05,
      "loss": 0.2192,
      "step": 1915
    },
    {
      "epoch": 0.3581643144219086,
      "grad_norm": 1.4535084962844849,
      "learning_rate": 5.6822429906542066e-05,
      "loss": 0.0908,
      "step": 1916
    },
    {
      "epoch": 0.35835124778016636,
      "grad_norm": 2.5303549766540527,
      "learning_rate": 5.674766355140188e-05,
      "loss": 0.2274,
      "step": 1917
    },
    {
      "epoch": 0.3585381811384242,
      "grad_norm": 1.8978688716888428,
      "learning_rate": 5.667289719626169e-05,
      "loss": 0.1273,
      "step": 1918
    },
    {
      "epoch": 0.35872511449668193,
      "grad_norm": 1.4700242280960083,
      "learning_rate": 5.65981308411215e-05,
      "loss": 0.1696,
      "step": 1919
    },
    {
      "epoch": 0.3589120478549397,
      "grad_norm": 1.3884670734405518,
      "learning_rate": 5.6523364485981315e-05,
      "loss": 0.1395,
      "step": 1920
    },
    {
      "epoch": 0.3590989812131975,
      "grad_norm": 2.8746683597564697,
      "learning_rate": 5.644859813084113e-05,
      "loss": 0.1555,
      "step": 1921
    },
    {
      "epoch": 0.35928591457145526,
      "grad_norm": 2.2222225666046143,
      "learning_rate": 5.637383177570094e-05,
      "loss": 0.1986,
      "step": 1922
    },
    {
      "epoch": 0.35947284792971307,
      "grad_norm": 1.157899260520935,
      "learning_rate": 5.629906542056075e-05,
      "loss": 0.1186,
      "step": 1923
    },
    {
      "epoch": 0.3596597812879708,
      "grad_norm": 1.3087804317474365,
      "learning_rate": 5.622429906542056e-05,
      "loss": 0.2383,
      "step": 1924
    },
    {
      "epoch": 0.35984671464622864,
      "grad_norm": 1.8120895624160767,
      "learning_rate": 5.6149532710280375e-05,
      "loss": 0.2218,
      "step": 1925
    },
    {
      "epoch": 0.3600336480044864,
      "grad_norm": 1.033848524093628,
      "learning_rate": 5.607476635514019e-05,
      "loss": 0.0956,
      "step": 1926
    },
    {
      "epoch": 0.3602205813627442,
      "grad_norm": 1.4356162548065186,
      "learning_rate": 5.6000000000000006e-05,
      "loss": 0.1925,
      "step": 1927
    },
    {
      "epoch": 0.36040751472100196,
      "grad_norm": 2.581167221069336,
      "learning_rate": 5.592523364485982e-05,
      "loss": 0.2533,
      "step": 1928
    },
    {
      "epoch": 0.3605944480792597,
      "grad_norm": 1.3458638191223145,
      "learning_rate": 5.585046728971963e-05,
      "loss": 0.227,
      "step": 1929
    },
    {
      "epoch": 0.36078138143751753,
      "grad_norm": 2.941318988800049,
      "learning_rate": 5.577570093457944e-05,
      "loss": 0.1787,
      "step": 1930
    },
    {
      "epoch": 0.3609683147957753,
      "grad_norm": 3.808669328689575,
      "learning_rate": 5.5700934579439254e-05,
      "loss": 0.1592,
      "step": 1931
    },
    {
      "epoch": 0.3611552481540331,
      "grad_norm": 1.1244796514511108,
      "learning_rate": 5.5626168224299066e-05,
      "loss": 0.1441,
      "step": 1932
    },
    {
      "epoch": 0.36134218151229086,
      "grad_norm": 3.201218366622925,
      "learning_rate": 5.555140186915888e-05,
      "loss": 0.2676,
      "step": 1933
    },
    {
      "epoch": 0.3615291148705487,
      "grad_norm": 1.172528862953186,
      "learning_rate": 5.547663551401869e-05,
      "loss": 0.1139,
      "step": 1934
    },
    {
      "epoch": 0.36171604822880643,
      "grad_norm": 1.2498183250427246,
      "learning_rate": 5.54018691588785e-05,
      "loss": 0.1934,
      "step": 1935
    },
    {
      "epoch": 0.3619029815870642,
      "grad_norm": 2.262589693069458,
      "learning_rate": 5.5327102803738315e-05,
      "loss": 0.1964,
      "step": 1936
    },
    {
      "epoch": 0.362089914945322,
      "grad_norm": 1.8079249858856201,
      "learning_rate": 5.525233644859813e-05,
      "loss": 0.1619,
      "step": 1937
    },
    {
      "epoch": 0.36227684830357976,
      "grad_norm": 1.8167718648910522,
      "learning_rate": 5.517757009345794e-05,
      "loss": 0.1468,
      "step": 1938
    },
    {
      "epoch": 0.36246378166183757,
      "grad_norm": 1.6234599351882935,
      "learning_rate": 5.510280373831775e-05,
      "loss": 0.255,
      "step": 1939
    },
    {
      "epoch": 0.3626507150200953,
      "grad_norm": 1.9012525081634521,
      "learning_rate": 5.502803738317758e-05,
      "loss": 0.1876,
      "step": 1940
    },
    {
      "epoch": 0.36283764837835314,
      "grad_norm": 1.8840343952178955,
      "learning_rate": 5.495327102803739e-05,
      "loss": 0.1972,
      "step": 1941
    },
    {
      "epoch": 0.3630245817366109,
      "grad_norm": 1.0620238780975342,
      "learning_rate": 5.48785046728972e-05,
      "loss": 0.1608,
      "step": 1942
    },
    {
      "epoch": 0.36321151509486865,
      "grad_norm": 3.5351524353027344,
      "learning_rate": 5.480373831775701e-05,
      "loss": 0.2939,
      "step": 1943
    },
    {
      "epoch": 0.36339844845312647,
      "grad_norm": 1.7706042528152466,
      "learning_rate": 5.4728971962616825e-05,
      "loss": 0.1465,
      "step": 1944
    },
    {
      "epoch": 0.3635853818113842,
      "grad_norm": 1.3565653562545776,
      "learning_rate": 5.465420560747664e-05,
      "loss": 0.1556,
      "step": 1945
    },
    {
      "epoch": 0.36377231516964204,
      "grad_norm": 1.5161898136138916,
      "learning_rate": 5.457943925233645e-05,
      "loss": 0.1724,
      "step": 1946
    },
    {
      "epoch": 0.3639592485278998,
      "grad_norm": 2.289518117904663,
      "learning_rate": 5.450467289719626e-05,
      "loss": 0.19,
      "step": 1947
    },
    {
      "epoch": 0.3641461818861576,
      "grad_norm": 0.8297168016433716,
      "learning_rate": 5.442990654205607e-05,
      "loss": 0.0988,
      "step": 1948
    },
    {
      "epoch": 0.36433311524441536,
      "grad_norm": 0.792608380317688,
      "learning_rate": 5.4355140186915885e-05,
      "loss": 0.0912,
      "step": 1949
    },
    {
      "epoch": 0.3645200486026732,
      "grad_norm": 11.759136199951172,
      "learning_rate": 5.42803738317757e-05,
      "loss": 0.2195,
      "step": 1950
    },
    {
      "epoch": 0.36470698196093093,
      "grad_norm": 2.089341640472412,
      "learning_rate": 5.420560747663551e-05,
      "loss": 0.2321,
      "step": 1951
    },
    {
      "epoch": 0.3648939153191887,
      "grad_norm": 2.9003489017486572,
      "learning_rate": 5.4130841121495335e-05,
      "loss": 0.1744,
      "step": 1952
    },
    {
      "epoch": 0.3650808486774465,
      "grad_norm": 3.4666500091552734,
      "learning_rate": 5.405607476635515e-05,
      "loss": 0.1452,
      "step": 1953
    },
    {
      "epoch": 0.36526778203570426,
      "grad_norm": 1.475853681564331,
      "learning_rate": 5.398130841121496e-05,
      "loss": 0.1461,
      "step": 1954
    },
    {
      "epoch": 0.36545471539396207,
      "grad_norm": 3.546793222427368,
      "learning_rate": 5.390654205607477e-05,
      "loss": 0.1156,
      "step": 1955
    },
    {
      "epoch": 0.36564164875221983,
      "grad_norm": 1.788572072982788,
      "learning_rate": 5.3831775700934584e-05,
      "loss": 0.0986,
      "step": 1956
    },
    {
      "epoch": 0.36582858211047764,
      "grad_norm": 1.8576459884643555,
      "learning_rate": 5.3757009345794396e-05,
      "loss": 0.279,
      "step": 1957
    },
    {
      "epoch": 0.3660155154687354,
      "grad_norm": 1.9125862121582031,
      "learning_rate": 5.368224299065421e-05,
      "loss": 0.1833,
      "step": 1958
    },
    {
      "epoch": 0.36620244882699315,
      "grad_norm": 2.6338860988616943,
      "learning_rate": 5.360747663551402e-05,
      "loss": 0.1757,
      "step": 1959
    },
    {
      "epoch": 0.36638938218525097,
      "grad_norm": 1.6990023851394653,
      "learning_rate": 5.353271028037383e-05,
      "loss": 0.1539,
      "step": 1960
    },
    {
      "epoch": 0.3665763155435087,
      "grad_norm": 1.7579481601715088,
      "learning_rate": 5.3457943925233644e-05,
      "loss": 0.1827,
      "step": 1961
    },
    {
      "epoch": 0.36676324890176654,
      "grad_norm": 2.3138771057128906,
      "learning_rate": 5.3383177570093456e-05,
      "loss": 0.1124,
      "step": 1962
    },
    {
      "epoch": 0.3669501822600243,
      "grad_norm": 0.8410988450050354,
      "learning_rate": 5.330841121495327e-05,
      "loss": 0.1234,
      "step": 1963
    },
    {
      "epoch": 0.3671371156182821,
      "grad_norm": 0.9839915633201599,
      "learning_rate": 5.3233644859813094e-05,
      "loss": 0.0908,
      "step": 1964
    },
    {
      "epoch": 0.36732404897653986,
      "grad_norm": 1.4751064777374268,
      "learning_rate": 5.3158878504672906e-05,
      "loss": 0.1821,
      "step": 1965
    },
    {
      "epoch": 0.3675109823347976,
      "grad_norm": 1.6820279359817505,
      "learning_rate": 5.308411214953272e-05,
      "loss": 0.1889,
      "step": 1966
    },
    {
      "epoch": 0.36769791569305543,
      "grad_norm": 0.9755252003669739,
      "learning_rate": 5.300934579439253e-05,
      "loss": 0.1407,
      "step": 1967
    },
    {
      "epoch": 0.3678848490513132,
      "grad_norm": 5.480130195617676,
      "learning_rate": 5.293457943925234e-05,
      "loss": 0.3247,
      "step": 1968
    },
    {
      "epoch": 0.368071782409571,
      "grad_norm": 0.6571348905563354,
      "learning_rate": 5.2859813084112154e-05,
      "loss": 0.0719,
      "step": 1969
    },
    {
      "epoch": 0.36825871576782876,
      "grad_norm": 1.2974051237106323,
      "learning_rate": 5.2785046728971966e-05,
      "loss": 0.1668,
      "step": 1970
    },
    {
      "epoch": 0.36844564912608657,
      "grad_norm": 1.0672272443771362,
      "learning_rate": 5.271028037383178e-05,
      "loss": 0.0923,
      "step": 1971
    },
    {
      "epoch": 0.36863258248434433,
      "grad_norm": 1.8305858373641968,
      "learning_rate": 5.263551401869159e-05,
      "loss": 0.2095,
      "step": 1972
    },
    {
      "epoch": 0.3688195158426021,
      "grad_norm": 4.150510787963867,
      "learning_rate": 5.25607476635514e-05,
      "loss": 0.2796,
      "step": 1973
    },
    {
      "epoch": 0.3690064492008599,
      "grad_norm": 3.366368293762207,
      "learning_rate": 5.2485981308411215e-05,
      "loss": 0.144,
      "step": 1974
    },
    {
      "epoch": 0.36919338255911766,
      "grad_norm": 3.9932351112365723,
      "learning_rate": 5.241121495327103e-05,
      "loss": 0.1167,
      "step": 1975
    },
    {
      "epoch": 0.36938031591737547,
      "grad_norm": 1.4989616870880127,
      "learning_rate": 5.233644859813084e-05,
      "loss": 0.2183,
      "step": 1976
    },
    {
      "epoch": 0.3695672492756332,
      "grad_norm": 1.0016645193099976,
      "learning_rate": 5.2261682242990665e-05,
      "loss": 0.1163,
      "step": 1977
    },
    {
      "epoch": 0.36975418263389104,
      "grad_norm": 3.100950002670288,
      "learning_rate": 5.218691588785048e-05,
      "loss": 0.165,
      "step": 1978
    },
    {
      "epoch": 0.3699411159921488,
      "grad_norm": 1.621944546699524,
      "learning_rate": 5.211214953271029e-05,
      "loss": 0.1051,
      "step": 1979
    },
    {
      "epoch": 0.3701280493504066,
      "grad_norm": 2.258183240890503,
      "learning_rate": 5.20373831775701e-05,
      "loss": 0.2387,
      "step": 1980
    },
    {
      "epoch": 0.37031498270866436,
      "grad_norm": 0.994550347328186,
      "learning_rate": 5.196261682242991e-05,
      "loss": 0.0899,
      "step": 1981
    },
    {
      "epoch": 0.3705019160669221,
      "grad_norm": 1.5371216535568237,
      "learning_rate": 5.1887850467289725e-05,
      "loss": 0.0888,
      "step": 1982
    },
    {
      "epoch": 0.37068884942517993,
      "grad_norm": 1.3701647520065308,
      "learning_rate": 5.181308411214954e-05,
      "loss": 0.1357,
      "step": 1983
    },
    {
      "epoch": 0.3708757827834377,
      "grad_norm": 4.6361165046691895,
      "learning_rate": 5.173831775700935e-05,
      "loss": 0.3129,
      "step": 1984
    },
    {
      "epoch": 0.3710627161416955,
      "grad_norm": 1.922060251235962,
      "learning_rate": 5.166355140186916e-05,
      "loss": 0.1246,
      "step": 1985
    },
    {
      "epoch": 0.37124964949995326,
      "grad_norm": 1.6612234115600586,
      "learning_rate": 5.1588785046728973e-05,
      "loss": 0.102,
      "step": 1986
    },
    {
      "epoch": 0.3714365828582111,
      "grad_norm": 2.870701313018799,
      "learning_rate": 5.1514018691588786e-05,
      "loss": 0.2189,
      "step": 1987
    },
    {
      "epoch": 0.37162351621646883,
      "grad_norm": 2.894726276397705,
      "learning_rate": 5.14392523364486e-05,
      "loss": 0.1307,
      "step": 1988
    },
    {
      "epoch": 0.3718104495747266,
      "grad_norm": 3.5179738998413086,
      "learning_rate": 5.1364485981308417e-05,
      "loss": 0.1711,
      "step": 1989
    },
    {
      "epoch": 0.3719973829329844,
      "grad_norm": 2.905625820159912,
      "learning_rate": 5.128971962616823e-05,
      "loss": 0.0799,
      "step": 1990
    },
    {
      "epoch": 0.37218431629124216,
      "grad_norm": 1.6861298084259033,
      "learning_rate": 5.121495327102804e-05,
      "loss": 0.2032,
      "step": 1991
    },
    {
      "epoch": 0.37237124964949997,
      "grad_norm": 4.05501651763916,
      "learning_rate": 5.114018691588785e-05,
      "loss": 0.2359,
      "step": 1992
    },
    {
      "epoch": 0.3725581830077577,
      "grad_norm": 3.511331558227539,
      "learning_rate": 5.1065420560747665e-05,
      "loss": 0.1454,
      "step": 1993
    },
    {
      "epoch": 0.37274511636601554,
      "grad_norm": 1.675461769104004,
      "learning_rate": 5.099065420560748e-05,
      "loss": 0.1908,
      "step": 1994
    },
    {
      "epoch": 0.3729320497242733,
      "grad_norm": 2.5978527069091797,
      "learning_rate": 5.091588785046729e-05,
      "loss": 0.1641,
      "step": 1995
    },
    {
      "epoch": 0.37311898308253105,
      "grad_norm": 1.3513866662979126,
      "learning_rate": 5.08411214953271e-05,
      "loss": 0.1196,
      "step": 1996
    },
    {
      "epoch": 0.37330591644078887,
      "grad_norm": 2.0508532524108887,
      "learning_rate": 5.076635514018691e-05,
      "loss": 0.1845,
      "step": 1997
    },
    {
      "epoch": 0.3734928497990466,
      "grad_norm": 1.156821608543396,
      "learning_rate": 5.0691588785046725e-05,
      "loss": 0.2442,
      "step": 1998
    },
    {
      "epoch": 0.37367978315730443,
      "grad_norm": 1.7055126428604126,
      "learning_rate": 5.061682242990654e-05,
      "loss": 0.2466,
      "step": 1999
    },
    {
      "epoch": 0.3738667165155622,
      "grad_norm": 3.945328712463379,
      "learning_rate": 5.0542056074766356e-05,
      "loss": 0.2782,
      "step": 2000
    },
    {
      "epoch": 0.37405364987382,
      "grad_norm": 1.8464947938919067,
      "learning_rate": 5.046728971962617e-05,
      "loss": 0.2145,
      "step": 2001
    },
    {
      "epoch": 0.37424058323207776,
      "grad_norm": 2.8887112140655518,
      "learning_rate": 5.039252336448599e-05,
      "loss": 0.2082,
      "step": 2002
    },
    {
      "epoch": 0.3744275165903355,
      "grad_norm": 2.2860300540924072,
      "learning_rate": 5.03177570093458e-05,
      "loss": 0.1567,
      "step": 2003
    },
    {
      "epoch": 0.37461444994859333,
      "grad_norm": 1.776275396347046,
      "learning_rate": 5.024299065420561e-05,
      "loss": 0.212,
      "step": 2004
    },
    {
      "epoch": 0.3748013833068511,
      "grad_norm": 1.009508490562439,
      "learning_rate": 5.0168224299065423e-05,
      "loss": 0.1491,
      "step": 2005
    },
    {
      "epoch": 0.3749883166651089,
      "grad_norm": 1.9153306484222412,
      "learning_rate": 5.0093457943925236e-05,
      "loss": 0.1496,
      "step": 2006
    },
    {
      "epoch": 0.37517525002336666,
      "grad_norm": 3.761408805847168,
      "learning_rate": 5.001869158878505e-05,
      "loss": 0.2083,
      "step": 2007
    },
    {
      "epoch": 0.37536218338162447,
      "grad_norm": 0.9195727705955505,
      "learning_rate": 4.994392523364486e-05,
      "loss": 0.0836,
      "step": 2008
    },
    {
      "epoch": 0.3755491167398822,
      "grad_norm": 0.9882425665855408,
      "learning_rate": 4.986915887850467e-05,
      "loss": 0.1219,
      "step": 2009
    },
    {
      "epoch": 0.37573605009814004,
      "grad_norm": 1.3558251857757568,
      "learning_rate": 4.9794392523364484e-05,
      "loss": 0.2082,
      "step": 2010
    },
    {
      "epoch": 0.3759229834563978,
      "grad_norm": 18.793941497802734,
      "learning_rate": 4.97196261682243e-05,
      "loss": 0.242,
      "step": 2011
    },
    {
      "epoch": 0.37610991681465555,
      "grad_norm": 4.334590435028076,
      "learning_rate": 4.9644859813084115e-05,
      "loss": 0.3268,
      "step": 2012
    },
    {
      "epoch": 0.37629685017291337,
      "grad_norm": 1.4744794368743896,
      "learning_rate": 4.957009345794393e-05,
      "loss": 0.1366,
      "step": 2013
    },
    {
      "epoch": 0.3764837835311711,
      "grad_norm": 1.9813227653503418,
      "learning_rate": 4.949532710280374e-05,
      "loss": 0.1696,
      "step": 2014
    },
    {
      "epoch": 0.37667071688942894,
      "grad_norm": 1.8974682092666626,
      "learning_rate": 4.942056074766355e-05,
      "loss": 0.1641,
      "step": 2015
    },
    {
      "epoch": 0.3768576502476867,
      "grad_norm": 2.1010401248931885,
      "learning_rate": 4.934579439252336e-05,
      "loss": 0.2234,
      "step": 2016
    },
    {
      "epoch": 0.3770445836059445,
      "grad_norm": 2.3715927600860596,
      "learning_rate": 4.927102803738318e-05,
      "loss": 0.1831,
      "step": 2017
    },
    {
      "epoch": 0.37723151696420226,
      "grad_norm": 2.3017356395721436,
      "learning_rate": 4.9196261682242994e-05,
      "loss": 0.1864,
      "step": 2018
    },
    {
      "epoch": 0.37741845032246,
      "grad_norm": 1.6605457067489624,
      "learning_rate": 4.9121495327102806e-05,
      "loss": 0.2064,
      "step": 2019
    },
    {
      "epoch": 0.37760538368071783,
      "grad_norm": 0.9790886044502258,
      "learning_rate": 4.904672897196262e-05,
      "loss": 0.0951,
      "step": 2020
    },
    {
      "epoch": 0.3777923170389756,
      "grad_norm": 2.649310827255249,
      "learning_rate": 4.897196261682243e-05,
      "loss": 0.2366,
      "step": 2021
    },
    {
      "epoch": 0.3779792503972334,
      "grad_norm": 1.1865590810775757,
      "learning_rate": 4.889719626168224e-05,
      "loss": 0.0821,
      "step": 2022
    },
    {
      "epoch": 0.37816618375549116,
      "grad_norm": 1.5968559980392456,
      "learning_rate": 4.882242990654206e-05,
      "loss": 0.1545,
      "step": 2023
    },
    {
      "epoch": 0.37835311711374897,
      "grad_norm": 1.6963433027267456,
      "learning_rate": 4.8747663551401874e-05,
      "loss": 0.2143,
      "step": 2024
    },
    {
      "epoch": 0.37854005047200673,
      "grad_norm": 1.4460835456848145,
      "learning_rate": 4.8672897196261686e-05,
      "loss": 0.1807,
      "step": 2025
    },
    {
      "epoch": 0.3787269838302645,
      "grad_norm": 1.8918685913085938,
      "learning_rate": 4.85981308411215e-05,
      "loss": 0.2353,
      "step": 2026
    },
    {
      "epoch": 0.3789139171885223,
      "grad_norm": 1.1074252128601074,
      "learning_rate": 4.852336448598131e-05,
      "loss": 0.1087,
      "step": 2027
    },
    {
      "epoch": 0.37910085054678005,
      "grad_norm": 1.7372217178344727,
      "learning_rate": 4.844859813084112e-05,
      "loss": 0.1807,
      "step": 2028
    },
    {
      "epoch": 0.37928778390503787,
      "grad_norm": 1.5375478267669678,
      "learning_rate": 4.837383177570094e-05,
      "loss": 0.1534,
      "step": 2029
    },
    {
      "epoch": 0.3794747172632956,
      "grad_norm": 0.8645308017730713,
      "learning_rate": 4.829906542056075e-05,
      "loss": 0.134,
      "step": 2030
    },
    {
      "epoch": 0.37966165062155344,
      "grad_norm": 0.8619292378425598,
      "learning_rate": 4.8224299065420565e-05,
      "loss": 0.0774,
      "step": 2031
    },
    {
      "epoch": 0.3798485839798112,
      "grad_norm": 1.0535905361175537,
      "learning_rate": 4.814953271028038e-05,
      "loss": 0.1431,
      "step": 2032
    },
    {
      "epoch": 0.38003551733806895,
      "grad_norm": 1.4208340644836426,
      "learning_rate": 4.807476635514019e-05,
      "loss": 0.1972,
      "step": 2033
    },
    {
      "epoch": 0.38022245069632676,
      "grad_norm": 2.745917558670044,
      "learning_rate": 4.8e-05,
      "loss": 0.1352,
      "step": 2034
    },
    {
      "epoch": 0.3804093840545845,
      "grad_norm": 1.7146964073181152,
      "learning_rate": 4.792523364485981e-05,
      "loss": 0.2921,
      "step": 2035
    },
    {
      "epoch": 0.38059631741284233,
      "grad_norm": 2.095939874649048,
      "learning_rate": 4.785046728971963e-05,
      "loss": 0.1509,
      "step": 2036
    },
    {
      "epoch": 0.3807832507711001,
      "grad_norm": 1.3888505697250366,
      "learning_rate": 4.7775700934579444e-05,
      "loss": 0.1847,
      "step": 2037
    },
    {
      "epoch": 0.3809701841293579,
      "grad_norm": 2.6550087928771973,
      "learning_rate": 4.7700934579439256e-05,
      "loss": 0.2197,
      "step": 2038
    },
    {
      "epoch": 0.38115711748761566,
      "grad_norm": 4.521394729614258,
      "learning_rate": 4.762616822429907e-05,
      "loss": 0.1984,
      "step": 2039
    },
    {
      "epoch": 0.3813440508458735,
      "grad_norm": 1.889620065689087,
      "learning_rate": 4.755140186915888e-05,
      "loss": 0.0912,
      "step": 2040
    },
    {
      "epoch": 0.38153098420413123,
      "grad_norm": 0.8323513865470886,
      "learning_rate": 4.747663551401869e-05,
      "loss": 0.1404,
      "step": 2041
    },
    {
      "epoch": 0.381717917562389,
      "grad_norm": 1.851271390914917,
      "learning_rate": 4.740186915887851e-05,
      "loss": 0.2809,
      "step": 2042
    },
    {
      "epoch": 0.3819048509206468,
      "grad_norm": 1.0661616325378418,
      "learning_rate": 4.7327102803738324e-05,
      "loss": 0.1253,
      "step": 2043
    },
    {
      "epoch": 0.38209178427890456,
      "grad_norm": 3.623619794845581,
      "learning_rate": 4.7252336448598136e-05,
      "loss": 0.1194,
      "step": 2044
    },
    {
      "epoch": 0.38227871763716237,
      "grad_norm": 1.6462209224700928,
      "learning_rate": 4.717757009345795e-05,
      "loss": 0.1548,
      "step": 2045
    },
    {
      "epoch": 0.3824656509954201,
      "grad_norm": 2.021312713623047,
      "learning_rate": 4.710280373831776e-05,
      "loss": 0.2118,
      "step": 2046
    },
    {
      "epoch": 0.38265258435367794,
      "grad_norm": 1.1152355670928955,
      "learning_rate": 4.702803738317757e-05,
      "loss": 0.107,
      "step": 2047
    },
    {
      "epoch": 0.3828395177119357,
      "grad_norm": 2.175663709640503,
      "learning_rate": 4.695327102803739e-05,
      "loss": 0.2087,
      "step": 2048
    },
    {
      "epoch": 0.38302645107019345,
      "grad_norm": 1.2860068082809448,
      "learning_rate": 4.68785046728972e-05,
      "loss": 0.1283,
      "step": 2049
    },
    {
      "epoch": 0.38321338442845126,
      "grad_norm": 1.3614803552627563,
      "learning_rate": 4.6803738317757015e-05,
      "loss": 0.1492,
      "step": 2050
    },
    {
      "epoch": 0.383400317786709,
      "grad_norm": 2.4522855281829834,
      "learning_rate": 4.672897196261683e-05,
      "loss": 0.2802,
      "step": 2051
    },
    {
      "epoch": 0.38358725114496683,
      "grad_norm": 4.062304973602295,
      "learning_rate": 4.665420560747664e-05,
      "loss": 0.3547,
      "step": 2052
    },
    {
      "epoch": 0.3837741845032246,
      "grad_norm": 2.1341726779937744,
      "learning_rate": 4.657943925233645e-05,
      "loss": 0.1559,
      "step": 2053
    },
    {
      "epoch": 0.3839611178614824,
      "grad_norm": 1.2890005111694336,
      "learning_rate": 4.650467289719626e-05,
      "loss": 0.1223,
      "step": 2054
    },
    {
      "epoch": 0.38414805121974016,
      "grad_norm": 0.9271180629730225,
      "learning_rate": 4.6429906542056075e-05,
      "loss": 0.068,
      "step": 2055
    },
    {
      "epoch": 0.3843349845779979,
      "grad_norm": 0.986538827419281,
      "learning_rate": 4.635514018691589e-05,
      "loss": 0.0641,
      "step": 2056
    },
    {
      "epoch": 0.38452191793625573,
      "grad_norm": 1.2437636852264404,
      "learning_rate": 4.62803738317757e-05,
      "loss": 0.1512,
      "step": 2057
    },
    {
      "epoch": 0.3847088512945135,
      "grad_norm": 1.437434196472168,
      "learning_rate": 4.620560747663552e-05,
      "loss": 0.1972,
      "step": 2058
    },
    {
      "epoch": 0.3848957846527713,
      "grad_norm": 1.4152849912643433,
      "learning_rate": 4.613084112149533e-05,
      "loss": 0.0543,
      "step": 2059
    },
    {
      "epoch": 0.38508271801102906,
      "grad_norm": 2.8199470043182373,
      "learning_rate": 4.605607476635514e-05,
      "loss": 0.2444,
      "step": 2060
    },
    {
      "epoch": 0.38526965136928687,
      "grad_norm": 1.3571279048919678,
      "learning_rate": 4.5981308411214955e-05,
      "loss": 0.1206,
      "step": 2061
    },
    {
      "epoch": 0.3854565847275446,
      "grad_norm": 0.9609629511833191,
      "learning_rate": 4.590654205607477e-05,
      "loss": 0.1062,
      "step": 2062
    },
    {
      "epoch": 0.38564351808580244,
      "grad_norm": 2.0389277935028076,
      "learning_rate": 4.583177570093458e-05,
      "loss": 0.1175,
      "step": 2063
    },
    {
      "epoch": 0.3858304514440602,
      "grad_norm": 1.1268439292907715,
      "learning_rate": 4.575700934579439e-05,
      "loss": 0.1006,
      "step": 2064
    },
    {
      "epoch": 0.38601738480231795,
      "grad_norm": 1.4686051607131958,
      "learning_rate": 4.56822429906542e-05,
      "loss": 0.1245,
      "step": 2065
    },
    {
      "epoch": 0.38620431816057577,
      "grad_norm": 7.729521751403809,
      "learning_rate": 4.5607476635514015e-05,
      "loss": 0.2964,
      "step": 2066
    },
    {
      "epoch": 0.3863912515188335,
      "grad_norm": 3.3924765586853027,
      "learning_rate": 4.5532710280373834e-05,
      "loss": 0.2531,
      "step": 2067
    },
    {
      "epoch": 0.38657818487709134,
      "grad_norm": 2.8551290035247803,
      "learning_rate": 4.5457943925233646e-05,
      "loss": 0.2362,
      "step": 2068
    },
    {
      "epoch": 0.3867651182353491,
      "grad_norm": 1.5653281211853027,
      "learning_rate": 4.538317757009346e-05,
      "loss": 0.1937,
      "step": 2069
    },
    {
      "epoch": 0.3869520515936069,
      "grad_norm": 1.473628282546997,
      "learning_rate": 4.530841121495327e-05,
      "loss": 0.1311,
      "step": 2070
    },
    {
      "epoch": 0.38713898495186466,
      "grad_norm": 2.2177300453186035,
      "learning_rate": 4.523364485981308e-05,
      "loss": 0.2872,
      "step": 2071
    },
    {
      "epoch": 0.3873259183101224,
      "grad_norm": 1.3585307598114014,
      "learning_rate": 4.5158878504672895e-05,
      "loss": 0.1273,
      "step": 2072
    },
    {
      "epoch": 0.38751285166838023,
      "grad_norm": 2.560126304626465,
      "learning_rate": 4.508411214953271e-05,
      "loss": 0.3158,
      "step": 2073
    },
    {
      "epoch": 0.387699785026638,
      "grad_norm": 1.9087772369384766,
      "learning_rate": 4.5009345794392525e-05,
      "loss": 0.1798,
      "step": 2074
    },
    {
      "epoch": 0.3878867183848958,
      "grad_norm": 1.166442632675171,
      "learning_rate": 4.493457943925234e-05,
      "loss": 0.1724,
      "step": 2075
    },
    {
      "epoch": 0.38807365174315356,
      "grad_norm": 1.2021610736846924,
      "learning_rate": 4.485981308411215e-05,
      "loss": 0.1059,
      "step": 2076
    },
    {
      "epoch": 0.38826058510141137,
      "grad_norm": 1.8757562637329102,
      "learning_rate": 4.478504672897196e-05,
      "loss": 0.1992,
      "step": 2077
    },
    {
      "epoch": 0.3884475184596691,
      "grad_norm": 1.2008247375488281,
      "learning_rate": 4.4710280373831774e-05,
      "loss": 0.1745,
      "step": 2078
    },
    {
      "epoch": 0.3886344518179269,
      "grad_norm": 2.1542775630950928,
      "learning_rate": 4.463551401869159e-05,
      "loss": 0.2633,
      "step": 2079
    },
    {
      "epoch": 0.3888213851761847,
      "grad_norm": 1.0885236263275146,
      "learning_rate": 4.4560747663551405e-05,
      "loss": 0.1278,
      "step": 2080
    },
    {
      "epoch": 0.38900831853444245,
      "grad_norm": 1.4106693267822266,
      "learning_rate": 4.448598130841122e-05,
      "loss": 0.1668,
      "step": 2081
    },
    {
      "epoch": 0.38919525189270027,
      "grad_norm": 2.551774501800537,
      "learning_rate": 4.441121495327103e-05,
      "loss": 0.1755,
      "step": 2082
    },
    {
      "epoch": 0.389382185250958,
      "grad_norm": 1.2664978504180908,
      "learning_rate": 4.433644859813084e-05,
      "loss": 0.0473,
      "step": 2083
    },
    {
      "epoch": 0.38956911860921584,
      "grad_norm": 1.1379176378250122,
      "learning_rate": 4.426168224299065e-05,
      "loss": 0.1366,
      "step": 2084
    },
    {
      "epoch": 0.3897560519674736,
      "grad_norm": 4.833642482757568,
      "learning_rate": 4.418691588785047e-05,
      "loss": 0.2479,
      "step": 2085
    },
    {
      "epoch": 0.38994298532573135,
      "grad_norm": 2.5865983963012695,
      "learning_rate": 4.4112149532710284e-05,
      "loss": 0.2691,
      "step": 2086
    },
    {
      "epoch": 0.39012991868398916,
      "grad_norm": 1.737505316734314,
      "learning_rate": 4.4037383177570096e-05,
      "loss": 0.1856,
      "step": 2087
    },
    {
      "epoch": 0.3903168520422469,
      "grad_norm": 1.7274216413497925,
      "learning_rate": 4.396261682242991e-05,
      "loss": 0.1482,
      "step": 2088
    },
    {
      "epoch": 0.39050378540050473,
      "grad_norm": 1.635389804840088,
      "learning_rate": 4.388785046728972e-05,
      "loss": 0.2409,
      "step": 2089
    },
    {
      "epoch": 0.3906907187587625,
      "grad_norm": 2.1598856449127197,
      "learning_rate": 4.381308411214953e-05,
      "loss": 0.1133,
      "step": 2090
    },
    {
      "epoch": 0.3908776521170203,
      "grad_norm": 1.962560772895813,
      "learning_rate": 4.373831775700935e-05,
      "loss": 0.2761,
      "step": 2091
    },
    {
      "epoch": 0.39106458547527806,
      "grad_norm": 1.5926374197006226,
      "learning_rate": 4.3663551401869163e-05,
      "loss": 0.2483,
      "step": 2092
    },
    {
      "epoch": 0.39125151883353587,
      "grad_norm": 3.1715734004974365,
      "learning_rate": 4.3588785046728976e-05,
      "loss": 0.1108,
      "step": 2093
    },
    {
      "epoch": 0.39143845219179363,
      "grad_norm": 1.5834566354751587,
      "learning_rate": 4.351401869158879e-05,
      "loss": 0.2736,
      "step": 2094
    },
    {
      "epoch": 0.3916253855500514,
      "grad_norm": 1.1654244661331177,
      "learning_rate": 4.34392523364486e-05,
      "loss": 0.0958,
      "step": 2095
    },
    {
      "epoch": 0.3918123189083092,
      "grad_norm": 1.5601146221160889,
      "learning_rate": 4.336448598130841e-05,
      "loss": 0.2417,
      "step": 2096
    },
    {
      "epoch": 0.39199925226656696,
      "grad_norm": 1.2886000871658325,
      "learning_rate": 4.3289719626168224e-05,
      "loss": 0.1402,
      "step": 2097
    },
    {
      "epoch": 0.39218618562482477,
      "grad_norm": 1.0626987218856812,
      "learning_rate": 4.321495327102804e-05,
      "loss": 0.1189,
      "step": 2098
    },
    {
      "epoch": 0.3923731189830825,
      "grad_norm": 1.734115719795227,
      "learning_rate": 4.3140186915887855e-05,
      "loss": 0.2132,
      "step": 2099
    },
    {
      "epoch": 0.39256005234134034,
      "grad_norm": 1.994665265083313,
      "learning_rate": 4.306542056074767e-05,
      "loss": 0.1489,
      "step": 2100
    },
    {
      "epoch": 0.3927469856995981,
      "grad_norm": 1.8589123487472534,
      "learning_rate": 4.299065420560748e-05,
      "loss": 0.1631,
      "step": 2101
    },
    {
      "epoch": 0.39293391905785585,
      "grad_norm": 1.5772470235824585,
      "learning_rate": 4.291588785046729e-05,
      "loss": 0.1685,
      "step": 2102
    },
    {
      "epoch": 0.39312085241611366,
      "grad_norm": 3.5457613468170166,
      "learning_rate": 4.28411214953271e-05,
      "loss": 0.2498,
      "step": 2103
    },
    {
      "epoch": 0.3933077857743714,
      "grad_norm": 2.3808698654174805,
      "learning_rate": 4.276635514018692e-05,
      "loss": 0.1485,
      "step": 2104
    },
    {
      "epoch": 0.39349471913262923,
      "grad_norm": 2.0630993843078613,
      "learning_rate": 4.2691588785046734e-05,
      "loss": 0.2271,
      "step": 2105
    },
    {
      "epoch": 0.393681652490887,
      "grad_norm": 1.0002057552337646,
      "learning_rate": 4.2616822429906546e-05,
      "loss": 0.1372,
      "step": 2106
    },
    {
      "epoch": 0.3938685858491448,
      "grad_norm": 1.645835280418396,
      "learning_rate": 4.254205607476636e-05,
      "loss": 0.1965,
      "step": 2107
    },
    {
      "epoch": 0.39405551920740256,
      "grad_norm": 1.1540486812591553,
      "learning_rate": 4.246728971962617e-05,
      "loss": 0.1815,
      "step": 2108
    },
    {
      "epoch": 0.3942424525656603,
      "grad_norm": 1.5061235427856445,
      "learning_rate": 4.239252336448598e-05,
      "loss": 0.1344,
      "step": 2109
    },
    {
      "epoch": 0.39442938592391813,
      "grad_norm": 2.3111023902893066,
      "learning_rate": 4.23177570093458e-05,
      "loss": 0.3083,
      "step": 2110
    },
    {
      "epoch": 0.3946163192821759,
      "grad_norm": 1.1632280349731445,
      "learning_rate": 4.2242990654205613e-05,
      "loss": 0.155,
      "step": 2111
    },
    {
      "epoch": 0.3948032526404337,
      "grad_norm": 1.188904047012329,
      "learning_rate": 4.2168224299065426e-05,
      "loss": 0.165,
      "step": 2112
    },
    {
      "epoch": 0.39499018599869146,
      "grad_norm": 2.0742557048797607,
      "learning_rate": 4.209345794392524e-05,
      "loss": 0.1353,
      "step": 2113
    },
    {
      "epoch": 0.39517711935694927,
      "grad_norm": 2.4501428604125977,
      "learning_rate": 4.201869158878505e-05,
      "loss": 0.2988,
      "step": 2114
    },
    {
      "epoch": 0.395364052715207,
      "grad_norm": 1.074023962020874,
      "learning_rate": 4.194392523364486e-05,
      "loss": 0.1557,
      "step": 2115
    },
    {
      "epoch": 0.3955509860734648,
      "grad_norm": 3.4872732162475586,
      "learning_rate": 4.186915887850468e-05,
      "loss": 0.2865,
      "step": 2116
    },
    {
      "epoch": 0.3957379194317226,
      "grad_norm": 3.818899631500244,
      "learning_rate": 4.179439252336449e-05,
      "loss": 0.2047,
      "step": 2117
    },
    {
      "epoch": 0.39592485278998035,
      "grad_norm": 1.3302360773086548,
      "learning_rate": 4.1719626168224305e-05,
      "loss": 0.1802,
      "step": 2118
    },
    {
      "epoch": 0.39611178614823817,
      "grad_norm": 1.2380439043045044,
      "learning_rate": 4.164485981308412e-05,
      "loss": 0.2236,
      "step": 2119
    },
    {
      "epoch": 0.3962987195064959,
      "grad_norm": 4.554717540740967,
      "learning_rate": 4.157009345794393e-05,
      "loss": 0.2445,
      "step": 2120
    },
    {
      "epoch": 0.39648565286475373,
      "grad_norm": 2.2941370010375977,
      "learning_rate": 4.149532710280374e-05,
      "loss": 0.1997,
      "step": 2121
    },
    {
      "epoch": 0.3966725862230115,
      "grad_norm": 1.400447964668274,
      "learning_rate": 4.142056074766355e-05,
      "loss": 0.2288,
      "step": 2122
    },
    {
      "epoch": 0.3968595195812693,
      "grad_norm": 1.1705721616744995,
      "learning_rate": 4.1345794392523365e-05,
      "loss": 0.2205,
      "step": 2123
    },
    {
      "epoch": 0.39704645293952706,
      "grad_norm": 2.81148362159729,
      "learning_rate": 4.127102803738318e-05,
      "loss": 0.2792,
      "step": 2124
    },
    {
      "epoch": 0.3972333862977848,
      "grad_norm": 1.7985841035842896,
      "learning_rate": 4.119626168224299e-05,
      "loss": 0.2634,
      "step": 2125
    },
    {
      "epoch": 0.39742031965604263,
      "grad_norm": 1.3221306800842285,
      "learning_rate": 4.11214953271028e-05,
      "loss": 0.1174,
      "step": 2126
    },
    {
      "epoch": 0.3976072530143004,
      "grad_norm": 3.1933579444885254,
      "learning_rate": 4.1046728971962614e-05,
      "loss": 0.208,
      "step": 2127
    },
    {
      "epoch": 0.3977941863725582,
      "grad_norm": 1.6709046363830566,
      "learning_rate": 4.097196261682243e-05,
      "loss": 0.1779,
      "step": 2128
    },
    {
      "epoch": 0.39798111973081596,
      "grad_norm": 1.4878705739974976,
      "learning_rate": 4.0897196261682245e-05,
      "loss": 0.2638,
      "step": 2129
    },
    {
      "epoch": 0.39816805308907377,
      "grad_norm": 1.851996898651123,
      "learning_rate": 4.082242990654206e-05,
      "loss": 0.1793,
      "step": 2130
    },
    {
      "epoch": 0.3983549864473315,
      "grad_norm": 2.713606357574463,
      "learning_rate": 4.074766355140187e-05,
      "loss": 0.2367,
      "step": 2131
    },
    {
      "epoch": 0.3985419198055893,
      "grad_norm": 1.2319749593734741,
      "learning_rate": 4.067289719626168e-05,
      "loss": 0.2148,
      "step": 2132
    },
    {
      "epoch": 0.3987288531638471,
      "grad_norm": 1.2237366437911987,
      "learning_rate": 4.059813084112149e-05,
      "loss": 0.2108,
      "step": 2133
    },
    {
      "epoch": 0.39891578652210485,
      "grad_norm": 1.1514033079147339,
      "learning_rate": 4.0523364485981305e-05,
      "loss": 0.1642,
      "step": 2134
    },
    {
      "epoch": 0.39910271988036267,
      "grad_norm": 1.3815422058105469,
      "learning_rate": 4.0448598130841124e-05,
      "loss": 0.2811,
      "step": 2135
    },
    {
      "epoch": 0.3992896532386204,
      "grad_norm": 1.4677993059158325,
      "learning_rate": 4.0373831775700936e-05,
      "loss": 0.1145,
      "step": 2136
    },
    {
      "epoch": 0.39947658659687824,
      "grad_norm": 1.6952787637710571,
      "learning_rate": 4.029906542056075e-05,
      "loss": 0.1287,
      "step": 2137
    },
    {
      "epoch": 0.399663519955136,
      "grad_norm": 1.0780421495437622,
      "learning_rate": 4.022429906542056e-05,
      "loss": 0.1949,
      "step": 2138
    },
    {
      "epoch": 0.39985045331339375,
      "grad_norm": 1.287132740020752,
      "learning_rate": 4.014953271028037e-05,
      "loss": 0.1323,
      "step": 2139
    },
    {
      "epoch": 0.40003738667165156,
      "grad_norm": 0.7116577625274658,
      "learning_rate": 4.0074766355140184e-05,
      "loss": 0.0882,
      "step": 2140
    },
    {
      "epoch": 0.4002243200299093,
      "grad_norm": 2.4589614868164062,
      "learning_rate": 4e-05,
      "loss": 0.3269,
      "step": 2141
    },
    {
      "epoch": 0.40041125338816713,
      "grad_norm": 1.4172935485839844,
      "learning_rate": 3.9925233644859815e-05,
      "loss": 0.147,
      "step": 2142
    },
    {
      "epoch": 0.4005981867464249,
      "grad_norm": 2.3736464977264404,
      "learning_rate": 3.985046728971963e-05,
      "loss": 0.2542,
      "step": 2143
    },
    {
      "epoch": 0.4007851201046827,
      "grad_norm": 1.0447967052459717,
      "learning_rate": 3.977570093457944e-05,
      "loss": 0.2186,
      "step": 2144
    },
    {
      "epoch": 0.40097205346294046,
      "grad_norm": 1.0484694242477417,
      "learning_rate": 3.970093457943925e-05,
      "loss": 0.0586,
      "step": 2145
    },
    {
      "epoch": 0.4011589868211982,
      "grad_norm": 1.0483084917068481,
      "learning_rate": 3.9626168224299064e-05,
      "loss": 0.1307,
      "step": 2146
    },
    {
      "epoch": 0.40134592017945603,
      "grad_norm": 2.820526599884033,
      "learning_rate": 3.955140186915888e-05,
      "loss": 0.2267,
      "step": 2147
    },
    {
      "epoch": 0.4015328535377138,
      "grad_norm": 1.9749231338500977,
      "learning_rate": 3.9476635514018695e-05,
      "loss": 0.1728,
      "step": 2148
    },
    {
      "epoch": 0.4017197868959716,
      "grad_norm": 1.3686611652374268,
      "learning_rate": 3.940186915887851e-05,
      "loss": 0.1111,
      "step": 2149
    },
    {
      "epoch": 0.40190672025422935,
      "grad_norm": 2.8512604236602783,
      "learning_rate": 3.932710280373832e-05,
      "loss": 0.1621,
      "step": 2150
    },
    {
      "epoch": 0.40209365361248717,
      "grad_norm": 2.2828049659729004,
      "learning_rate": 3.925233644859813e-05,
      "loss": 0.1806,
      "step": 2151
    },
    {
      "epoch": 0.4022805869707449,
      "grad_norm": 1.3771086931228638,
      "learning_rate": 3.917757009345794e-05,
      "loss": 0.1872,
      "step": 2152
    },
    {
      "epoch": 0.40246752032900274,
      "grad_norm": 1.8532309532165527,
      "learning_rate": 3.910280373831776e-05,
      "loss": 0.2811,
      "step": 2153
    },
    {
      "epoch": 0.4026544536872605,
      "grad_norm": 3.1235363483428955,
      "learning_rate": 3.9028037383177574e-05,
      "loss": 0.2403,
      "step": 2154
    },
    {
      "epoch": 0.40284138704551825,
      "grad_norm": 2.686171770095825,
      "learning_rate": 3.8953271028037386e-05,
      "loss": 0.188,
      "step": 2155
    },
    {
      "epoch": 0.40302832040377606,
      "grad_norm": 1.6906282901763916,
      "learning_rate": 3.88785046728972e-05,
      "loss": 0.1005,
      "step": 2156
    },
    {
      "epoch": 0.4032152537620338,
      "grad_norm": 1.620814323425293,
      "learning_rate": 3.880373831775701e-05,
      "loss": 0.24,
      "step": 2157
    },
    {
      "epoch": 0.40340218712029163,
      "grad_norm": 1.2282435894012451,
      "learning_rate": 3.872897196261682e-05,
      "loss": 0.2155,
      "step": 2158
    },
    {
      "epoch": 0.4035891204785494,
      "grad_norm": 1.0389906167984009,
      "learning_rate": 3.865420560747664e-05,
      "loss": 0.1644,
      "step": 2159
    },
    {
      "epoch": 0.4037760538368072,
      "grad_norm": 1.1006900072097778,
      "learning_rate": 3.857943925233645e-05,
      "loss": 0.1422,
      "step": 2160
    },
    {
      "epoch": 0.40396298719506496,
      "grad_norm": 0.6629382967948914,
      "learning_rate": 3.8504672897196265e-05,
      "loss": 0.1069,
      "step": 2161
    },
    {
      "epoch": 0.4041499205533227,
      "grad_norm": 1.0832103490829468,
      "learning_rate": 3.842990654205608e-05,
      "loss": 0.2654,
      "step": 2162
    },
    {
      "epoch": 0.40433685391158053,
      "grad_norm": 1.6723500490188599,
      "learning_rate": 3.835514018691589e-05,
      "loss": 0.1916,
      "step": 2163
    },
    {
      "epoch": 0.4045237872698383,
      "grad_norm": 1.4116607904434204,
      "learning_rate": 3.82803738317757e-05,
      "loss": 0.171,
      "step": 2164
    },
    {
      "epoch": 0.4047107206280961,
      "grad_norm": 1.1324551105499268,
      "learning_rate": 3.8205607476635514e-05,
      "loss": 0.067,
      "step": 2165
    },
    {
      "epoch": 0.40489765398635386,
      "grad_norm": 1.988650918006897,
      "learning_rate": 3.813084112149533e-05,
      "loss": 0.2159,
      "step": 2166
    },
    {
      "epoch": 0.40508458734461167,
      "grad_norm": 0.948701024055481,
      "learning_rate": 3.8056074766355145e-05,
      "loss": 0.1342,
      "step": 2167
    },
    {
      "epoch": 0.4052715207028694,
      "grad_norm": 2.1819112300872803,
      "learning_rate": 3.798130841121496e-05,
      "loss": 0.2264,
      "step": 2168
    },
    {
      "epoch": 0.4054584540611272,
      "grad_norm": 2.699132204055786,
      "learning_rate": 3.790654205607477e-05,
      "loss": 0.1541,
      "step": 2169
    },
    {
      "epoch": 0.405645387419385,
      "grad_norm": 1.6276183128356934,
      "learning_rate": 3.783177570093458e-05,
      "loss": 0.206,
      "step": 2170
    },
    {
      "epoch": 0.40583232077764275,
      "grad_norm": 2.5094847679138184,
      "learning_rate": 3.775700934579439e-05,
      "loss": 0.1488,
      "step": 2171
    },
    {
      "epoch": 0.40601925413590056,
      "grad_norm": 1.1947859525680542,
      "learning_rate": 3.768224299065421e-05,
      "loss": 0.0941,
      "step": 2172
    },
    {
      "epoch": 0.4062061874941583,
      "grad_norm": 1.2381705045700073,
      "learning_rate": 3.7607476635514024e-05,
      "loss": 0.1045,
      "step": 2173
    },
    {
      "epoch": 0.40639312085241613,
      "grad_norm": 6.038193225860596,
      "learning_rate": 3.7532710280373836e-05,
      "loss": 0.2744,
      "step": 2174
    },
    {
      "epoch": 0.4065800542106739,
      "grad_norm": 0.8895269632339478,
      "learning_rate": 3.745794392523365e-05,
      "loss": 0.186,
      "step": 2175
    },
    {
      "epoch": 0.40676698756893165,
      "grad_norm": 2.714266777038574,
      "learning_rate": 3.738317757009346e-05,
      "loss": 0.287,
      "step": 2176
    },
    {
      "epoch": 0.40695392092718946,
      "grad_norm": 1.603532314300537,
      "learning_rate": 3.730841121495327e-05,
      "loss": 0.2089,
      "step": 2177
    },
    {
      "epoch": 0.4071408542854472,
      "grad_norm": 1.222802996635437,
      "learning_rate": 3.723364485981309e-05,
      "loss": 0.1001,
      "step": 2178
    },
    {
      "epoch": 0.40732778764370503,
      "grad_norm": 2.016620635986328,
      "learning_rate": 3.71588785046729e-05,
      "loss": 0.1187,
      "step": 2179
    },
    {
      "epoch": 0.4075147210019628,
      "grad_norm": 1.6103014945983887,
      "learning_rate": 3.7084112149532715e-05,
      "loss": 0.1652,
      "step": 2180
    },
    {
      "epoch": 0.4077016543602206,
      "grad_norm": 2.7763991355895996,
      "learning_rate": 3.700934579439253e-05,
      "loss": 0.2643,
      "step": 2181
    },
    {
      "epoch": 0.40788858771847836,
      "grad_norm": 1.7809818983078003,
      "learning_rate": 3.693457943925234e-05,
      "loss": 0.1436,
      "step": 2182
    },
    {
      "epoch": 0.40807552107673617,
      "grad_norm": 2.2844207286834717,
      "learning_rate": 3.685981308411215e-05,
      "loss": 0.1872,
      "step": 2183
    },
    {
      "epoch": 0.4082624544349939,
      "grad_norm": 0.643949568271637,
      "learning_rate": 3.6785046728971964e-05,
      "loss": 0.0747,
      "step": 2184
    },
    {
      "epoch": 0.4084493877932517,
      "grad_norm": 1.1782035827636719,
      "learning_rate": 3.6710280373831776e-05,
      "loss": 0.0664,
      "step": 2185
    },
    {
      "epoch": 0.4086363211515095,
      "grad_norm": 1.8855904340744019,
      "learning_rate": 3.663551401869159e-05,
      "loss": 0.1917,
      "step": 2186
    },
    {
      "epoch": 0.40882325450976725,
      "grad_norm": 2.3836591243743896,
      "learning_rate": 3.656074766355141e-05,
      "loss": 0.2044,
      "step": 2187
    },
    {
      "epoch": 0.40901018786802507,
      "grad_norm": 2.078537702560425,
      "learning_rate": 3.648598130841122e-05,
      "loss": 0.1135,
      "step": 2188
    },
    {
      "epoch": 0.4091971212262828,
      "grad_norm": 2.1136834621429443,
      "learning_rate": 3.641121495327103e-05,
      "loss": 0.1639,
      "step": 2189
    },
    {
      "epoch": 0.40938405458454064,
      "grad_norm": 2.206902027130127,
      "learning_rate": 3.633644859813084e-05,
      "loss": 0.1807,
      "step": 2190
    },
    {
      "epoch": 0.4095709879427984,
      "grad_norm": 2.5109407901763916,
      "learning_rate": 3.6261682242990655e-05,
      "loss": 0.3081,
      "step": 2191
    },
    {
      "epoch": 0.40975792130105615,
      "grad_norm": 1.8349313735961914,
      "learning_rate": 3.618691588785047e-05,
      "loss": 0.0914,
      "step": 2192
    },
    {
      "epoch": 0.40994485465931396,
      "grad_norm": 3.8902764320373535,
      "learning_rate": 3.611214953271028e-05,
      "loss": 0.2674,
      "step": 2193
    },
    {
      "epoch": 0.4101317880175717,
      "grad_norm": 3.107699394226074,
      "learning_rate": 3.603738317757009e-05,
      "loss": 0.1837,
      "step": 2194
    },
    {
      "epoch": 0.41031872137582953,
      "grad_norm": 3.3541922569274902,
      "learning_rate": 3.5962616822429904e-05,
      "loss": 0.2483,
      "step": 2195
    },
    {
      "epoch": 0.4105056547340873,
      "grad_norm": 5.232593536376953,
      "learning_rate": 3.5887850467289716e-05,
      "loss": 0.1147,
      "step": 2196
    },
    {
      "epoch": 0.4106925880923451,
      "grad_norm": 2.3548285961151123,
      "learning_rate": 3.5813084112149535e-05,
      "loss": 0.139,
      "step": 2197
    },
    {
      "epoch": 0.41087952145060286,
      "grad_norm": 2.034095287322998,
      "learning_rate": 3.573831775700935e-05,
      "loss": 0.1514,
      "step": 2198
    },
    {
      "epoch": 0.4110664548088606,
      "grad_norm": 1.4723173379898071,
      "learning_rate": 3.566355140186916e-05,
      "loss": 0.0864,
      "step": 2199
    },
    {
      "epoch": 0.4112533881671184,
      "grad_norm": 1.7124502658843994,
      "learning_rate": 3.558878504672897e-05,
      "loss": 0.1781,
      "step": 2200
    },
    {
      "epoch": 0.4114403215253762,
      "grad_norm": 1.416418433189392,
      "learning_rate": 3.551401869158878e-05,
      "loss": 0.1885,
      "step": 2201
    },
    {
      "epoch": 0.411627254883634,
      "grad_norm": 1.7927128076553345,
      "learning_rate": 3.5439252336448595e-05,
      "loss": 0.1483,
      "step": 2202
    },
    {
      "epoch": 0.41181418824189175,
      "grad_norm": 1.37801194190979,
      "learning_rate": 3.5364485981308414e-05,
      "loss": 0.1532,
      "step": 2203
    },
    {
      "epoch": 0.41200112160014957,
      "grad_norm": 1.2407963275909424,
      "learning_rate": 3.5289719626168226e-05,
      "loss": 0.1354,
      "step": 2204
    },
    {
      "epoch": 0.4121880549584073,
      "grad_norm": 8.002511024475098,
      "learning_rate": 3.521495327102804e-05,
      "loss": 0.2941,
      "step": 2205
    },
    {
      "epoch": 0.41237498831666514,
      "grad_norm": 1.341960072517395,
      "learning_rate": 3.514018691588785e-05,
      "loss": 0.0987,
      "step": 2206
    },
    {
      "epoch": 0.4125619216749229,
      "grad_norm": 2.5646424293518066,
      "learning_rate": 3.506542056074766e-05,
      "loss": 0.176,
      "step": 2207
    },
    {
      "epoch": 0.41274885503318065,
      "grad_norm": 2.6353371143341064,
      "learning_rate": 3.4990654205607474e-05,
      "loss": 0.2035,
      "step": 2208
    },
    {
      "epoch": 0.41293578839143846,
      "grad_norm": 4.773593902587891,
      "learning_rate": 3.491588785046729e-05,
      "loss": 0.0909,
      "step": 2209
    },
    {
      "epoch": 0.4131227217496962,
      "grad_norm": 1.2132223844528198,
      "learning_rate": 3.4841121495327105e-05,
      "loss": 0.1056,
      "step": 2210
    },
    {
      "epoch": 0.41330965510795403,
      "grad_norm": 1.071857213973999,
      "learning_rate": 3.476635514018692e-05,
      "loss": 0.1747,
      "step": 2211
    },
    {
      "epoch": 0.4134965884662118,
      "grad_norm": 5.907964706420898,
      "learning_rate": 3.469158878504673e-05,
      "loss": 0.2551,
      "step": 2212
    },
    {
      "epoch": 0.4136835218244696,
      "grad_norm": 2.204592704772949,
      "learning_rate": 3.461682242990654e-05,
      "loss": 0.1496,
      "step": 2213
    },
    {
      "epoch": 0.41387045518272736,
      "grad_norm": 1.3472031354904175,
      "learning_rate": 3.4542056074766354e-05,
      "loss": 0.1111,
      "step": 2214
    },
    {
      "epoch": 0.4140573885409851,
      "grad_norm": 2.3922386169433594,
      "learning_rate": 3.446728971962617e-05,
      "loss": 0.2114,
      "step": 2215
    },
    {
      "epoch": 0.41424432189924293,
      "grad_norm": 1.8765559196472168,
      "learning_rate": 3.4392523364485985e-05,
      "loss": 0.1335,
      "step": 2216
    },
    {
      "epoch": 0.4144312552575007,
      "grad_norm": 1.40996253490448,
      "learning_rate": 3.43177570093458e-05,
      "loss": 0.1291,
      "step": 2217
    },
    {
      "epoch": 0.4146181886157585,
      "grad_norm": 1.49617600440979,
      "learning_rate": 3.424299065420561e-05,
      "loss": 0.1184,
      "step": 2218
    },
    {
      "epoch": 0.41480512197401626,
      "grad_norm": 0.7039779424667358,
      "learning_rate": 3.416822429906542e-05,
      "loss": 0.0404,
      "step": 2219
    },
    {
      "epoch": 0.41499205533227407,
      "grad_norm": 1.9580272436141968,
      "learning_rate": 3.409345794392523e-05,
      "loss": 0.078,
      "step": 2220
    },
    {
      "epoch": 0.4151789886905318,
      "grad_norm": 2.2422192096710205,
      "learning_rate": 3.401869158878505e-05,
      "loss": 0.1624,
      "step": 2221
    },
    {
      "epoch": 0.4153659220487896,
      "grad_norm": 1.3668551445007324,
      "learning_rate": 3.3943925233644864e-05,
      "loss": 0.1703,
      "step": 2222
    },
    {
      "epoch": 0.4155528554070474,
      "grad_norm": 1.7972897291183472,
      "learning_rate": 3.3869158878504676e-05,
      "loss": 0.203,
      "step": 2223
    },
    {
      "epoch": 0.41573978876530515,
      "grad_norm": 3.1705052852630615,
      "learning_rate": 3.379439252336449e-05,
      "loss": 0.2207,
      "step": 2224
    },
    {
      "epoch": 0.41592672212356296,
      "grad_norm": 2.6619699001312256,
      "learning_rate": 3.37196261682243e-05,
      "loss": 0.1786,
      "step": 2225
    },
    {
      "epoch": 0.4161136554818207,
      "grad_norm": 2.128289222717285,
      "learning_rate": 3.364485981308411e-05,
      "loss": 0.2283,
      "step": 2226
    },
    {
      "epoch": 0.41630058884007853,
      "grad_norm": 2.4961018562316895,
      "learning_rate": 3.3570093457943924e-05,
      "loss": 0.2498,
      "step": 2227
    },
    {
      "epoch": 0.4164875221983363,
      "grad_norm": 2.188760995864868,
      "learning_rate": 3.349532710280374e-05,
      "loss": 0.2413,
      "step": 2228
    },
    {
      "epoch": 0.41667445555659405,
      "grad_norm": 1.4744905233383179,
      "learning_rate": 3.3420560747663555e-05,
      "loss": 0.1917,
      "step": 2229
    },
    {
      "epoch": 0.41686138891485186,
      "grad_norm": 3.535184621810913,
      "learning_rate": 3.334579439252337e-05,
      "loss": 0.226,
      "step": 2230
    },
    {
      "epoch": 0.4170483222731096,
      "grad_norm": 1.636480450630188,
      "learning_rate": 3.327102803738318e-05,
      "loss": 0.1971,
      "step": 2231
    },
    {
      "epoch": 0.41723525563136743,
      "grad_norm": 1.7895467281341553,
      "learning_rate": 3.319626168224299e-05,
      "loss": 0.1702,
      "step": 2232
    },
    {
      "epoch": 0.4174221889896252,
      "grad_norm": 1.4476855993270874,
      "learning_rate": 3.3121495327102804e-05,
      "loss": 0.1738,
      "step": 2233
    },
    {
      "epoch": 0.417609122347883,
      "grad_norm": 2.655782461166382,
      "learning_rate": 3.304672897196262e-05,
      "loss": 0.2081,
      "step": 2234
    },
    {
      "epoch": 0.41779605570614076,
      "grad_norm": 1.9733084440231323,
      "learning_rate": 3.2971962616822435e-05,
      "loss": 0.1627,
      "step": 2235
    },
    {
      "epoch": 0.41798298906439857,
      "grad_norm": 1.0158822536468506,
      "learning_rate": 3.289719626168225e-05,
      "loss": 0.0465,
      "step": 2236
    },
    {
      "epoch": 0.4181699224226563,
      "grad_norm": 1.9126800298690796,
      "learning_rate": 3.282242990654206e-05,
      "loss": 0.2071,
      "step": 2237
    },
    {
      "epoch": 0.4183568557809141,
      "grad_norm": 2.766474962234497,
      "learning_rate": 3.274766355140187e-05,
      "loss": 0.1849,
      "step": 2238
    },
    {
      "epoch": 0.4185437891391719,
      "grad_norm": 2.0360212326049805,
      "learning_rate": 3.267289719626168e-05,
      "loss": 0.2204,
      "step": 2239
    },
    {
      "epoch": 0.41873072249742965,
      "grad_norm": 1.8800863027572632,
      "learning_rate": 3.25981308411215e-05,
      "loss": 0.1731,
      "step": 2240
    },
    {
      "epoch": 0.41891765585568747,
      "grad_norm": 1.5320390462875366,
      "learning_rate": 3.2523364485981314e-05,
      "loss": 0.1772,
      "step": 2241
    },
    {
      "epoch": 0.4191045892139452,
      "grad_norm": 3.547654628753662,
      "learning_rate": 3.2448598130841126e-05,
      "loss": 0.1911,
      "step": 2242
    },
    {
      "epoch": 0.41929152257220303,
      "grad_norm": 1.5452076196670532,
      "learning_rate": 3.237383177570094e-05,
      "loss": 0.1187,
      "step": 2243
    },
    {
      "epoch": 0.4194784559304608,
      "grad_norm": 1.3975341320037842,
      "learning_rate": 3.229906542056075e-05,
      "loss": 0.2369,
      "step": 2244
    },
    {
      "epoch": 0.41966538928871855,
      "grad_norm": 1.9968568086624146,
      "learning_rate": 3.222429906542056e-05,
      "loss": 0.2397,
      "step": 2245
    },
    {
      "epoch": 0.41985232264697636,
      "grad_norm": 2.9049692153930664,
      "learning_rate": 3.214953271028038e-05,
      "loss": 0.1452,
      "step": 2246
    },
    {
      "epoch": 0.4200392560052341,
      "grad_norm": 1.4369207620620728,
      "learning_rate": 3.207476635514019e-05,
      "loss": 0.196,
      "step": 2247
    },
    {
      "epoch": 0.42022618936349193,
      "grad_norm": 1.2668496370315552,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.1641,
      "step": 2248
    },
    {
      "epoch": 0.4204131227217497,
      "grad_norm": 1.5734702348709106,
      "learning_rate": 3.192523364485982e-05,
      "loss": 0.1474,
      "step": 2249
    },
    {
      "epoch": 0.4206000560800075,
      "grad_norm": 1.5491161346435547,
      "learning_rate": 3.185046728971963e-05,
      "loss": 0.2433,
      "step": 2250
    },
    {
      "epoch": 0.42078698943826526,
      "grad_norm": 2.8138723373413086,
      "learning_rate": 3.177570093457944e-05,
      "loss": 0.2544,
      "step": 2251
    },
    {
      "epoch": 0.420973922796523,
      "grad_norm": 4.545984268188477,
      "learning_rate": 3.1700934579439254e-05,
      "loss": 0.2304,
      "step": 2252
    },
    {
      "epoch": 0.4211608561547808,
      "grad_norm": 3.9262170791625977,
      "learning_rate": 3.1626168224299066e-05,
      "loss": 0.1365,
      "step": 2253
    },
    {
      "epoch": 0.4213477895130386,
      "grad_norm": 1.3441821336746216,
      "learning_rate": 3.155140186915888e-05,
      "loss": 0.1418,
      "step": 2254
    },
    {
      "epoch": 0.4215347228712964,
      "grad_norm": 1.4134104251861572,
      "learning_rate": 3.147663551401869e-05,
      "loss": 0.1136,
      "step": 2255
    },
    {
      "epoch": 0.42172165622955415,
      "grad_norm": 2.7814087867736816,
      "learning_rate": 3.14018691588785e-05,
      "loss": 0.2849,
      "step": 2256
    },
    {
      "epoch": 0.42190858958781197,
      "grad_norm": 1.6721304655075073,
      "learning_rate": 3.1327102803738314e-05,
      "loss": 0.1307,
      "step": 2257
    },
    {
      "epoch": 0.4220955229460697,
      "grad_norm": 1.2549941539764404,
      "learning_rate": 3.1252336448598126e-05,
      "loss": 0.1351,
      "step": 2258
    },
    {
      "epoch": 0.4222824563043275,
      "grad_norm": 1.3605517148971558,
      "learning_rate": 3.1177570093457945e-05,
      "loss": 0.1169,
      "step": 2259
    },
    {
      "epoch": 0.4224693896625853,
      "grad_norm": 1.460936427116394,
      "learning_rate": 3.110280373831776e-05,
      "loss": 0.0953,
      "step": 2260
    },
    {
      "epoch": 0.42265632302084305,
      "grad_norm": 1.4715650081634521,
      "learning_rate": 3.102803738317757e-05,
      "loss": 0.2366,
      "step": 2261
    },
    {
      "epoch": 0.42284325637910086,
      "grad_norm": 1.3421630859375,
      "learning_rate": 3.095327102803738e-05,
      "loss": 0.2065,
      "step": 2262
    },
    {
      "epoch": 0.4230301897373586,
      "grad_norm": 2.7590959072113037,
      "learning_rate": 3.0878504672897193e-05,
      "loss": 0.2389,
      "step": 2263
    },
    {
      "epoch": 0.42321712309561643,
      "grad_norm": 1.3519395589828491,
      "learning_rate": 3.0803738317757006e-05,
      "loss": 0.1332,
      "step": 2264
    },
    {
      "epoch": 0.4234040564538742,
      "grad_norm": 1.748192548751831,
      "learning_rate": 3.0728971962616824e-05,
      "loss": 0.1362,
      "step": 2265
    },
    {
      "epoch": 0.423590989812132,
      "grad_norm": 1.6964786052703857,
      "learning_rate": 3.0654205607476637e-05,
      "loss": 0.2071,
      "step": 2266
    },
    {
      "epoch": 0.42377792317038976,
      "grad_norm": 2.4662058353424072,
      "learning_rate": 3.057943925233645e-05,
      "loss": 0.2264,
      "step": 2267
    },
    {
      "epoch": 0.4239648565286475,
      "grad_norm": 4.552587509155273,
      "learning_rate": 3.0504672897196264e-05,
      "loss": 0.2845,
      "step": 2268
    },
    {
      "epoch": 0.42415178988690533,
      "grad_norm": 2.320624351501465,
      "learning_rate": 3.0429906542056076e-05,
      "loss": 0.1711,
      "step": 2269
    },
    {
      "epoch": 0.4243387232451631,
      "grad_norm": 0.974115252494812,
      "learning_rate": 3.0355140186915888e-05,
      "loss": 0.0671,
      "step": 2270
    },
    {
      "epoch": 0.4245256566034209,
      "grad_norm": 1.2190089225769043,
      "learning_rate": 3.0280373831775704e-05,
      "loss": 0.0813,
      "step": 2271
    },
    {
      "epoch": 0.42471258996167865,
      "grad_norm": 1.4011894464492798,
      "learning_rate": 3.0205607476635516e-05,
      "loss": 0.087,
      "step": 2272
    },
    {
      "epoch": 0.42489952331993647,
      "grad_norm": 1.572777509689331,
      "learning_rate": 3.0130841121495328e-05,
      "loss": 0.1235,
      "step": 2273
    },
    {
      "epoch": 0.4250864566781942,
      "grad_norm": 1.9555467367172241,
      "learning_rate": 3.005607476635514e-05,
      "loss": 0.244,
      "step": 2274
    },
    {
      "epoch": 0.425273390036452,
      "grad_norm": 1.3342467546463013,
      "learning_rate": 2.9981308411214952e-05,
      "loss": 0.0888,
      "step": 2275
    },
    {
      "epoch": 0.4254603233947098,
      "grad_norm": 1.5164092779159546,
      "learning_rate": 2.9906542056074764e-05,
      "loss": 0.1576,
      "step": 2276
    },
    {
      "epoch": 0.42564725675296755,
      "grad_norm": 2.093395948410034,
      "learning_rate": 2.9831775700934583e-05,
      "loss": 0.2007,
      "step": 2277
    },
    {
      "epoch": 0.42583419011122536,
      "grad_norm": 1.90924072265625,
      "learning_rate": 2.9757009345794395e-05,
      "loss": 0.2281,
      "step": 2278
    },
    {
      "epoch": 0.4260211234694831,
      "grad_norm": 2.3174173831939697,
      "learning_rate": 2.9682242990654207e-05,
      "loss": 0.238,
      "step": 2279
    },
    {
      "epoch": 0.42620805682774093,
      "grad_norm": 1.353786826133728,
      "learning_rate": 2.960747663551402e-05,
      "loss": 0.0927,
      "step": 2280
    },
    {
      "epoch": 0.4263949901859987,
      "grad_norm": 1.5906989574432373,
      "learning_rate": 2.953271028037383e-05,
      "loss": 0.1702,
      "step": 2281
    },
    {
      "epoch": 0.42658192354425645,
      "grad_norm": 1.4650617837905884,
      "learning_rate": 2.9457943925233643e-05,
      "loss": 0.2416,
      "step": 2282
    },
    {
      "epoch": 0.42676885690251426,
      "grad_norm": 2.4223906993865967,
      "learning_rate": 2.9383177570093462e-05,
      "loss": 0.1614,
      "step": 2283
    },
    {
      "epoch": 0.426955790260772,
      "grad_norm": 1.9657975435256958,
      "learning_rate": 2.9308411214953274e-05,
      "loss": 0.169,
      "step": 2284
    },
    {
      "epoch": 0.42714272361902983,
      "grad_norm": 2.153371810913086,
      "learning_rate": 2.9233644859813087e-05,
      "loss": 0.1801,
      "step": 2285
    },
    {
      "epoch": 0.4273296569772876,
      "grad_norm": 2.2688827514648438,
      "learning_rate": 2.91588785046729e-05,
      "loss": 0.1825,
      "step": 2286
    },
    {
      "epoch": 0.4275165903355454,
      "grad_norm": 1.5997358560562134,
      "learning_rate": 2.908411214953271e-05,
      "loss": 0.1462,
      "step": 2287
    },
    {
      "epoch": 0.42770352369380316,
      "grad_norm": 3.2289819717407227,
      "learning_rate": 2.9009345794392523e-05,
      "loss": 0.1688,
      "step": 2288
    },
    {
      "epoch": 0.4278904570520609,
      "grad_norm": 1.757946252822876,
      "learning_rate": 2.893457943925234e-05,
      "loss": 0.2199,
      "step": 2289
    },
    {
      "epoch": 0.4280773904103187,
      "grad_norm": 2.6632516384124756,
      "learning_rate": 2.8859813084112154e-05,
      "loss": 0.2107,
      "step": 2290
    },
    {
      "epoch": 0.4282643237685765,
      "grad_norm": 2.5591752529144287,
      "learning_rate": 2.8785046728971966e-05,
      "loss": 0.1358,
      "step": 2291
    },
    {
      "epoch": 0.4284512571268343,
      "grad_norm": 5.325900554656982,
      "learning_rate": 2.8710280373831778e-05,
      "loss": 0.2234,
      "step": 2292
    },
    {
      "epoch": 0.42863819048509205,
      "grad_norm": 2.2809383869171143,
      "learning_rate": 2.863551401869159e-05,
      "loss": 0.1947,
      "step": 2293
    },
    {
      "epoch": 0.42882512384334986,
      "grad_norm": 1.7917382717132568,
      "learning_rate": 2.8560747663551402e-05,
      "loss": 0.2319,
      "step": 2294
    },
    {
      "epoch": 0.4290120572016076,
      "grad_norm": 1.4460458755493164,
      "learning_rate": 2.8485981308411214e-05,
      "loss": 0.1491,
      "step": 2295
    },
    {
      "epoch": 0.42919899055986543,
      "grad_norm": 1.275766134262085,
      "learning_rate": 2.8411214953271033e-05,
      "loss": 0.1407,
      "step": 2296
    },
    {
      "epoch": 0.4293859239181232,
      "grad_norm": 1.465938925743103,
      "learning_rate": 2.8336448598130845e-05,
      "loss": 0.1328,
      "step": 2297
    },
    {
      "epoch": 0.42957285727638095,
      "grad_norm": 2.346902370452881,
      "learning_rate": 2.8261682242990657e-05,
      "loss": 0.2038,
      "step": 2298
    },
    {
      "epoch": 0.42975979063463876,
      "grad_norm": 1.8613390922546387,
      "learning_rate": 2.818691588785047e-05,
      "loss": 0.206,
      "step": 2299
    },
    {
      "epoch": 0.4299467239928965,
      "grad_norm": 1.2590941190719604,
      "learning_rate": 2.811214953271028e-05,
      "loss": 0.0997,
      "step": 2300
    },
    {
      "epoch": 0.43013365735115433,
      "grad_norm": 2.6209187507629395,
      "learning_rate": 2.8037383177570094e-05,
      "loss": 0.3456,
      "step": 2301
    },
    {
      "epoch": 0.4303205907094121,
      "grad_norm": 2.295015811920166,
      "learning_rate": 2.796261682242991e-05,
      "loss": 0.1884,
      "step": 2302
    },
    {
      "epoch": 0.4305075240676699,
      "grad_norm": 1.6967952251434326,
      "learning_rate": 2.788785046728972e-05,
      "loss": 0.1411,
      "step": 2303
    },
    {
      "epoch": 0.43069445742592766,
      "grad_norm": 2.0462746620178223,
      "learning_rate": 2.7813084112149533e-05,
      "loss": 0.1867,
      "step": 2304
    },
    {
      "epoch": 0.4308813907841854,
      "grad_norm": 1.3559483289718628,
      "learning_rate": 2.7738317757009345e-05,
      "loss": 0.1467,
      "step": 2305
    },
    {
      "epoch": 0.4310683241424432,
      "grad_norm": 2.9274847507476807,
      "learning_rate": 2.7663551401869157e-05,
      "loss": 0.3008,
      "step": 2306
    },
    {
      "epoch": 0.431255257500701,
      "grad_norm": 3.985255479812622,
      "learning_rate": 2.758878504672897e-05,
      "loss": 0.3247,
      "step": 2307
    },
    {
      "epoch": 0.4314421908589588,
      "grad_norm": 5.6741509437561035,
      "learning_rate": 2.751401869158879e-05,
      "loss": 0.3183,
      "step": 2308
    },
    {
      "epoch": 0.43162912421721655,
      "grad_norm": 1.4975343942642212,
      "learning_rate": 2.74392523364486e-05,
      "loss": 0.1989,
      "step": 2309
    },
    {
      "epoch": 0.43181605757547437,
      "grad_norm": 1.5309001207351685,
      "learning_rate": 2.7364485981308413e-05,
      "loss": 0.1509,
      "step": 2310
    },
    {
      "epoch": 0.4320029909337321,
      "grad_norm": 1.2902477979660034,
      "learning_rate": 2.7289719626168225e-05,
      "loss": 0.1149,
      "step": 2311
    },
    {
      "epoch": 0.4321899242919899,
      "grad_norm": 1.7402448654174805,
      "learning_rate": 2.7214953271028037e-05,
      "loss": 0.2322,
      "step": 2312
    },
    {
      "epoch": 0.4323768576502477,
      "grad_norm": 1.1651222705841064,
      "learning_rate": 2.714018691588785e-05,
      "loss": 0.1669,
      "step": 2313
    },
    {
      "epoch": 0.43256379100850545,
      "grad_norm": 1.4960323572158813,
      "learning_rate": 2.7065420560747668e-05,
      "loss": 0.1618,
      "step": 2314
    },
    {
      "epoch": 0.43275072436676326,
      "grad_norm": 1.6489126682281494,
      "learning_rate": 2.699065420560748e-05,
      "loss": 0.2516,
      "step": 2315
    },
    {
      "epoch": 0.432937657725021,
      "grad_norm": 1.542265772819519,
      "learning_rate": 2.6915887850467292e-05,
      "loss": 0.1873,
      "step": 2316
    },
    {
      "epoch": 0.43312459108327883,
      "grad_norm": 1.307271122932434,
      "learning_rate": 2.6841121495327104e-05,
      "loss": 0.2099,
      "step": 2317
    },
    {
      "epoch": 0.4333115244415366,
      "grad_norm": 2.340355396270752,
      "learning_rate": 2.6766355140186916e-05,
      "loss": 0.1652,
      "step": 2318
    },
    {
      "epoch": 0.43349845779979435,
      "grad_norm": 1.9687987565994263,
      "learning_rate": 2.6691588785046728e-05,
      "loss": 0.1353,
      "step": 2319
    },
    {
      "epoch": 0.43368539115805216,
      "grad_norm": 2.2768714427948,
      "learning_rate": 2.6616822429906547e-05,
      "loss": 0.1355,
      "step": 2320
    },
    {
      "epoch": 0.4338723245163099,
      "grad_norm": 1.9223721027374268,
      "learning_rate": 2.654205607476636e-05,
      "loss": 0.2236,
      "step": 2321
    },
    {
      "epoch": 0.4340592578745677,
      "grad_norm": 1.4206976890563965,
      "learning_rate": 2.646728971962617e-05,
      "loss": 0.2452,
      "step": 2322
    },
    {
      "epoch": 0.4342461912328255,
      "grad_norm": 1.2008239030838013,
      "learning_rate": 2.6392523364485983e-05,
      "loss": 0.1484,
      "step": 2323
    },
    {
      "epoch": 0.4344331245910833,
      "grad_norm": 1.141951084136963,
      "learning_rate": 2.6317757009345795e-05,
      "loss": 0.143,
      "step": 2324
    },
    {
      "epoch": 0.43462005794934105,
      "grad_norm": 1.8253998756408691,
      "learning_rate": 2.6242990654205607e-05,
      "loss": 0.0968,
      "step": 2325
    },
    {
      "epoch": 0.43480699130759887,
      "grad_norm": 5.515081405639648,
      "learning_rate": 2.616822429906542e-05,
      "loss": 0.2223,
      "step": 2326
    },
    {
      "epoch": 0.4349939246658566,
      "grad_norm": 1.5924371480941772,
      "learning_rate": 2.609345794392524e-05,
      "loss": 0.1187,
      "step": 2327
    },
    {
      "epoch": 0.4351808580241144,
      "grad_norm": 1.5809450149536133,
      "learning_rate": 2.601869158878505e-05,
      "loss": 0.2372,
      "step": 2328
    },
    {
      "epoch": 0.4353677913823722,
      "grad_norm": 2.0981650352478027,
      "learning_rate": 2.5943925233644863e-05,
      "loss": 0.1309,
      "step": 2329
    },
    {
      "epoch": 0.43555472474062995,
      "grad_norm": 1.4534211158752441,
      "learning_rate": 2.5869158878504675e-05,
      "loss": 0.1934,
      "step": 2330
    },
    {
      "epoch": 0.43574165809888776,
      "grad_norm": 1.7612569332122803,
      "learning_rate": 2.5794392523364487e-05,
      "loss": 0.191,
      "step": 2331
    },
    {
      "epoch": 0.4359285914571455,
      "grad_norm": 3.251573085784912,
      "learning_rate": 2.57196261682243e-05,
      "loss": 0.2593,
      "step": 2332
    },
    {
      "epoch": 0.43611552481540333,
      "grad_norm": 1.3027682304382324,
      "learning_rate": 2.5644859813084114e-05,
      "loss": 0.1744,
      "step": 2333
    },
    {
      "epoch": 0.4363024581736611,
      "grad_norm": 2.099268674850464,
      "learning_rate": 2.5570093457943926e-05,
      "loss": 0.1956,
      "step": 2334
    },
    {
      "epoch": 0.43648939153191885,
      "grad_norm": 1.8266124725341797,
      "learning_rate": 2.549532710280374e-05,
      "loss": 0.0961,
      "step": 2335
    },
    {
      "epoch": 0.43667632489017666,
      "grad_norm": 2.0396862030029297,
      "learning_rate": 2.542056074766355e-05,
      "loss": 0.1928,
      "step": 2336
    },
    {
      "epoch": 0.4368632582484344,
      "grad_norm": 6.3657941818237305,
      "learning_rate": 2.5345794392523363e-05,
      "loss": 0.1833,
      "step": 2337
    },
    {
      "epoch": 0.43705019160669223,
      "grad_norm": 4.309850215911865,
      "learning_rate": 2.5271028037383178e-05,
      "loss": 0.3712,
      "step": 2338
    },
    {
      "epoch": 0.43723712496495,
      "grad_norm": 1.508138656616211,
      "learning_rate": 2.5196261682242994e-05,
      "loss": 0.2108,
      "step": 2339
    },
    {
      "epoch": 0.4374240583232078,
      "grad_norm": 3.754124402999878,
      "learning_rate": 2.5121495327102806e-05,
      "loss": 0.1989,
      "step": 2340
    },
    {
      "epoch": 0.43761099168146556,
      "grad_norm": 2.270329475402832,
      "learning_rate": 2.5046728971962618e-05,
      "loss": 0.1373,
      "step": 2341
    },
    {
      "epoch": 0.4377979250397233,
      "grad_norm": 2.318228006362915,
      "learning_rate": 2.497196261682243e-05,
      "loss": 0.2315,
      "step": 2342
    },
    {
      "epoch": 0.4379848583979811,
      "grad_norm": 1.3933055400848389,
      "learning_rate": 2.4897196261682242e-05,
      "loss": 0.122,
      "step": 2343
    },
    {
      "epoch": 0.4381717917562389,
      "grad_norm": 6.324558258056641,
      "learning_rate": 2.4822429906542057e-05,
      "loss": 0.1621,
      "step": 2344
    },
    {
      "epoch": 0.4383587251144967,
      "grad_norm": 8.263326644897461,
      "learning_rate": 2.474766355140187e-05,
      "loss": 0.2025,
      "step": 2345
    },
    {
      "epoch": 0.43854565847275445,
      "grad_norm": 11.735170364379883,
      "learning_rate": 2.467289719626168e-05,
      "loss": 0.3494,
      "step": 2346
    },
    {
      "epoch": 0.43873259183101226,
      "grad_norm": 1.7419111728668213,
      "learning_rate": 2.4598130841121497e-05,
      "loss": 0.2033,
      "step": 2347
    },
    {
      "epoch": 0.43891952518927,
      "grad_norm": 2.27748441696167,
      "learning_rate": 2.452336448598131e-05,
      "loss": 0.1702,
      "step": 2348
    },
    {
      "epoch": 0.43910645854752783,
      "grad_norm": 1.1287462711334229,
      "learning_rate": 2.444859813084112e-05,
      "loss": 0.1237,
      "step": 2349
    },
    {
      "epoch": 0.4392933919057856,
      "grad_norm": 1.7648794651031494,
      "learning_rate": 2.4373831775700937e-05,
      "loss": 0.1697,
      "step": 2350
    },
    {
      "epoch": 0.43948032526404335,
      "grad_norm": 3.533139705657959,
      "learning_rate": 2.429906542056075e-05,
      "loss": 0.2094,
      "step": 2351
    },
    {
      "epoch": 0.43966725862230116,
      "grad_norm": 3.607243061065674,
      "learning_rate": 2.422429906542056e-05,
      "loss": 0.1928,
      "step": 2352
    },
    {
      "epoch": 0.4398541919805589,
      "grad_norm": 2.3061206340789795,
      "learning_rate": 2.4149532710280376e-05,
      "loss": 0.1913,
      "step": 2353
    },
    {
      "epoch": 0.44004112533881673,
      "grad_norm": 1.8923513889312744,
      "learning_rate": 2.407476635514019e-05,
      "loss": 0.2148,
      "step": 2354
    },
    {
      "epoch": 0.4402280586970745,
      "grad_norm": 3.085238456726074,
      "learning_rate": 2.4e-05,
      "loss": 0.2323,
      "step": 2355
    },
    {
      "epoch": 0.4404149920553323,
      "grad_norm": 1.4848432540893555,
      "learning_rate": 2.3925233644859816e-05,
      "loss": 0.1765,
      "step": 2356
    },
    {
      "epoch": 0.44060192541359006,
      "grad_norm": 2.7624895572662354,
      "learning_rate": 2.3850467289719628e-05,
      "loss": 0.1912,
      "step": 2357
    },
    {
      "epoch": 0.4407888587718478,
      "grad_norm": 1.02838134765625,
      "learning_rate": 2.377570093457944e-05,
      "loss": 0.0937,
      "step": 2358
    },
    {
      "epoch": 0.4409757921301056,
      "grad_norm": 1.0641735792160034,
      "learning_rate": 2.3700934579439256e-05,
      "loss": 0.1583,
      "step": 2359
    },
    {
      "epoch": 0.4411627254883634,
      "grad_norm": 1.7173190116882324,
      "learning_rate": 2.3626168224299068e-05,
      "loss": 0.2602,
      "step": 2360
    },
    {
      "epoch": 0.4413496588466212,
      "grad_norm": 1.203999638557434,
      "learning_rate": 2.355140186915888e-05,
      "loss": 0.1519,
      "step": 2361
    },
    {
      "epoch": 0.44153659220487895,
      "grad_norm": 1.612666368484497,
      "learning_rate": 2.3476635514018695e-05,
      "loss": 0.193,
      "step": 2362
    },
    {
      "epoch": 0.44172352556313677,
      "grad_norm": 1.6755918264389038,
      "learning_rate": 2.3401869158878507e-05,
      "loss": 0.2963,
      "step": 2363
    },
    {
      "epoch": 0.4419104589213945,
      "grad_norm": 4.151277542114258,
      "learning_rate": 2.332710280373832e-05,
      "loss": 0.3127,
      "step": 2364
    },
    {
      "epoch": 0.4420973922796523,
      "grad_norm": 1.2996060848236084,
      "learning_rate": 2.325233644859813e-05,
      "loss": 0.1397,
      "step": 2365
    },
    {
      "epoch": 0.4422843256379101,
      "grad_norm": 1.1725438833236694,
      "learning_rate": 2.3177570093457944e-05,
      "loss": 0.2601,
      "step": 2366
    },
    {
      "epoch": 0.44247125899616785,
      "grad_norm": 1.2320325374603271,
      "learning_rate": 2.310280373831776e-05,
      "loss": 0.1835,
      "step": 2367
    },
    {
      "epoch": 0.44265819235442566,
      "grad_norm": 2.3891868591308594,
      "learning_rate": 2.302803738317757e-05,
      "loss": 0.2217,
      "step": 2368
    },
    {
      "epoch": 0.4428451257126834,
      "grad_norm": 1.6853071451187134,
      "learning_rate": 2.2953271028037383e-05,
      "loss": 0.1705,
      "step": 2369
    },
    {
      "epoch": 0.44303205907094123,
      "grad_norm": 2.1259806156158447,
      "learning_rate": 2.2878504672897196e-05,
      "loss": 0.1997,
      "step": 2370
    },
    {
      "epoch": 0.443218992429199,
      "grad_norm": 1.2061978578567505,
      "learning_rate": 2.2803738317757008e-05,
      "loss": 0.1952,
      "step": 2371
    },
    {
      "epoch": 0.44340592578745675,
      "grad_norm": 1.7526066303253174,
      "learning_rate": 2.2728971962616823e-05,
      "loss": 0.1465,
      "step": 2372
    },
    {
      "epoch": 0.44359285914571456,
      "grad_norm": 2.3013558387756348,
      "learning_rate": 2.2654205607476635e-05,
      "loss": 0.1976,
      "step": 2373
    },
    {
      "epoch": 0.4437797925039723,
      "grad_norm": 1.8765500783920288,
      "learning_rate": 2.2579439252336447e-05,
      "loss": 0.1457,
      "step": 2374
    },
    {
      "epoch": 0.4439667258622301,
      "grad_norm": 0.7392211556434631,
      "learning_rate": 2.2504672897196263e-05,
      "loss": 0.1147,
      "step": 2375
    },
    {
      "epoch": 0.4441536592204879,
      "grad_norm": 1.440477728843689,
      "learning_rate": 2.2429906542056075e-05,
      "loss": 0.1555,
      "step": 2376
    },
    {
      "epoch": 0.4443405925787457,
      "grad_norm": 1.3067858219146729,
      "learning_rate": 2.2355140186915887e-05,
      "loss": 0.158,
      "step": 2377
    },
    {
      "epoch": 0.44452752593700345,
      "grad_norm": 2.712857246398926,
      "learning_rate": 2.2280373831775702e-05,
      "loss": 0.2913,
      "step": 2378
    },
    {
      "epoch": 0.44471445929526127,
      "grad_norm": 1.4181805849075317,
      "learning_rate": 2.2205607476635514e-05,
      "loss": 0.1968,
      "step": 2379
    },
    {
      "epoch": 0.444901392653519,
      "grad_norm": 2.6710362434387207,
      "learning_rate": 2.2130841121495327e-05,
      "loss": 0.1692,
      "step": 2380
    },
    {
      "epoch": 0.4450883260117768,
      "grad_norm": 1.258917212486267,
      "learning_rate": 2.2056074766355142e-05,
      "loss": 0.1355,
      "step": 2381
    },
    {
      "epoch": 0.4452752593700346,
      "grad_norm": 0.841216504573822,
      "learning_rate": 2.1981308411214954e-05,
      "loss": 0.1207,
      "step": 2382
    },
    {
      "epoch": 0.44546219272829235,
      "grad_norm": 1.9188246726989746,
      "learning_rate": 2.1906542056074766e-05,
      "loss": 0.1912,
      "step": 2383
    },
    {
      "epoch": 0.44564912608655016,
      "grad_norm": 1.2271523475646973,
      "learning_rate": 2.1831775700934582e-05,
      "loss": 0.134,
      "step": 2384
    },
    {
      "epoch": 0.4458360594448079,
      "grad_norm": 5.240095615386963,
      "learning_rate": 2.1757009345794394e-05,
      "loss": 0.1941,
      "step": 2385
    },
    {
      "epoch": 0.44602299280306573,
      "grad_norm": 1.3566347360610962,
      "learning_rate": 2.1682242990654206e-05,
      "loss": 0.1965,
      "step": 2386
    },
    {
      "epoch": 0.4462099261613235,
      "grad_norm": 1.8310706615447998,
      "learning_rate": 2.160747663551402e-05,
      "loss": 0.2006,
      "step": 2387
    },
    {
      "epoch": 0.44639685951958125,
      "grad_norm": 1.899376392364502,
      "learning_rate": 2.1532710280373833e-05,
      "loss": 0.1602,
      "step": 2388
    },
    {
      "epoch": 0.44658379287783906,
      "grad_norm": 1.2858684062957764,
      "learning_rate": 2.1457943925233646e-05,
      "loss": 0.1244,
      "step": 2389
    },
    {
      "epoch": 0.4467707262360968,
      "grad_norm": 1.2200596332550049,
      "learning_rate": 2.138317757009346e-05,
      "loss": 0.1533,
      "step": 2390
    },
    {
      "epoch": 0.44695765959435463,
      "grad_norm": 0.6419885158538818,
      "learning_rate": 2.1308411214953273e-05,
      "loss": 0.0534,
      "step": 2391
    },
    {
      "epoch": 0.4471445929526124,
      "grad_norm": 1.4356471300125122,
      "learning_rate": 2.1233644859813085e-05,
      "loss": 0.2248,
      "step": 2392
    },
    {
      "epoch": 0.4473315263108702,
      "grad_norm": 1.4225199222564697,
      "learning_rate": 2.11588785046729e-05,
      "loss": 0.0922,
      "step": 2393
    },
    {
      "epoch": 0.44751845966912795,
      "grad_norm": 2.0331408977508545,
      "learning_rate": 2.1084112149532713e-05,
      "loss": 0.1899,
      "step": 2394
    },
    {
      "epoch": 0.4477053930273857,
      "grad_norm": 1.3666186332702637,
      "learning_rate": 2.1009345794392525e-05,
      "loss": 0.1664,
      "step": 2395
    },
    {
      "epoch": 0.4478923263856435,
      "grad_norm": 1.8396559953689575,
      "learning_rate": 2.093457943925234e-05,
      "loss": 0.1354,
      "step": 2396
    },
    {
      "epoch": 0.4480792597439013,
      "grad_norm": 1.715602159500122,
      "learning_rate": 2.0859813084112152e-05,
      "loss": 0.258,
      "step": 2397
    },
    {
      "epoch": 0.4482661931021591,
      "grad_norm": 2.871258497238159,
      "learning_rate": 2.0785046728971965e-05,
      "loss": 0.2688,
      "step": 2398
    },
    {
      "epoch": 0.44845312646041685,
      "grad_norm": 2.174943685531616,
      "learning_rate": 2.0710280373831777e-05,
      "loss": 0.3596,
      "step": 2399
    },
    {
      "epoch": 0.44864005981867466,
      "grad_norm": 1.2465345859527588,
      "learning_rate": 2.063551401869159e-05,
      "loss": 0.148,
      "step": 2400
    },
    {
      "epoch": 0.4488269931769324,
      "grad_norm": 1.687140941619873,
      "learning_rate": 2.05607476635514e-05,
      "loss": 0.2523,
      "step": 2401
    },
    {
      "epoch": 0.4490139265351902,
      "grad_norm": 1.605965495109558,
      "learning_rate": 2.0485981308411216e-05,
      "loss": 0.1538,
      "step": 2402
    },
    {
      "epoch": 0.449200859893448,
      "grad_norm": 1.174816608428955,
      "learning_rate": 2.041121495327103e-05,
      "loss": 0.1562,
      "step": 2403
    },
    {
      "epoch": 0.44938779325170575,
      "grad_norm": 1.3574228286743164,
      "learning_rate": 2.033644859813084e-05,
      "loss": 0.1263,
      "step": 2404
    },
    {
      "epoch": 0.44957472660996356,
      "grad_norm": 1.03102707862854,
      "learning_rate": 2.0261682242990653e-05,
      "loss": 0.1022,
      "step": 2405
    },
    {
      "epoch": 0.4497616599682213,
      "grad_norm": 1.8006290197372437,
      "learning_rate": 2.0186915887850468e-05,
      "loss": 0.1822,
      "step": 2406
    },
    {
      "epoch": 0.44994859332647913,
      "grad_norm": 1.1397954225540161,
      "learning_rate": 2.011214953271028e-05,
      "loss": 0.1139,
      "step": 2407
    },
    {
      "epoch": 0.4501355266847369,
      "grad_norm": 1.443877935409546,
      "learning_rate": 2.0037383177570092e-05,
      "loss": 0.1465,
      "step": 2408
    },
    {
      "epoch": 0.4503224600429947,
      "grad_norm": 3.5760657787323,
      "learning_rate": 1.9962616822429908e-05,
      "loss": 0.2909,
      "step": 2409
    },
    {
      "epoch": 0.45050939340125246,
      "grad_norm": 1.7451778650283813,
      "learning_rate": 1.988785046728972e-05,
      "loss": 0.2581,
      "step": 2410
    },
    {
      "epoch": 0.4506963267595102,
      "grad_norm": 1.4702956676483154,
      "learning_rate": 1.9813084112149532e-05,
      "loss": 0.1551,
      "step": 2411
    },
    {
      "epoch": 0.450883260117768,
      "grad_norm": 1.4715029001235962,
      "learning_rate": 1.9738317757009347e-05,
      "loss": 0.1758,
      "step": 2412
    },
    {
      "epoch": 0.4510701934760258,
      "grad_norm": 1.279375433921814,
      "learning_rate": 1.966355140186916e-05,
      "loss": 0.1349,
      "step": 2413
    },
    {
      "epoch": 0.4512571268342836,
      "grad_norm": 2.367581844329834,
      "learning_rate": 1.958878504672897e-05,
      "loss": 0.3504,
      "step": 2414
    },
    {
      "epoch": 0.45144406019254135,
      "grad_norm": 1.2507175207138062,
      "learning_rate": 1.9514018691588787e-05,
      "loss": 0.2328,
      "step": 2415
    },
    {
      "epoch": 0.45163099355079916,
      "grad_norm": 4.279140472412109,
      "learning_rate": 1.94392523364486e-05,
      "loss": 0.2665,
      "step": 2416
    },
    {
      "epoch": 0.4518179269090569,
      "grad_norm": 2.634960412979126,
      "learning_rate": 1.936448598130841e-05,
      "loss": 0.2145,
      "step": 2417
    },
    {
      "epoch": 0.4520048602673147,
      "grad_norm": 1.5829991102218628,
      "learning_rate": 1.9289719626168227e-05,
      "loss": 0.1889,
      "step": 2418
    },
    {
      "epoch": 0.4521917936255725,
      "grad_norm": 1.4743188619613647,
      "learning_rate": 1.921495327102804e-05,
      "loss": 0.2069,
      "step": 2419
    },
    {
      "epoch": 0.45237872698383025,
      "grad_norm": 1.7427339553833008,
      "learning_rate": 1.914018691588785e-05,
      "loss": 0.1417,
      "step": 2420
    },
    {
      "epoch": 0.45256566034208806,
      "grad_norm": 2.17099666595459,
      "learning_rate": 1.9065420560747666e-05,
      "loss": 0.2119,
      "step": 2421
    },
    {
      "epoch": 0.4527525937003458,
      "grad_norm": 1.3235158920288086,
      "learning_rate": 1.899065420560748e-05,
      "loss": 0.1108,
      "step": 2422
    },
    {
      "epoch": 0.45293952705860363,
      "grad_norm": 1.8682972192764282,
      "learning_rate": 1.891588785046729e-05,
      "loss": 0.1066,
      "step": 2423
    },
    {
      "epoch": 0.4531264604168614,
      "grad_norm": 1.3362386226654053,
      "learning_rate": 1.8841121495327106e-05,
      "loss": 0.1689,
      "step": 2424
    },
    {
      "epoch": 0.45331339377511914,
      "grad_norm": 1.4857817888259888,
      "learning_rate": 1.8766355140186918e-05,
      "loss": 0.1999,
      "step": 2425
    },
    {
      "epoch": 0.45350032713337696,
      "grad_norm": 2.613889694213867,
      "learning_rate": 1.869158878504673e-05,
      "loss": 0.2764,
      "step": 2426
    },
    {
      "epoch": 0.4536872604916347,
      "grad_norm": 1.3951869010925293,
      "learning_rate": 1.8616822429906546e-05,
      "loss": 0.1094,
      "step": 2427
    },
    {
      "epoch": 0.4538741938498925,
      "grad_norm": 1.3489634990692139,
      "learning_rate": 1.8542056074766358e-05,
      "loss": 0.16,
      "step": 2428
    },
    {
      "epoch": 0.4540611272081503,
      "grad_norm": 1.146939992904663,
      "learning_rate": 1.846728971962617e-05,
      "loss": 0.0943,
      "step": 2429
    },
    {
      "epoch": 0.4542480605664081,
      "grad_norm": 1.418251395225525,
      "learning_rate": 1.8392523364485982e-05,
      "loss": 0.1334,
      "step": 2430
    },
    {
      "epoch": 0.45443499392466585,
      "grad_norm": 1.6251131296157837,
      "learning_rate": 1.8317757009345794e-05,
      "loss": 0.1763,
      "step": 2431
    },
    {
      "epoch": 0.4546219272829236,
      "grad_norm": 1.4526751041412354,
      "learning_rate": 1.824299065420561e-05,
      "loss": 0.1675,
      "step": 2432
    },
    {
      "epoch": 0.4548088606411814,
      "grad_norm": 1.496021032333374,
      "learning_rate": 1.816822429906542e-05,
      "loss": 0.1366,
      "step": 2433
    },
    {
      "epoch": 0.4549957939994392,
      "grad_norm": 2.3415374755859375,
      "learning_rate": 1.8093457943925234e-05,
      "loss": 0.2063,
      "step": 2434
    },
    {
      "epoch": 0.455182727357697,
      "grad_norm": 1.2827836275100708,
      "learning_rate": 1.8018691588785046e-05,
      "loss": 0.1683,
      "step": 2435
    },
    {
      "epoch": 0.45536966071595475,
      "grad_norm": 4.4026947021484375,
      "learning_rate": 1.7943925233644858e-05,
      "loss": 0.2188,
      "step": 2436
    },
    {
      "epoch": 0.45555659407421256,
      "grad_norm": 2.9323010444641113,
      "learning_rate": 1.7869158878504673e-05,
      "loss": 0.111,
      "step": 2437
    },
    {
      "epoch": 0.4557435274324703,
      "grad_norm": 1.2561771869659424,
      "learning_rate": 1.7794392523364485e-05,
      "loss": 0.0815,
      "step": 2438
    },
    {
      "epoch": 0.45593046079072813,
      "grad_norm": 2.46431303024292,
      "learning_rate": 1.7719626168224297e-05,
      "loss": 0.2791,
      "step": 2439
    },
    {
      "epoch": 0.4561173941489859,
      "grad_norm": 0.9038158655166626,
      "learning_rate": 1.7644859813084113e-05,
      "loss": 0.0994,
      "step": 2440
    },
    {
      "epoch": 0.45630432750724365,
      "grad_norm": 1.2136330604553223,
      "learning_rate": 1.7570093457943925e-05,
      "loss": 0.1694,
      "step": 2441
    },
    {
      "epoch": 0.45649126086550146,
      "grad_norm": 1.0026911497116089,
      "learning_rate": 1.7495327102803737e-05,
      "loss": 0.0533,
      "step": 2442
    },
    {
      "epoch": 0.4566781942237592,
      "grad_norm": 1.2152531147003174,
      "learning_rate": 1.7420560747663553e-05,
      "loss": 0.1805,
      "step": 2443
    },
    {
      "epoch": 0.456865127582017,
      "grad_norm": 1.5323137044906616,
      "learning_rate": 1.7345794392523365e-05,
      "loss": 0.1869,
      "step": 2444
    },
    {
      "epoch": 0.4570520609402748,
      "grad_norm": 4.445047378540039,
      "learning_rate": 1.7271028037383177e-05,
      "loss": 0.1158,
      "step": 2445
    },
    {
      "epoch": 0.4572389942985326,
      "grad_norm": 2.484617233276367,
      "learning_rate": 1.7196261682242992e-05,
      "loss": 0.2559,
      "step": 2446
    },
    {
      "epoch": 0.45742592765679035,
      "grad_norm": 1.8874144554138184,
      "learning_rate": 1.7121495327102804e-05,
      "loss": 0.2261,
      "step": 2447
    },
    {
      "epoch": 0.4576128610150481,
      "grad_norm": 1.9978110790252686,
      "learning_rate": 1.7046728971962616e-05,
      "loss": 0.1352,
      "step": 2448
    },
    {
      "epoch": 0.4577997943733059,
      "grad_norm": 1.3023070096969604,
      "learning_rate": 1.6971962616822432e-05,
      "loss": 0.1073,
      "step": 2449
    },
    {
      "epoch": 0.4579867277315637,
      "grad_norm": 1.4016917943954468,
      "learning_rate": 1.6897196261682244e-05,
      "loss": 0.1145,
      "step": 2450
    },
    {
      "epoch": 0.4581736610898215,
      "grad_norm": 1.8891490697860718,
      "learning_rate": 1.6822429906542056e-05,
      "loss": 0.2994,
      "step": 2451
    },
    {
      "epoch": 0.45836059444807925,
      "grad_norm": 2.3786380290985107,
      "learning_rate": 1.674766355140187e-05,
      "loss": 0.1737,
      "step": 2452
    },
    {
      "epoch": 0.45854752780633706,
      "grad_norm": 1.947412133216858,
      "learning_rate": 1.6672897196261684e-05,
      "loss": 0.2195,
      "step": 2453
    },
    {
      "epoch": 0.4587344611645948,
      "grad_norm": 5.184816360473633,
      "learning_rate": 1.6598130841121496e-05,
      "loss": 0.2889,
      "step": 2454
    },
    {
      "epoch": 0.4589213945228526,
      "grad_norm": 2.697272777557373,
      "learning_rate": 1.652336448598131e-05,
      "loss": 0.2673,
      "step": 2455
    },
    {
      "epoch": 0.4591083278811104,
      "grad_norm": 2.3935225009918213,
      "learning_rate": 1.6448598130841123e-05,
      "loss": 0.1842,
      "step": 2456
    },
    {
      "epoch": 0.45929526123936815,
      "grad_norm": 1.408286452293396,
      "learning_rate": 1.6373831775700935e-05,
      "loss": 0.1013,
      "step": 2457
    },
    {
      "epoch": 0.45948219459762596,
      "grad_norm": 2.3856823444366455,
      "learning_rate": 1.629906542056075e-05,
      "loss": 0.1503,
      "step": 2458
    },
    {
      "epoch": 0.4596691279558837,
      "grad_norm": 1.7398570775985718,
      "learning_rate": 1.6224299065420563e-05,
      "loss": 0.1809,
      "step": 2459
    },
    {
      "epoch": 0.45985606131414153,
      "grad_norm": 0.8733804821968079,
      "learning_rate": 1.6149532710280375e-05,
      "loss": 0.0562,
      "step": 2460
    },
    {
      "epoch": 0.4600429946723993,
      "grad_norm": 1.1687555313110352,
      "learning_rate": 1.607476635514019e-05,
      "loss": 0.0623,
      "step": 2461
    },
    {
      "epoch": 0.4602299280306571,
      "grad_norm": 0.9448838233947754,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.1994,
      "step": 2462
    },
    {
      "epoch": 0.46041686138891486,
      "grad_norm": 1.496984839439392,
      "learning_rate": 1.5925233644859815e-05,
      "loss": 0.1289,
      "step": 2463
    },
    {
      "epoch": 0.4606037947471726,
      "grad_norm": 1.6165765523910522,
      "learning_rate": 1.5850467289719627e-05,
      "loss": 0.1514,
      "step": 2464
    },
    {
      "epoch": 0.4607907281054304,
      "grad_norm": 1.4639660120010376,
      "learning_rate": 1.577570093457944e-05,
      "loss": 0.2206,
      "step": 2465
    },
    {
      "epoch": 0.4609776614636882,
      "grad_norm": 1.5276150703430176,
      "learning_rate": 1.570093457943925e-05,
      "loss": 0.0941,
      "step": 2466
    },
    {
      "epoch": 0.461164594821946,
      "grad_norm": 1.1243221759796143,
      "learning_rate": 1.5626168224299063e-05,
      "loss": 0.1294,
      "step": 2467
    },
    {
      "epoch": 0.46135152818020375,
      "grad_norm": 0.9347257614135742,
      "learning_rate": 1.555140186915888e-05,
      "loss": 0.1035,
      "step": 2468
    },
    {
      "epoch": 0.46153846153846156,
      "grad_norm": 1.5514787435531616,
      "learning_rate": 1.547663551401869e-05,
      "loss": 0.13,
      "step": 2469
    },
    {
      "epoch": 0.4617253948967193,
      "grad_norm": 2.636869192123413,
      "learning_rate": 1.5401869158878503e-05,
      "loss": 0.2409,
      "step": 2470
    },
    {
      "epoch": 0.4619123282549771,
      "grad_norm": 1.3527343273162842,
      "learning_rate": 1.5327102803738318e-05,
      "loss": 0.1724,
      "step": 2471
    },
    {
      "epoch": 0.4620992616132349,
      "grad_norm": 1.9967999458312988,
      "learning_rate": 1.5252336448598132e-05,
      "loss": 0.0816,
      "step": 2472
    },
    {
      "epoch": 0.46228619497149265,
      "grad_norm": 2.529910087585449,
      "learning_rate": 1.5177570093457944e-05,
      "loss": 0.0989,
      "step": 2473
    },
    {
      "epoch": 0.46247312832975046,
      "grad_norm": 1.9101643562316895,
      "learning_rate": 1.5102803738317758e-05,
      "loss": 0.3118,
      "step": 2474
    },
    {
      "epoch": 0.4626600616880082,
      "grad_norm": 1.7365432977676392,
      "learning_rate": 1.502803738317757e-05,
      "loss": 0.1147,
      "step": 2475
    },
    {
      "epoch": 0.46284699504626603,
      "grad_norm": 1.7226110696792603,
      "learning_rate": 1.4953271028037382e-05,
      "loss": 0.1442,
      "step": 2476
    },
    {
      "epoch": 0.4630339284045238,
      "grad_norm": 1.647278070449829,
      "learning_rate": 1.4878504672897198e-05,
      "loss": 0.174,
      "step": 2477
    },
    {
      "epoch": 0.46322086176278154,
      "grad_norm": 2.4361250400543213,
      "learning_rate": 1.480373831775701e-05,
      "loss": 0.2448,
      "step": 2478
    },
    {
      "epoch": 0.46340779512103936,
      "grad_norm": 2.7601189613342285,
      "learning_rate": 1.4728971962616822e-05,
      "loss": 0.1435,
      "step": 2479
    },
    {
      "epoch": 0.4635947284792971,
      "grad_norm": 1.647028923034668,
      "learning_rate": 1.4654205607476637e-05,
      "loss": 0.1986,
      "step": 2480
    },
    {
      "epoch": 0.4637816618375549,
      "grad_norm": 1.7009285688400269,
      "learning_rate": 1.457943925233645e-05,
      "loss": 0.1742,
      "step": 2481
    },
    {
      "epoch": 0.4639685951958127,
      "grad_norm": 1.4680168628692627,
      "learning_rate": 1.4504672897196261e-05,
      "loss": 0.2004,
      "step": 2482
    },
    {
      "epoch": 0.4641555285540705,
      "grad_norm": 1.370662808418274,
      "learning_rate": 1.4429906542056077e-05,
      "loss": 0.1332,
      "step": 2483
    },
    {
      "epoch": 0.46434246191232825,
      "grad_norm": 1.2574012279510498,
      "learning_rate": 1.4355140186915889e-05,
      "loss": 0.1233,
      "step": 2484
    },
    {
      "epoch": 0.464529395270586,
      "grad_norm": 1.2658499479293823,
      "learning_rate": 1.4280373831775701e-05,
      "loss": 0.1912,
      "step": 2485
    },
    {
      "epoch": 0.4647163286288438,
      "grad_norm": 1.7929072380065918,
      "learning_rate": 1.4205607476635517e-05,
      "loss": 0.2446,
      "step": 2486
    },
    {
      "epoch": 0.4649032619871016,
      "grad_norm": 3.651562452316284,
      "learning_rate": 1.4130841121495329e-05,
      "loss": 0.1624,
      "step": 2487
    },
    {
      "epoch": 0.4650901953453594,
      "grad_norm": 2.3488426208496094,
      "learning_rate": 1.405607476635514e-05,
      "loss": 0.2642,
      "step": 2488
    },
    {
      "epoch": 0.46527712870361715,
      "grad_norm": 1.663103461265564,
      "learning_rate": 1.3981308411214955e-05,
      "loss": 0.1872,
      "step": 2489
    },
    {
      "epoch": 0.46546406206187496,
      "grad_norm": 3.286301612854004,
      "learning_rate": 1.3906542056074767e-05,
      "loss": 0.1431,
      "step": 2490
    },
    {
      "epoch": 0.4656509954201327,
      "grad_norm": 1.1534905433654785,
      "learning_rate": 1.3831775700934579e-05,
      "loss": 0.1487,
      "step": 2491
    },
    {
      "epoch": 0.46583792877839053,
      "grad_norm": 1.3638497591018677,
      "learning_rate": 1.3757009345794394e-05,
      "loss": 0.1287,
      "step": 2492
    },
    {
      "epoch": 0.4660248621366483,
      "grad_norm": 1.56427001953125,
      "learning_rate": 1.3682242990654206e-05,
      "loss": 0.1761,
      "step": 2493
    },
    {
      "epoch": 0.46621179549490604,
      "grad_norm": 1.3177947998046875,
      "learning_rate": 1.3607476635514018e-05,
      "loss": 0.102,
      "step": 2494
    },
    {
      "epoch": 0.46639872885316386,
      "grad_norm": 3.0803749561309814,
      "learning_rate": 1.3532710280373834e-05,
      "loss": 0.1002,
      "step": 2495
    },
    {
      "epoch": 0.4665856622114216,
      "grad_norm": 2.107034683227539,
      "learning_rate": 1.3457943925233646e-05,
      "loss": 0.1924,
      "step": 2496
    },
    {
      "epoch": 0.4667725955696794,
      "grad_norm": 3.5381600856781006,
      "learning_rate": 1.3383177570093458e-05,
      "loss": 0.2378,
      "step": 2497
    },
    {
      "epoch": 0.4669595289279372,
      "grad_norm": 1.3647404909133911,
      "learning_rate": 1.3308411214953273e-05,
      "loss": 0.2005,
      "step": 2498
    },
    {
      "epoch": 0.467146462286195,
      "grad_norm": 2.535250663757324,
      "learning_rate": 1.3233644859813086e-05,
      "loss": 0.1951,
      "step": 2499
    },
    {
      "epoch": 0.46733339564445275,
      "grad_norm": 1.5983929634094238,
      "learning_rate": 1.3158878504672898e-05,
      "loss": 0.1073,
      "step": 2500
    },
    {
      "epoch": 0.4675203290027105,
      "grad_norm": 1.7448508739471436,
      "learning_rate": 1.308411214953271e-05,
      "loss": 0.1254,
      "step": 2501
    },
    {
      "epoch": 0.4677072623609683,
      "grad_norm": 2.9755308628082275,
      "learning_rate": 1.3009345794392525e-05,
      "loss": 0.1803,
      "step": 2502
    },
    {
      "epoch": 0.4678941957192261,
      "grad_norm": 1.9257768392562866,
      "learning_rate": 1.2934579439252337e-05,
      "loss": 0.1722,
      "step": 2503
    },
    {
      "epoch": 0.4680811290774839,
      "grad_norm": 2.188129425048828,
      "learning_rate": 1.285981308411215e-05,
      "loss": 0.1753,
      "step": 2504
    },
    {
      "epoch": 0.46826806243574165,
      "grad_norm": 1.1521371603012085,
      "learning_rate": 1.2785046728971963e-05,
      "loss": 0.1277,
      "step": 2505
    },
    {
      "epoch": 0.46845499579399946,
      "grad_norm": 1.217669129371643,
      "learning_rate": 1.2710280373831775e-05,
      "loss": 0.1402,
      "step": 2506
    },
    {
      "epoch": 0.4686419291522572,
      "grad_norm": 4.335115909576416,
      "learning_rate": 1.2635514018691589e-05,
      "loss": 0.207,
      "step": 2507
    },
    {
      "epoch": 0.468828862510515,
      "grad_norm": 1.811019778251648,
      "learning_rate": 1.2560747663551403e-05,
      "loss": 0.1298,
      "step": 2508
    },
    {
      "epoch": 0.4690157958687728,
      "grad_norm": 1.5164436101913452,
      "learning_rate": 1.2485981308411215e-05,
      "loss": 0.1203,
      "step": 2509
    },
    {
      "epoch": 0.46920272922703055,
      "grad_norm": 2.3357388973236084,
      "learning_rate": 1.2411214953271029e-05,
      "loss": 0.2693,
      "step": 2510
    },
    {
      "epoch": 0.46938966258528836,
      "grad_norm": 1.5029668807983398,
      "learning_rate": 1.233644859813084e-05,
      "loss": 0.1793,
      "step": 2511
    },
    {
      "epoch": 0.4695765959435461,
      "grad_norm": 1.9433015584945679,
      "learning_rate": 1.2261682242990655e-05,
      "loss": 0.2412,
      "step": 2512
    },
    {
      "epoch": 0.46976352930180393,
      "grad_norm": 0.9449354410171509,
      "learning_rate": 1.2186915887850468e-05,
      "loss": 0.1195,
      "step": 2513
    },
    {
      "epoch": 0.4699504626600617,
      "grad_norm": 2.2366292476654053,
      "learning_rate": 1.211214953271028e-05,
      "loss": 0.1913,
      "step": 2514
    },
    {
      "epoch": 0.47013739601831944,
      "grad_norm": 1.2654048204421997,
      "learning_rate": 1.2037383177570094e-05,
      "loss": 0.1264,
      "step": 2515
    },
    {
      "epoch": 0.47032432937657725,
      "grad_norm": 10.838383674621582,
      "learning_rate": 1.1962616822429908e-05,
      "loss": 0.2415,
      "step": 2516
    },
    {
      "epoch": 0.470511262734835,
      "grad_norm": 1.210991382598877,
      "learning_rate": 1.188785046728972e-05,
      "loss": 0.1684,
      "step": 2517
    },
    {
      "epoch": 0.4706981960930928,
      "grad_norm": 1.9064056873321533,
      "learning_rate": 1.1813084112149534e-05,
      "loss": 0.2028,
      "step": 2518
    },
    {
      "epoch": 0.4708851294513506,
      "grad_norm": 1.5199620723724365,
      "learning_rate": 1.1738317757009348e-05,
      "loss": 0.1459,
      "step": 2519
    },
    {
      "epoch": 0.4710720628096084,
      "grad_norm": 1.176767349243164,
      "learning_rate": 1.166355140186916e-05,
      "loss": 0.1212,
      "step": 2520
    },
    {
      "epoch": 0.47125899616786615,
      "grad_norm": 1.0059531927108765,
      "learning_rate": 1.1588785046728972e-05,
      "loss": 0.0789,
      "step": 2521
    },
    {
      "epoch": 0.47144592952612396,
      "grad_norm": 1.2388465404510498,
      "learning_rate": 1.1514018691588786e-05,
      "loss": 0.1765,
      "step": 2522
    },
    {
      "epoch": 0.4716328628843817,
      "grad_norm": 1.5005406141281128,
      "learning_rate": 1.1439252336448598e-05,
      "loss": 0.1527,
      "step": 2523
    },
    {
      "epoch": 0.4718197962426395,
      "grad_norm": 10.24960994720459,
      "learning_rate": 1.1364485981308412e-05,
      "loss": 0.3286,
      "step": 2524
    },
    {
      "epoch": 0.4720067296008973,
      "grad_norm": 5.3363776206970215,
      "learning_rate": 1.1289719626168224e-05,
      "loss": 0.1476,
      "step": 2525
    },
    {
      "epoch": 0.47219366295915505,
      "grad_norm": 1.9360623359680176,
      "learning_rate": 1.1214953271028037e-05,
      "loss": 0.1874,
      "step": 2526
    },
    {
      "epoch": 0.47238059631741286,
      "grad_norm": 0.9552204012870789,
      "learning_rate": 1.1140186915887851e-05,
      "loss": 0.0734,
      "step": 2527
    },
    {
      "epoch": 0.4725675296756706,
      "grad_norm": 2.9538888931274414,
      "learning_rate": 1.1065420560747663e-05,
      "loss": 0.1381,
      "step": 2528
    },
    {
      "epoch": 0.47275446303392843,
      "grad_norm": 0.9753272533416748,
      "learning_rate": 1.0990654205607477e-05,
      "loss": 0.0896,
      "step": 2529
    },
    {
      "epoch": 0.4729413963921862,
      "grad_norm": 1.0712847709655762,
      "learning_rate": 1.0915887850467291e-05,
      "loss": 0.0998,
      "step": 2530
    },
    {
      "epoch": 0.47312832975044394,
      "grad_norm": 0.7768780589103699,
      "learning_rate": 1.0841121495327103e-05,
      "loss": 0.0546,
      "step": 2531
    },
    {
      "epoch": 0.47331526310870176,
      "grad_norm": 1.2742981910705566,
      "learning_rate": 1.0766355140186917e-05,
      "loss": 0.1337,
      "step": 2532
    },
    {
      "epoch": 0.4735021964669595,
      "grad_norm": 2.2762019634246826,
      "learning_rate": 1.069158878504673e-05,
      "loss": 0.1314,
      "step": 2533
    },
    {
      "epoch": 0.4736891298252173,
      "grad_norm": 0.5411030650138855,
      "learning_rate": 1.0616822429906543e-05,
      "loss": 0.0758,
      "step": 2534
    },
    {
      "epoch": 0.4738760631834751,
      "grad_norm": 1.0392314195632935,
      "learning_rate": 1.0542056074766356e-05,
      "loss": 0.1043,
      "step": 2535
    },
    {
      "epoch": 0.4740629965417329,
      "grad_norm": 2.9006385803222656,
      "learning_rate": 1.046728971962617e-05,
      "loss": 0.1867,
      "step": 2536
    },
    {
      "epoch": 0.47424992989999065,
      "grad_norm": 3.4449706077575684,
      "learning_rate": 1.0392523364485982e-05,
      "loss": 0.2358,
      "step": 2537
    },
    {
      "epoch": 0.4744368632582484,
      "grad_norm": 2.075572967529297,
      "learning_rate": 1.0317757009345794e-05,
      "loss": 0.1783,
      "step": 2538
    },
    {
      "epoch": 0.4746237966165062,
      "grad_norm": 1.2646695375442505,
      "learning_rate": 1.0242990654205608e-05,
      "loss": 0.203,
      "step": 2539
    },
    {
      "epoch": 0.474810729974764,
      "grad_norm": 1.5701689720153809,
      "learning_rate": 1.016822429906542e-05,
      "loss": 0.18,
      "step": 2540
    },
    {
      "epoch": 0.4749976633330218,
      "grad_norm": 0.9719337224960327,
      "learning_rate": 1.0093457943925234e-05,
      "loss": 0.0773,
      "step": 2541
    },
    {
      "epoch": 0.47518459669127955,
      "grad_norm": 3.9162437915802,
      "learning_rate": 1.0018691588785046e-05,
      "loss": 0.3132,
      "step": 2542
    },
    {
      "epoch": 0.47537153004953736,
      "grad_norm": 1.1221381425857544,
      "learning_rate": 9.94392523364486e-06,
      "loss": 0.1675,
      "step": 2543
    },
    {
      "epoch": 0.4755584634077951,
      "grad_norm": 1.1752938032150269,
      "learning_rate": 9.869158878504674e-06,
      "loss": 0.1411,
      "step": 2544
    },
    {
      "epoch": 0.4757453967660529,
      "grad_norm": 1.8447155952453613,
      "learning_rate": 9.794392523364486e-06,
      "loss": 0.1843,
      "step": 2545
    },
    {
      "epoch": 0.4759323301243107,
      "grad_norm": 1.0922807455062866,
      "learning_rate": 9.7196261682243e-06,
      "loss": 0.1442,
      "step": 2546
    },
    {
      "epoch": 0.47611926348256844,
      "grad_norm": 3.1777749061584473,
      "learning_rate": 9.644859813084113e-06,
      "loss": 0.4305,
      "step": 2547
    },
    {
      "epoch": 0.47630619684082626,
      "grad_norm": 0.7789682745933533,
      "learning_rate": 9.570093457943925e-06,
      "loss": 0.0894,
      "step": 2548
    },
    {
      "epoch": 0.476493130199084,
      "grad_norm": 2.735740900039673,
      "learning_rate": 9.49532710280374e-06,
      "loss": 0.1676,
      "step": 2549
    },
    {
      "epoch": 0.4766800635573418,
      "grad_norm": 1.4079158306121826,
      "learning_rate": 9.420560747663553e-06,
      "loss": 0.1512,
      "step": 2550
    },
    {
      "epoch": 0.4768669969155996,
      "grad_norm": 1.4460266828536987,
      "learning_rate": 9.345794392523365e-06,
      "loss": 0.1316,
      "step": 2551
    },
    {
      "epoch": 0.4770539302738574,
      "grad_norm": 1.865301251411438,
      "learning_rate": 9.271028037383179e-06,
      "loss": 0.2011,
      "step": 2552
    },
    {
      "epoch": 0.47724086363211515,
      "grad_norm": 1.6367415189743042,
      "learning_rate": 9.196261682242991e-06,
      "loss": 0.2235,
      "step": 2553
    },
    {
      "epoch": 0.4774277969903729,
      "grad_norm": 1.1779358386993408,
      "learning_rate": 9.121495327102805e-06,
      "loss": 0.1381,
      "step": 2554
    },
    {
      "epoch": 0.4776147303486307,
      "grad_norm": 4.35983943939209,
      "learning_rate": 9.046728971962617e-06,
      "loss": 0.3024,
      "step": 2555
    },
    {
      "epoch": 0.4778016637068885,
      "grad_norm": 1.6198521852493286,
      "learning_rate": 8.971962616822429e-06,
      "loss": 0.1958,
      "step": 2556
    },
    {
      "epoch": 0.4779885970651463,
      "grad_norm": 1.6666128635406494,
      "learning_rate": 8.897196261682243e-06,
      "loss": 0.2077,
      "step": 2557
    },
    {
      "epoch": 0.47817553042340405,
      "grad_norm": 1.8359596729278564,
      "learning_rate": 8.822429906542056e-06,
      "loss": 0.1881,
      "step": 2558
    },
    {
      "epoch": 0.47836246378166186,
      "grad_norm": 2.7369425296783447,
      "learning_rate": 8.747663551401869e-06,
      "loss": 0.1213,
      "step": 2559
    },
    {
      "epoch": 0.4785493971399196,
      "grad_norm": 3.0371060371398926,
      "learning_rate": 8.672897196261682e-06,
      "loss": 0.1921,
      "step": 2560
    },
    {
      "epoch": 0.4787363304981774,
      "grad_norm": 2.0738325119018555,
      "learning_rate": 8.598130841121496e-06,
      "loss": 0.2453,
      "step": 2561
    },
    {
      "epoch": 0.4789232638564352,
      "grad_norm": 1.3492907285690308,
      "learning_rate": 8.523364485981308e-06,
      "loss": 0.2036,
      "step": 2562
    },
    {
      "epoch": 0.47911019721469295,
      "grad_norm": 2.696047067642212,
      "learning_rate": 8.448598130841122e-06,
      "loss": 0.0895,
      "step": 2563
    },
    {
      "epoch": 0.47929713057295076,
      "grad_norm": 2.3330042362213135,
      "learning_rate": 8.373831775700936e-06,
      "loss": 0.1834,
      "step": 2564
    },
    {
      "epoch": 0.4794840639312085,
      "grad_norm": 1.942621111869812,
      "learning_rate": 8.299065420560748e-06,
      "loss": 0.1262,
      "step": 2565
    },
    {
      "epoch": 0.4796709972894663,
      "grad_norm": 1.7087037563323975,
      "learning_rate": 8.224299065420562e-06,
      "loss": 0.0912,
      "step": 2566
    },
    {
      "epoch": 0.4798579306477241,
      "grad_norm": 1.3580458164215088,
      "learning_rate": 8.149532710280375e-06,
      "loss": 0.1728,
      "step": 2567
    },
    {
      "epoch": 0.48004486400598184,
      "grad_norm": 1.6547991037368774,
      "learning_rate": 8.074766355140188e-06,
      "loss": 0.1752,
      "step": 2568
    },
    {
      "epoch": 0.48023179736423965,
      "grad_norm": 0.8923231959342957,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.1093,
      "step": 2569
    },
    {
      "epoch": 0.4804187307224974,
      "grad_norm": 1.2897493839263916,
      "learning_rate": 7.925233644859813e-06,
      "loss": 0.2168,
      "step": 2570
    },
    {
      "epoch": 0.4806056640807552,
      "grad_norm": 1.136130928993225,
      "learning_rate": 7.850467289719626e-06,
      "loss": 0.0986,
      "step": 2571
    },
    {
      "epoch": 0.480792597439013,
      "grad_norm": 1.4101719856262207,
      "learning_rate": 7.77570093457944e-06,
      "loss": 0.0849,
      "step": 2572
    },
    {
      "epoch": 0.4809795307972708,
      "grad_norm": 3.5091280937194824,
      "learning_rate": 7.700934579439251e-06,
      "loss": 0.2474,
      "step": 2573
    },
    {
      "epoch": 0.48116646415552855,
      "grad_norm": 1.5508482456207275,
      "learning_rate": 7.626168224299066e-06,
      "loss": 0.1276,
      "step": 2574
    },
    {
      "epoch": 0.4813533975137863,
      "grad_norm": 8.345069885253906,
      "learning_rate": 7.551401869158879e-06,
      "loss": 0.4821,
      "step": 2575
    },
    {
      "epoch": 0.4815403308720441,
      "grad_norm": 1.7412666082382202,
      "learning_rate": 7.476635514018691e-06,
      "loss": 0.1785,
      "step": 2576
    },
    {
      "epoch": 0.4817272642303019,
      "grad_norm": 0.9324601888656616,
      "learning_rate": 7.401869158878505e-06,
      "loss": 0.1018,
      "step": 2577
    },
    {
      "epoch": 0.4819141975885597,
      "grad_norm": 1.149871587753296,
      "learning_rate": 7.327102803738319e-06,
      "loss": 0.1635,
      "step": 2578
    },
    {
      "epoch": 0.48210113094681745,
      "grad_norm": 3.0420336723327637,
      "learning_rate": 7.252336448598131e-06,
      "loss": 0.2175,
      "step": 2579
    },
    {
      "epoch": 0.48228806430507526,
      "grad_norm": 1.0805898904800415,
      "learning_rate": 7.1775700934579445e-06,
      "loss": 0.1364,
      "step": 2580
    },
    {
      "epoch": 0.482474997663333,
      "grad_norm": 1.6343353986740112,
      "learning_rate": 7.102803738317758e-06,
      "loss": 0.1666,
      "step": 2581
    },
    {
      "epoch": 0.48266193102159083,
      "grad_norm": 0.9829802513122559,
      "learning_rate": 7.02803738317757e-06,
      "loss": 0.111,
      "step": 2582
    },
    {
      "epoch": 0.4828488643798486,
      "grad_norm": 1.8654553890228271,
      "learning_rate": 6.953271028037383e-06,
      "loss": 0.1806,
      "step": 2583
    },
    {
      "epoch": 0.48303579773810634,
      "grad_norm": 1.1763585805892944,
      "learning_rate": 6.878504672897197e-06,
      "loss": 0.1109,
      "step": 2584
    },
    {
      "epoch": 0.48322273109636416,
      "grad_norm": 2.66056752204895,
      "learning_rate": 6.803738317757009e-06,
      "loss": 0.117,
      "step": 2585
    },
    {
      "epoch": 0.4834096644546219,
      "grad_norm": 2.51767897605896,
      "learning_rate": 6.728971962616823e-06,
      "loss": 0.1597,
      "step": 2586
    },
    {
      "epoch": 0.4835965978128797,
      "grad_norm": 1.3428586721420288,
      "learning_rate": 6.654205607476637e-06,
      "loss": 0.1713,
      "step": 2587
    },
    {
      "epoch": 0.4837835311711375,
      "grad_norm": 0.9908161163330078,
      "learning_rate": 6.579439252336449e-06,
      "loss": 0.0838,
      "step": 2588
    },
    {
      "epoch": 0.4839704645293953,
      "grad_norm": 1.8318836688995361,
      "learning_rate": 6.504672897196263e-06,
      "loss": 0.2491,
      "step": 2589
    },
    {
      "epoch": 0.48415739788765305,
      "grad_norm": 1.1613993644714355,
      "learning_rate": 6.429906542056075e-06,
      "loss": 0.1315,
      "step": 2590
    },
    {
      "epoch": 0.4843443312459108,
      "grad_norm": 1.144452452659607,
      "learning_rate": 6.355140186915888e-06,
      "loss": 0.171,
      "step": 2591
    },
    {
      "epoch": 0.4845312646041686,
      "grad_norm": 1.6834235191345215,
      "learning_rate": 6.2803738317757014e-06,
      "loss": 0.2166,
      "step": 2592
    },
    {
      "epoch": 0.4847181979624264,
      "grad_norm": 3.279996871948242,
      "learning_rate": 6.205607476635514e-06,
      "loss": 0.1227,
      "step": 2593
    },
    {
      "epoch": 0.4849051313206842,
      "grad_norm": 7.255350112915039,
      "learning_rate": 6.130841121495327e-06,
      "loss": 0.478,
      "step": 2594
    },
    {
      "epoch": 0.48509206467894195,
      "grad_norm": 1.8352394104003906,
      "learning_rate": 6.05607476635514e-06,
      "loss": 0.1632,
      "step": 2595
    },
    {
      "epoch": 0.48527899803719976,
      "grad_norm": 4.512792110443115,
      "learning_rate": 5.981308411214954e-06,
      "loss": 0.2921,
      "step": 2596
    },
    {
      "epoch": 0.4854659313954575,
      "grad_norm": 1.0392892360687256,
      "learning_rate": 5.906542056074767e-06,
      "loss": 0.216,
      "step": 2597
    },
    {
      "epoch": 0.4856528647537153,
      "grad_norm": 1.1466147899627686,
      "learning_rate": 5.83177570093458e-06,
      "loss": 0.1664,
      "step": 2598
    },
    {
      "epoch": 0.4858397981119731,
      "grad_norm": 1.9019325971603394,
      "learning_rate": 5.757009345794393e-06,
      "loss": 0.2533,
      "step": 2599
    },
    {
      "epoch": 0.48602673147023084,
      "grad_norm": 2.232154369354248,
      "learning_rate": 5.682242990654206e-06,
      "loss": 0.2002,
      "step": 2600
    },
    {
      "epoch": 0.48621366482848866,
      "grad_norm": 1.158602237701416,
      "learning_rate": 5.607476635514019e-06,
      "loss": 0.152,
      "step": 2601
    },
    {
      "epoch": 0.4864005981867464,
      "grad_norm": 1.7631500959396362,
      "learning_rate": 5.532710280373832e-06,
      "loss": 0.1188,
      "step": 2602
    },
    {
      "epoch": 0.4865875315450042,
      "grad_norm": 1.2128393650054932,
      "learning_rate": 5.4579439252336454e-06,
      "loss": 0.1074,
      "step": 2603
    },
    {
      "epoch": 0.486774464903262,
      "grad_norm": 1.6554797887802124,
      "learning_rate": 5.383177570093458e-06,
      "loss": 0.1599,
      "step": 2604
    },
    {
      "epoch": 0.4869613982615198,
      "grad_norm": 1.4009745121002197,
      "learning_rate": 5.308411214953271e-06,
      "loss": 0.1336,
      "step": 2605
    },
    {
      "epoch": 0.48714833161977755,
      "grad_norm": 1.4288033246994019,
      "learning_rate": 5.233644859813085e-06,
      "loss": 0.1601,
      "step": 2606
    },
    {
      "epoch": 0.4873352649780353,
      "grad_norm": 1.9025142192840576,
      "learning_rate": 5.158878504672897e-06,
      "loss": 0.2046,
      "step": 2607
    },
    {
      "epoch": 0.4875221983362931,
      "grad_norm": 1.6135265827178955,
      "learning_rate": 5.08411214953271e-06,
      "loss": 0.1482,
      "step": 2608
    },
    {
      "epoch": 0.4877091316945509,
      "grad_norm": 2.1464197635650635,
      "learning_rate": 5.009345794392523e-06,
      "loss": 0.185,
      "step": 2609
    },
    {
      "epoch": 0.4878960650528087,
      "grad_norm": 1.1851356029510498,
      "learning_rate": 4.934579439252337e-06,
      "loss": 0.1242,
      "step": 2610
    },
    {
      "epoch": 0.48808299841106645,
      "grad_norm": 3.055976390838623,
      "learning_rate": 4.85981308411215e-06,
      "loss": 0.3065,
      "step": 2611
    },
    {
      "epoch": 0.48826993176932426,
      "grad_norm": 1.5659304857254028,
      "learning_rate": 4.785046728971963e-06,
      "loss": 0.1656,
      "step": 2612
    },
    {
      "epoch": 0.488456865127582,
      "grad_norm": 0.8587788939476013,
      "learning_rate": 4.7102803738317765e-06,
      "loss": 0.0615,
      "step": 2613
    },
    {
      "epoch": 0.4886437984858398,
      "grad_norm": 1.0531266927719116,
      "learning_rate": 4.6355140186915894e-06,
      "loss": 0.1052,
      "step": 2614
    },
    {
      "epoch": 0.4888307318440976,
      "grad_norm": 1.0152966976165771,
      "learning_rate": 4.560747663551402e-06,
      "loss": 0.0669,
      "step": 2615
    },
    {
      "epoch": 0.48901766520235534,
      "grad_norm": 2.274864673614502,
      "learning_rate": 4.4859813084112145e-06,
      "loss": 0.3202,
      "step": 2616
    },
    {
      "epoch": 0.48920459856061316,
      "grad_norm": 1.0699807405471802,
      "learning_rate": 4.411214953271028e-06,
      "loss": 0.0763,
      "step": 2617
    },
    {
      "epoch": 0.4893915319188709,
      "grad_norm": 1.2863218784332275,
      "learning_rate": 4.336448598130841e-06,
      "loss": 0.2384,
      "step": 2618
    },
    {
      "epoch": 0.4895784652771287,
      "grad_norm": 1.2103713750839233,
      "learning_rate": 4.261682242990654e-06,
      "loss": 0.1757,
      "step": 2619
    },
    {
      "epoch": 0.4897653986353865,
      "grad_norm": 2.9024782180786133,
      "learning_rate": 4.186915887850468e-06,
      "loss": 0.1848,
      "step": 2620
    },
    {
      "epoch": 0.48995233199364424,
      "grad_norm": 1.4782912731170654,
      "learning_rate": 4.112149532710281e-06,
      "loss": 0.1682,
      "step": 2621
    },
    {
      "epoch": 0.49013926535190205,
      "grad_norm": 1.5596877336502075,
      "learning_rate": 4.037383177570094e-06,
      "loss": 0.1274,
      "step": 2622
    },
    {
      "epoch": 0.4903261987101598,
      "grad_norm": 1.9613016843795776,
      "learning_rate": 3.962616822429907e-06,
      "loss": 0.2846,
      "step": 2623
    },
    {
      "epoch": 0.4905131320684176,
      "grad_norm": 0.9661224484443665,
      "learning_rate": 3.88785046728972e-06,
      "loss": 0.1047,
      "step": 2624
    },
    {
      "epoch": 0.4907000654266754,
      "grad_norm": 3.141319751739502,
      "learning_rate": 3.813084112149533e-06,
      "loss": 0.2538,
      "step": 2625
    },
    {
      "epoch": 0.4908869987849332,
      "grad_norm": 2.923769474029541,
      "learning_rate": 3.7383177570093455e-06,
      "loss": 0.1276,
      "step": 2626
    },
    {
      "epoch": 0.49107393214319095,
      "grad_norm": 1.6850942373275757,
      "learning_rate": 3.6635514018691593e-06,
      "loss": 0.185,
      "step": 2627
    },
    {
      "epoch": 0.4912608655014487,
      "grad_norm": 1.4858845472335815,
      "learning_rate": 3.5887850467289722e-06,
      "loss": 0.1601,
      "step": 2628
    },
    {
      "epoch": 0.4914477988597065,
      "grad_norm": 3.0309038162231445,
      "learning_rate": 3.514018691588785e-06,
      "loss": 0.2387,
      "step": 2629
    },
    {
      "epoch": 0.4916347322179643,
      "grad_norm": 3.549856662750244,
      "learning_rate": 3.4392523364485985e-06,
      "loss": 0.1685,
      "step": 2630
    },
    {
      "epoch": 0.4918216655762221,
      "grad_norm": 2.0535542964935303,
      "learning_rate": 3.3644859813084115e-06,
      "loss": 0.1627,
      "step": 2631
    },
    {
      "epoch": 0.49200859893447985,
      "grad_norm": 3.036679267883301,
      "learning_rate": 3.2897196261682244e-06,
      "loss": 0.1171,
      "step": 2632
    },
    {
      "epoch": 0.49219553229273766,
      "grad_norm": 1.335602045059204,
      "learning_rate": 3.2149532710280374e-06,
      "loss": 0.158,
      "step": 2633
    },
    {
      "epoch": 0.4923824656509954,
      "grad_norm": 1.4568345546722412,
      "learning_rate": 3.1401869158878507e-06,
      "loss": 0.1575,
      "step": 2634
    },
    {
      "epoch": 0.49256939900925323,
      "grad_norm": 6.207457065582275,
      "learning_rate": 3.0654205607476637e-06,
      "loss": 0.2372,
      "step": 2635
    },
    {
      "epoch": 0.492756332367511,
      "grad_norm": 1.203599452972412,
      "learning_rate": 2.990654205607477e-06,
      "loss": 0.0866,
      "step": 2636
    },
    {
      "epoch": 0.49294326572576874,
      "grad_norm": 2.167904853820801,
      "learning_rate": 2.91588785046729e-06,
      "loss": 0.2114,
      "step": 2637
    },
    {
      "epoch": 0.49313019908402655,
      "grad_norm": 1.3014416694641113,
      "learning_rate": 2.841121495327103e-06,
      "loss": 0.1452,
      "step": 2638
    },
    {
      "epoch": 0.4933171324422843,
      "grad_norm": 1.530693531036377,
      "learning_rate": 2.766355140186916e-06,
      "loss": 0.1462,
      "step": 2639
    },
    {
      "epoch": 0.4935040658005421,
      "grad_norm": 1.2411863803863525,
      "learning_rate": 2.691588785046729e-06,
      "loss": 0.1118,
      "step": 2640
    },
    {
      "epoch": 0.4936909991587999,
      "grad_norm": 1.3047033548355103,
      "learning_rate": 2.6168224299065425e-06,
      "loss": 0.127,
      "step": 2641
    },
    {
      "epoch": 0.4938779325170577,
      "grad_norm": 2.6786816120147705,
      "learning_rate": 2.542056074766355e-06,
      "loss": 0.2505,
      "step": 2642
    },
    {
      "epoch": 0.49406486587531545,
      "grad_norm": 1.9563099145889282,
      "learning_rate": 2.4672897196261684e-06,
      "loss": 0.1346,
      "step": 2643
    },
    {
      "epoch": 0.4942517992335732,
      "grad_norm": 1.2064552307128906,
      "learning_rate": 2.3925233644859814e-06,
      "loss": 0.1189,
      "step": 2644
    },
    {
      "epoch": 0.494438732591831,
      "grad_norm": 1.2720890045166016,
      "learning_rate": 2.3177570093457947e-06,
      "loss": 0.1659,
      "step": 2645
    },
    {
      "epoch": 0.4946256659500888,
      "grad_norm": 1.2287803888320923,
      "learning_rate": 2.2429906542056072e-06,
      "loss": 0.116,
      "step": 2646
    },
    {
      "epoch": 0.4948125993083466,
      "grad_norm": 2.1304805278778076,
      "learning_rate": 2.1682242990654206e-06,
      "loss": 0.2611,
      "step": 2647
    },
    {
      "epoch": 0.49499953266660435,
      "grad_norm": 1.9226938486099243,
      "learning_rate": 2.093457943925234e-06,
      "loss": 0.182,
      "step": 2648
    },
    {
      "epoch": 0.49518646602486216,
      "grad_norm": 2.813961982727051,
      "learning_rate": 2.018691588785047e-06,
      "loss": 0.1904,
      "step": 2649
    },
    {
      "epoch": 0.4953733993831199,
      "grad_norm": 1.3948544263839722,
      "learning_rate": 1.94392523364486e-06,
      "loss": 0.1179,
      "step": 2650
    }
  ],
  "logging_steps": 1,
  "max_steps": 2675,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 50,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.0754529918354432e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
